[
    {
        "basename": "232b43584b2236669c0a53702ad89ab10c3886ea.grobid",
        "fulltext": 15,
        "footnote_size": 0,
        "reference": 49,
        "authors": [
            "Dabney",
            "Ostrovski",
            "Silver",
            "Munos"
        ]
    },
    {
        "title": "Implicit Quantile Networks for Distributional Reinforcement Learning",
        "abstract": "In this work, we build on recent advances in distributional reinforcement learning to give a generally applicable, flexible, and state-of-the-art distributional variant of DQN. We achieve this by using quantile regression to approximate the full quantile function for the state-action return distribution. By reparameterizing a distribution over the sample space, this yields an implicitly defined return distribution and gives rise to a large class of risk-sensitive policies. We demonstrate improved performance on the 57 Atari 2600 games in the ALE, and use our algorithm's implicitly defined distributions to study the effects of risk-sensitive policies in Atari games.",
        "Introduction": "Distributional reinforcement learning  #b18  #b35  #b45 Morimura et al., 2010b; #b5  focuses on the intrinsic randomness of returns within the reinforcement learning (RL) framework. As the agent interacts with the environment, irreducible randomness seeps in through the stochasticity of these interactions, the approximations in the agent's representation, and even the inherently chaotic nature of physical interaction  #b48 . Distributional RL aims to model the distribution over returns, whose mean is the traditional value function, and to use these distributions to evaluate and optimize a policy.Any distributional RL algorithm is characterized by two aspects: the parameterization of the return distribution, and the distance metric or loss function being optimized. Together, these choices control assumptions about the random returns and how approximations will be traded off. Categorical DQN (Bellemare et al., 2017, C51) combines a categorical distribution and the cross-entropy loss with the Cram\u00e9r-minimizing projection  #b32 . For Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s). this, it assumes returns are bounded in a known range and trades off mean-preservation at the cost of overestimating variance.C51 outperformed all previous improvements to DQN on a set of 57 Atari 2600 games in the Arcade Learning Environment  #b4 , which we refer to as the Atari-57 benchmark. Subsequently, several papers have built upon this successful combination to achieve significant improvements to the state-of-the-art in Atari-57  #b15  #b14 , and challenging continuous control tasks  #b3 .These algorithms are restricted to assigning probabilities to an a priori fixed, discrete set of possible returns.  #b9  propose an alternate pair of choices, parameterizing the distribution by a uniform mixture of Diracs whose locations are adjusted using quantile regression. Their algorithm, QR-DQN, while restricted to a discrete set of quantiles, automatically adapts return quantiles to minimize the Wasserstein distance between the Bellman updated and current return distributions. This flexibility allows QR-DQN to significantly improve on C51's Atari-57 performance.In this paper, we extend the approach of  #b9 , from learning a discrete set of quantiles to learning the full quantile function, a continuous map from probabilities to returns. When combined with a base distribution, such as U ([0, 1]), this forms an implicit distribution capable of approximating any distribution over returns given sufficient network capacity. Our approach, implicit quantile networks (IQN), is best viewed as a simple distributional generalization of the DQN algorithm  #b24 , and provides several benefits over QR-DQN.First, the approximation error for the distribution is no longer controlled by the number of quantiles output by the network, but by the size of the network itself, and the amount of training. Second, IQN can be used with as few, or as many, samples per update as desired, providing improved data efficiency with increasing number of samples per training update. Third, the implicit representation of the return distribution allows us to expand the class of policies to more fully take advantage of the learned distribution. Specifically, by taking the base distribution to be non-uniform, we expand the class of policies to -greedy policies on arbitrary distortion risk measures  #b47  #b41 .",
        "arXiv:1806.06923v1 [cs.LG] 14 Jun 2018": "We begin by reviewing distributional reinforcement learning, related work, and introducing the concepts surrounding risk-sensitive RL. In subsequent sections, we introduce our proposed algorithm, IQN, and present a series of experiments using the Atari-57 benchmark, investigating the robustness and performance of IQN. Despite being a simple distributional extension to DQN, and forgoing any other improvements, IQN significantly outperforms QR-DQN and nearly matches the performance of Rainbow, which combines many orthogonal advances. In fact, in human-starts as well as in the hardest Atari games (where current RL agents still underperform human players) IQN improves over Rainbow.",
        "Background / Related Work": "We consider the standard RL setting, in which the interaction of an agent and an environment is modeled as a Markov Decision Process (X , A, R, P, \u03b3)  #b31 , where X and A denote the state and action spaces, R the (state-and action-dependent) reward function, P (\u2022|x, a) the transition kernel, and \u03b3 \u2208 (0, 1) a discount factor. A policy \u03c0(\u2022|x) maps a state to a distribution over actions.For an agent following policy \u03c0, the discounted sum of future rewards is denoted by the random variableZ \u03c0 (x, a) = \u221e t=0 \u03b3 t R(x t , a t ), where x 0 = x, a 0 = a, x t \u223c P (\u2022|x t-1 , a t-1 ), and a t \u223c \u03c0(\u2022|x t ). The action-value function is defined as Q \u03c0 (x, a) = E [Z \u03c0 (x, a)],and can be characterized by the Bellman equationQ \u03c0 (x, a) = E [R(x, a)] + \u03b3E P,\u03c0 [Q \u03c0 (x , a )] .The objective in RL is to find an optimal policy \u03c0 * , which maximizes E[Z \u03c0 ], i.e. Q \u03c0 * (x, a) \u2265 Q \u03c0 (x, a) for all \u03c0 and all x, a. One approach is to find the unique fixed pointQ * = Q \u03c0 *of the Bellman optimality operator  #b6 :Q(x, a) = T Q(x, a) := E [R(x, a)] + \u03b3E P max a Q(x , a ).To this end, Q-learning  #b44  iteratively improves an estimate, Q \u03b8 , of the optimal action-value function, Q * , by repeatedly applying the Bellman update:Q \u03b8 (x, a) \u2190 E [R(x, a)] + \u03b3E P max a Q \u03b8 (x , a ) .The action-value function can be approximated by a parameterized function Q \u03b8 (e.g. a neural network), and trained by minimizing the squared temporal difference (TD) error,\u03b4 2 t = r t + \u03b3 max a \u2208A Q \u03b8 (x t+1 , a ) -Q \u03b8 (x t , a t ) 2 ,over samples (x t , a t , r t , x t+1 ) observed while following an -greedy policy over Q \u03b8 . This policy acts greedily with respect to Q \u03b8 with probability 1and uniformly at random otherwise. DQN  #b24  uses a convolutional neural network to parameterize Q \u03b8 and the Q-learning algorithm to achieve human-level play on the Atari-57 benchmark.",
        "Distributional RL": "In distributional RL, the distribution over returns (the law of Z \u03c0 ) is considered instead of the scalar value function Q \u03c0 that is its expectation. This change in perspective has yielded new insights into the dynamics of RL  #b2 , and been a useful tool for analysis  #b20 . Empirically, distributional RL algorithms show improved sample complexity and final performance, as well as increased robustness to hyperparameter variation  #b3 .An analogous distributional Bellman equation of the formZ \u03c0 (x, a) D = R(x, a) + \u03b3Z \u03c0 (X , A )can be derived, where A D = B denotes that two random variables A and B have equal probability laws, and the random variables X and A are distributed according to P (\u2022|x, a) and \u03c0(\u2022|x ), respectively. Morimura et al. (2010a) defined the distributional Bellman operator explicitly in terms of conditional probabilities, parameterized by the mean and scale of a Gaussian or Laplace distribution, and minimized the Kullback-Leibler (KL) divergence between the Bellman target and the current estimated return distribution. However, the distributional Bellman operator is not a contraction in the KL.As with the scalar setting, a distributional Bellman optimality operator can be defined byT Z(x, a) D := R(x, a) + \u03b3Z(X , arg max a \u2208A E Z(X , a )),with X distributed according to P (\u2022|x, a). While the distributional Bellman operator for policy evaluation is a contraction in the p-Wasserstein distance  #b5 , this no longer holds for the control case. Convergence to the optimal policy can still be established, but requires a more involved argument.  #b5  parameterize the return distribution as a categorical distribution over a fixed set of equidistant points and minimize the KL divergence to the projected distributional Bellman target. Their algorithm, C51, outperformed previous DQN variants on the Atari-57 benchmark. Subsequently,  #b15  combined C51 with enhancements such as prioritized experience replay  #b34 , n-step updates  #b36 , and the dueling architecture  #b43 , leading to the Rainbow agent, current state-of-the-art in Atari-57.The categorical parameterization, using the projected KL loss, has also been used in recent work to improve the critic of a policy gradient algorithm, D4PG, achieving significantly improved robustness and state-of-the-art performance across a variety of continuous control tasks  #b3 .",
        "p-Wasserstein Metric": "The p-Wasserstein metric, for p \u2208 [1, \u221e], plays a key role in recent results in distributional RL  #b5  #b9 . It has also been a topic of increasing interest in generative modeling  #b1  #b7  #b37 , because unlike the KL divergence, the Wasserstein metric inherently trades off approximate solutions with likelihoods.The p-Wasserstein distance is the L p metric on inverse cumulative distribution functions (c.d.f.), also known as quantile functions  #b28 . For random variables U and V with quantile functions F -1 U and F -1 V , respectively, the p-Wasserstein distance is given byW p (U, V ) = 1 0 |F -1 U (\u03c9) -F -1 V (\u03c9)| p d\u03c9 1/p .The class of optimal transport metrics express distances between distributions in terms of the minimal cost for transporting mass to make the two distributions identical. This cost is given in terms of some metric, c : X \u00d7 X \u2192 R \u22650 , on the underlying space X . The p-Wasserstein metric corresponds to c = L p . We are particularly interested in the Wasserstein metrics due to the predominant use of L p spaces in mean-value reinforcement learning.2.3. Quantile Regression for Distributional RL  #b5  showed that the distributional Bellman operator is a contraction in the p-Wasserstein metric, but as the proposed algorithm did not itself minimize the Wasserstein metric, this left a theory-practice gap for distributional RL. Recently, this gap was closed, in both directions. First and most relevant to this work,  #b9  proposed the use of quantile regression for distributional RL and showed that by choosing the quantile targets suitably the resulting projected distributional Bellman operator is a contraction in the \u221e-Wasserstein metric. Concurrently,  #b32  showed the original class of categorical algorithms are a contraction in the Cram\u00e9r distance, the L 2 metric on cumulative distribution functions.By estimating the quantile function at precisely chosen points, QR-DQN minimizes the Wasserstein distance to the distributional Bellman target  #b9 . This estimation uses quantile regression, which has been shown to converge to the true quantile function value when mini-mized using stochastic approximation  #b19 .In QR-DQN, the random return is approximated by a uniform mixture of N Diracs,Z \u03b8 (x, a) := 1 N N i=1 \u03b4 \u03b8i(x,a) ,with each \u03b8 i assigned a fixed quantile target, \u03c4i = \u03c4i-1+\u03c4i 2 for 1 \u2264 i \u2264 N , where \u03c4 i = i/N . These quantile estimates are trained using the Huber (1964) quantile regression loss, with threshold \u03ba,\u03c1 \u03ba \u03c4 (\u03b4 ij ) = |\u03c4 -I{\u03b4 ij < 0}| L \u03ba (\u03b4 ij ) \u03ba , with L \u03ba (\u03b4 ij ) = 1 2 \u03b4 2 ij , if |\u03b4 ij | \u2264 \u03ba \u03ba(|\u03b4 ij | -1 2 \u03ba), otherwise ,on the pairwise TD-errors\u03b4 ij = r + \u03b3\u03b8 j (x , \u03c0(x )) -\u03b8 i (x, a).At the time of this writing, QR-DQN achieves the best performance on Atari-57, human-normalized mean and median, of all agents that do not combine distributional RL, prioritized replay, and n-step updates  #b9  #b15  #b14 .",
        "Risk in Reinforcement Learning": "Distributional RL algorithms have been theoretically justified for the Wasserstein and Cram\u00e9r metrics  #b5  #b32 , and learning the distribution over returns, in and of itself, empirically results in significant improvements to data efficiency, final performance, and stability  #b5  #b9  #b14  #b3 . However, in each of these recent works the policy used was based entirely on the mean of the return distribution, just as in standard reinforcement learning. A natural question arises: can we expand the class of policies using information provided by the distribution over returns (i.e. to the class of risk-sensitive policies)? Furthermore, when would this larger policy class be beneficial?Here, 'risk' refers to the uncertainty over possible outcomes, and risk-sensitive policies are those which depend upon more than the mean of the outcomes. At this point, it is important to highlight the difference between intrinsic uncertainty, captured by the distribution over returns, and parametric uncertainty, the uncertainty over the value estimate typically associated with Bayesian approaches such as PSRL  #b30  and Kalman TD  #b12 . Distributional RL seeks to capture the former, which classic approaches to risk are built upon 1 .Expected utility theory states that if a decision policy is consistent with a particular set of four axioms regarding its choices then the decision policy behaves as though it is maximizing the expected value of some utility function U  #b40 ,\u03c0(x) = arg max a E Z(x,a) [U (z)].This is perhaps the most pervasive notion of risk-sensitivity.A policy maximizing a linear utility function is called riskneutral, whereas concave or convex utility functions give rise to risk-averse or risk-seeking policies, respectively.Many previous studies on risk-sensitive RL adopt the utility function approach  #b16  #b23  #b21 .A crucial axiom of expected utility is independence: given random variables X, Y and Z, such that X Y (X preferred over Y ), any mixture between X and Z is preferred to the same mixture between Y and Z (von Neumann &  #b40 . Stated in terms of the cumulative probability functions,\u03b1F X +(1-\u03b1)F Z \u2265 \u03b1F Y +(1-\u03b1)F Z , \u2200\u03b1 \u2208 [0, 1].This axiom in particular has troubled many researchers because it is consistently violated by human behavior  #b38 . The Allais paradox is a frequently used example of a decision problem where people violate the independence axiom of expected utility theory  #b0 .However, as  #b47  showed, this axiom can be replaced by one in terms of convex combinations of outcome values, instead of mixtures of distributions. Specifically, if as before X Y , then for any \u03b1 \u2208 [0, 1] and random variable Z, \u03b1F -1X + (1 -\u03b1)F -1 Z \u2265 \u03b1F -1 Y + (1 -\u03b1)F -1Z . This leads to an alternate, dual, theory of choice than that of expected utility. Under these axioms the decision policy behaves as though it is maximizing a distorted expectation, for some continuous monotonic function h:\u03c0(x) = arg max a \u221e -\u221e z \u2202 \u2202z (h \u2022 F Z(x,a) )(z) dz.Such a function h is known as a distortion risk measure, as it distorts the cumulative probabilities of the random variable  #b41 . That is, we have two fundamentally equivalent approaches to risk-sensitivity. Either, we choose a utility function and follow the expectation of this utility. Or, we choose a reweighting of the distribution and compute expectation under this distortion measure. Indeed,  #b47  further showed that these two functions are inverses of each other. The choice between them amounts to a choice 1 One exception is the recent work  #b25  towards combining both forms of uncertainty to improve exploration. over whether the behavior should be invariant to mixing with random events or to convex combinations of outcomes.Distortion risk measures include, as special cases, cumulative probability weighting used in cumulative prospect theory  #b38 , conditional value at risk  #b8 , and many other methods (Morimura et al., 2010b). Recently  #b22  argued for the use of distortion risk measures in robotics.",
        "Implicit Quantile Networks": "We now introduce the implicit quantile network (IQN), a deterministic parametric function trained to reparameterize samples from a base distribution, e.g. \u03c4 \u223c U ([0, 1]), to the respective quantile values of a target distribution. IQN provides an effective way to learn an implicit representation of the return distribution, yielding a powerful function approximator for a new DQN-like agent.Let F -1 Z (\u03c4 ) be the quantile function at \u03c4 \u2208 [0, 1] for the random variable Z. For notational simplicity we writeZ \u03c4 := F -1 Z (\u03c4 ), thus for \u03c4 \u223c U ([0, 1]) the resulting state- action return distribution sample is Z \u03c4 (x, a) \u223c Z(x, a).We propose to model the state-action quantile function as a mapping from state-actions and samples from some base distribution, typically \u03c4 \u223c U ([0, 1]), to Z \u03c4 (x, a), viewed as samples from the implicitly defined return distribution.Let \u03b2 : [0, 1] \u2192 [0, 1] be a distortion risk measure, with identity corresponding to risk-neutrality. Then, the distorted expectation of Z(x, a) under \u03b2 is given byQ \u03b2 (x, a) := E \u03c4 \u223cU ([0,1]) Z \u03b2(\u03c4 ) (x, a) .Notice that the distorted expectation is equal to the expected value of F -1 Z(x,a) weighted by \u03b2, that is,Q \u03b2 = 1 0 F -1 Z (\u03c4 )d\u03b2(\u03c4 ).The immediate implication of this is that for any \u03b2, there exists a sampling distribution for \u03c4 such that the mean of Z \u03c4 is equal to the distorted expectation of Z under \u03b2, that is, any distorted expectation can be represented as a weighted sum over the quantiles  #b10 . Denote by \u03c0 \u03b2 the risk-sensitive greedy policy\u03c0 \u03b2 (x) = arg max a\u2208A Q \u03b2 (x, a).(1)For two samples \u03c4, \u03c4 \u223c U ([0, 1]), and policy \u03c0 \u03b2 , the sampled temporal difference (TD) error at step t is\u03b4 \u03c4,\u03c4 t = r t + \u03b3Z \u03c4 (x t+1 , \u03c0 \u03b2 (x t+1 )) -Z \u03c4 (x t , a t ). (2)Then, the IQN loss function is given byL(x t , a t , r t , x t+1 ) = 1 N N i=1 N j=1 \u03c1 \u03ba \u03c4i \u03b4 \u03c4i,\u03c4 j t ,(3)where N and N denote the respective number of iid samples \u03c4 i , \u03c4 j \u223c U ([0, 1]) used to estimate the loss. A corresponding sample-based risk-sensitive policy is obtained by approximatingQ \u03b2 in Equation 1 by K samples of \u03c4 \u223c U ([0, 1]): \u03c0\u03b2 (x) = arg max a\u2208A 1 K K k=1 Z \u03b2(\u03c4 k ) (x, a).Implicit quantile networks differ from the approach of Dabney et al. ( 2018) in two ways. First, instead of approximating the quantile function at n fixed values of \u03c4 we approximate it with Z \u03c4 (x, a) \u2248 f (\u03c8(x), \u03c6(\u03c4 )) a for some differentiable functions f , \u03c8, and \u03c6. If we ignore the distributional interpretation for a moment and view each Z \u03c4 (x, a) as a separate action-value function, this highlights that implicit quantile networks are a type of universal value function approximator (UVFA)  #b33 . There may be additional benefits to implicit quantile networks beyond the obvious increase in representational fidelity. As with UVFAs, we might hope that training over many different \u03c4 's (goals in the case of the UVFA) leads to better generalization between values and improved sample complexity than attempting to train each separately.Second, \u03c4 , \u03c4 , and \u03c4 are sampled from continuous, independent, distributions. Besides U ([0, 1]), we also explore risk-sentive policies \u03c0 \u03b2 , with non-linear \u03b2. The independent sampling of each \u03c4 , \u03c4 results in the sample TD errors being decorrelated, and the estimated action-values go from being the true mean of a mixture of n Diracs to a sample mean of the implicit distribution defined by reparameterizing the sampling distribution via the learned quantile function.",
        "Implementation": "Consider the neural network structure used by the DQN agent  #b24 . Let \u03c8 : X \u2192 R d be the function computed by the convolutional layers and f : R d \u2192 R |A| the subsequent fully-connected layers mapping \u03c8(x) to the estimated action-values, such that Q(x, a) \u2248 f (\u03c8(x)) a . For our network we use the same functions \u03c8 and f as in DQN, but include an additional function \u03c6 : [0, 1] \u2192 R d computing an embedding for the sample point \u03c4 . We combine these to form the approximation Z \u03c4 (x, a) \u2248 f (\u03c8(x) \u03c6(\u03c4 )) a , where denotes the element-wise (Hadamard) product.As the network for f is not particularly deep, we use the multiplicative form, \u03c8 \u03c6, to force interaction between the convolutional features and the sample embedding. Alternative functional forms, e.g. concatenation or a 'residual' function \u03c8 (1 + \u03c6), are conceivable, and \u03c6(\u03c4 ) can be parameterized in different ways. To investigate these, we compared performance across a number of architectural variants on six Atari 2600 games (ASTERIX, ASSAULT, BREAKOUT, MS.PACMAN, QBERT, SPACE INVADERS).Full results are given in the Appendix. Despite minor variation in performance, we found the general approach to be robust to the various choices. Based upon the results we used the following function in our later experiments, for embedding dimension n = 64:\u03c6 j (\u03c4 ) := ReLU( n-1 i=0 cos(\u03c0i\u03c4 )w ij + b j ).(4)After settling on a network architecture, we study the effect of the number of samples, N and N , used in the estimate terms of Equation 3.We hypothesized that N , the number of samples of \u03c4 \u223c U ([0, 1]), would affect the sample complexity of IQN, with larger values leading to faster learning, and that with N = 1 one would potentially approach the performance of DQN. This would support the hypothesis that the improved performance of many distributional RL algorithms rests on their effect as auxiliary loss functions, which would vanish in the case of N = 1. Furthermore, we believed that N , the number of samples of \u03c4 \u223c U ([0, 1]), would affect the variance of the gradient estimates much like a mini-batch size hyperparameter. Our prediction was that N would have the greatest effect on variance of the long-term performance of the agent.We used the same set of six games as before, with our chosen architecture, and varied N, N \u2208 {1, 8, 32, 64}. In Figure 2 we report the average human-normalized scores on the six games for each configuration. affected performance very differently than expected: it had a strong effect on early performance, but minimal impact on long-term performance past N = 8.Overall, while using more samples for both distributions is generally favorable, N = N = 8 appears to be sufficient to achieve the majority of improvements offered by IQN for long-term performance, with variation past this point largely insignificant. To our surprise we found that even for N = N = 1, which is comparable to DQN in the number of loss components, the longer term performance is still quite strong (\u2248 3\u00d7 DQN).In an informal evaluation, we did not find IQN to be sensitive to K, the number of samples used for the policy, and have fixed it at K = 32 for all experiments.",
        "Risk-Sensitive Reinforcement Learning": "In this section, we explore the effects of varying the distortion risk measure, \u03b2, away from identity. This only affects the policy, \u03c0 \u03b2 , used both in Equation 2 and for acting in the environment. As we have argued, evaluating under different distortion risk measures is equivalent to changing the sampling distribution for \u03c4 , allowing us to achieve various forms of risk-sensitive policies. We focus on a handful of sampling distributions and their corresponding distortion measures. The first one is the cumulative probability weighting parameterization proposed in cumulative prospect theory  #b38  #b13 :CPW(\u03b7, \u03c4 ) = \u03c4 \u03b7 (\u03c4 \u03b7 + (1 -\u03c4 ) \u03b7 ) 1 \u03b7.In particular, we use the parameter value \u03b7 = 0.71 found by  #b46  to most closely match human subjects. This choice is interesting as, unlike the others we consider, it is neither globally convex nor concave. For small values of \u03c4 it is locally concave and for larger values of \u03c4 it becomes locally convex. Recall that concavity corresponds to risk-averse and convexity to risk-seeking policies.Second, we consider the distortion risk measure proposed by  #b42 , where \u03a6 and \u03a6 -1 are taken to be the standard Normal cumulative distribution function and its inverse:Wang(\u03b7, \u03c4 ) = \u03a6(\u03a6 -1 (\u03c4 ) + \u03b7).For \u03b7 < 0, this produces risk-averse policies and we include it due to its simple interpretation and ability to switch between risk-averse and risk-seeking distortions.Third, we consider a simple power formula for risk-averse (\u03b7 < 0) or risk-seeking (\u03b7 > 0) policies:Pow(\u03b7, \u03c4 ) = \u03c4 1 1+|\u03b7| , if \u03b7 \u2265 0 1 -(1 -\u03c4 ) 1 1+|\u03b7| ,otherwise .Finally, we consider conditional value-at-risk (CVaR):CVaR(\u03b7, \u03c4 ) = \u03b7\u03c4.CVaR has been widely studied in and out of reinforcement learning  #b8 . Its implementation as a modification to the sampling distribution of \u03c4 is particularly simple, as it changes\u03c4 \u223c U ([0, 1]) to \u03c4 \u223c U ([0, \u03b7]).Another interesting sampling distribution, not included in our experiments, is denoted Norm(\u03b7) and corresponds to \u03c4 sampled by averaging \u03b7 samples from U ([0, 1]).In Figure 3 (right) we give an example of a distribution (Neutral) and how each of these distortion measures affects the implied distribution due to changing the sampling distribution of \u03c4 . Norm(3) and CPW(.71) reduce the impact of the tails of the distribution, while Wang and CVaR heavily shift the distribution mass towards the tails, creating a riskaverse or risk-seeking preference. Additionally, while CVaR entirely ignores all values corresponding to \u03c4 > \u03b7, Wang gives these non-zero, but vanishingly small, probability.By using these sampling distributions we can induce various risk-sensitive policies in IQN. We evaluate these on the same set of six Atari 2600 games previously used. Our algorithm simply changes the policy to maximize the distorted expectations instead of the usual sample mean. Figure 3 (left) shows our results in this experiment, with average scores reported under the usual, risk-neutral, evaluation criterion.Intuitively, we expected to see a qualitative effect from risk-sensitive training, e.g. strengthened exploration from a risk-seeking objective. Although we did see qualitative differences, these did not always match our expectations. For two of the games, ASTERIX and ASSAULT, there is a very significant advantage to the risk-averse policies. Although CPW tends to perform almost identically to the standard risk-neutral policy, and the risk-seeking Wang(1.5) performs as well or worse than risk-neutral, we find that both risk-averse policies improve performance over standard IQN. However, we also observe that the more risk-averse of the two, CVaR(0.1), suffers some loss in performance on two other games (QBERT and SPACE INVADERS).Additionally, we note that the risk-seeking policy significantly underperforms the risk-neutral policy on three of the six games.It remains an open question as to exactly why we see improved performance for risk-averse policies.There are many possible explanations for this phenomenon, e.g. that risk-aversion encodes a heuristic to stay alive longer, which in many games is correlated with increased rewards.",
        "Full Atari-57 Results": "Finally, we evaluate IQN on the full Atari-57 benchmark, comparing with the state-of-the-art performance of Rainbow, a distributional RL agent that combines several advances in deep RL  #b15 , the closely related algorithm QR-DQN  #b9 , prioritized experience replay DQN  #b34 , and the original DQN agent  #b24 . Note that in this section we use the risk-neutral variant of the IQN, that is, the policy of the IQN agent is the regular -greedy policy with respect to the mean of the state-action return distribution.It is important to remember that Rainbow builds upon the distributional RL algorithm C51  #b5 , but also includes prioritized experience replay  #b34 , Double DQN (van Hasselt et al., 2016), Dueling Network architecture  #b43 , Noisy Networks  #b11 , and multi-step updates  #b36 . In particular, besides the distributional update, nstep updates and prioritized experience replay were found to have significant impact on the performance of Rainbow. Our other competitive baseline is QR-DQN, which is currently state-of-the-art for agents that do not combine distributional updates, n-step updates, and prioritized replay.Thus, between QR-DQN and the much more complex Rain-bow we compare to the two most closely related, and best performing, agents in published work. In particular, we would expect that IQN would benefit from the additional enhancements in Rainbow, just as Rainbow improved significantly over C51.Figure 4 shows the mean (left) and median (right) humannormalized scores during training over the Atari-57 benchmark. IQN dramatically improves over QR-DQN, which itself improves on many previously published results. At 100 million frames IQN has reached the same level of performance as QR-DQN at 200 million frames. Table 1 gives a comparison between the same methods in terms of their best, human-normalized, scores per game under the 30 random no-op start condition. These are averages over the given number of seeds. Additionally, using human-starts, IQN achieves 162% median human-normalized score, whereas Rainbow reaches 153%  #b15 , see   Finally, we took a closer look at the games in which each algorithm continues to underperform humans, and computed, on average, how far below human-level they perform 2 . We refer to this value as the human-gap 3 metric and give results in Table 1. Interestingly, C51 outperforms QR-DQN in this metric, and IQN outperforms all others. This shows that the remaining gap between Rainbow and IQN is entirely from games on which both algorithms are already super-human.The games where the most progress in RL is needed happen to be the games where IQN shows the greatest improvement over QR-DQN and Rainbow.",
        "Discussion and Conclusions": "We have proposed a generalization of recent work based around using quantile regression to learn the distribution over returns of the current policy. Our generalization leads to a simple change to the DQN agent to enable distributional RL, the natural integration of risk-sensitive policies, and significantly improved performance over existing methods. The IQN algorithm provides, for the first time, a fully integrated distributional RL agent without prior assumptions on the parameterization of the return distribution.IQN can be trained with as little as a single sample from each state-action value distribution, or as many as computational limits allow to improve the algorithm's data efficiency. Furthermore, IQN allows us to expand the class of control policies to a large class of risk-sensitive policies connected to distortion risk measures. Finally, we show substantial gains on the Atari-57 benchmark over QR-DQN, and even halving the distance between QR-DQN and Rainbow.Despite the significant empirical successes in this paper 2 Details of how this is computed can be found in the Appendix.3 Thanks to Joseph Modayil for proposing this metric.there are many areas in need of additional theoretical analysis. We highlight a few particularly relevant open questions we were unable to address in the present work. First, samplebased convergence results have been recently shown for a class of categorical distributional RL algorithms  #b32 . Could existing sample-based RL convergence results be extended to the QR-based algorithms?Second, can the contraction mapping results for a fixed grid of quantiles given by  #b9  be extended to the more general class of approximate quantile functions studied in this work? Finally, and particularly salient to our experiments with distortion risk measures, theoretical guarantees for risk-sensitive RL have been building over recent years, but have been largely limited to special cases and restricted classes of risk-sensitive policies. Can the convergence of the distribution of returns under the Bellman operator be leveraged to show convergence to a fixed-point in distorted expectations? In particular, can the control results of  #b5  be expanded to cover some class of risk-sensitive policies?There remain many intriguing directions for future research into distributional RL, even on purely empirical fronts. Hessel et al. ( 2018) recently showed that distributional RL agents can be significantly improved, when combined with other techniques. Creating a Rainbow-IQN agent could yield even greater improvements on Atari-57. We also recall the surprisingly rich return distributions found by  #b3 , and hypothesize that the continuous control setting may be a particularly fruitful area for the application of distributional RL in general, and IQN in particular.",
        "Appendix Architecture and Hyperparameters": "We considered multiple architectural variants for parameterizing an IQN. All of these build on the Q-network of a regular DQN  #b24 , which can be seen as the composition of a convolutional stack \u03c8 : X \u2192 R For the embedding \u03c6, we considered a number of variants: a learned linear embedding, a learned MLP embedding with a single hidden layer of size n, and a learned linear function of n cosine basis functions of the form cos(\u03c0i\u03c4 ), i = 1, . . . , n. Each of those was followed by either a ReLU or sigmoid nonlinearity.For the merging function m, the simplest choice would be a simple vector concatenation of \u03c8(x) and \u03c6(\u03c4 ). Note however, that the MLP f which takes in the output of m and outputs the action-value quantiles, only has a single hidden layer in the DQN network. Therefore, to force a sufficiently early interaction between the two representations, we also considered a multiplicative function m(\u03c8, \u03c6) = \u03c8 \u03c6, where denotes the element-wise (Hadamard) product of two vectors, as well as a 'residual' function m(\u03c8, \u03c6) = \u03c8 (1 + \u03c6).Early experiments showed that a simple linear embedding of \u03c4 was insufficient to achieve good performance, and the residual version of m didn't show any marked difference to the multiplicative variant, so we do not include results for these here. For the other configurations, Figure 5 shows pairwise comparisons between 1) a cosine basis function embedding and a completely learned MLP embedding, 2) an embedding size (hidden layer size or number of cosine basis elements) 32 and 64, 3) ReLU and sigmoid nonlinearity following the embedding, and 4) concatenation and a multiplicative interaction between \u03c8(x) and \u03c6(\u03c4 ).Each comparison 'violin plot' can be understood as a marginalization over the other variants of the architecture, with the human-normalized performance at the end of training, averaged across six Atari 2600 games, on the y-axis. Each white dot corresponds to a configuration (each represented by two seeds), the black dots show the position of our preferred configuration. The width of the colored regions corresponds to a kernel density estimate of the number of configurations at each performance level.Our final choice is a multiplicative interaction with a linear function of a cosine embedding, with n = 64 and a ReLU nonlinearity (see Equation 4), as this configuration yielded the highest performance consistently over multiple seeds. Also noteworthy is the overall robustness of the approach to these variations: most of the configurations consistently outperform the QR-DQN baseline shown as a grey horizontal line for comparison.We give pseudo-code for the IQN loss in Algorithm 1. All other hyperparameters for this agent correspond to the ones used by  #b9 . In particular, the Bellman target is computed using a target network. Notice that IQN will generally be more computationally expensive per-sample than QR-DQN. However, in practice IQN requires many fewer samples per update than QR-DQN so that the actual running times are comparable.Algorithm 1 Implicit Quantile Network Loss Require: N, N , K, \u03ba and functions \u03b2, Z input x, a, r, x , \u03b3 \u2208 [0, 1) # Compute greedy next action a * \u2190 arg max a1 K K k Z \u03c4k (x , a ), \u03c4k \u223c \u03b2(\u2022) # Sample quantile thresholds \u03c4 i , \u03c4 j \u223c U ([0, 1]), 1 \u2264 i \u2264 N, 1 \u2264 j \u2264 N # Compute distributional temporal differences \u03b4 ij \u2190 r + \u03b3Z \u03c4 j (x , a * ) -Z \u03c4i (x, a), \u2200i, j # Compute Huber quantile loss output N i=1 E \u03c4 \u03c1 \u03ba \u03c4i (\u03b4 ij )",
        "Evaluation": "The human-normalized scores reported in this paper are given by the formula  #b39  #b9  score = agentrandom humanrandom ,where agent, human and random are the per-game raw scores (undiscounted returns) for the given agent, a reference human player, and random agent baseline  #b24 .The 'human-gap' metric referred to at the end of Section 5 builds on the human-normalized score, but emphasizes the remaining improvement for the agent to reach super-human performance. It is given by gap = max(1score, 0), with a value of 1 corresponding to random play, and a value of 0 corresponding to super-human level of performance. To avoid degeneracies in the case of human < random, the quantity is being clipped above at 1. "
    },
    {},
    {
        "b0": [
            "Allais paradox",
            "",
            "",
            "",
            "Allais"
        ],
        "b1": [
            "Wasserstein Generative Adversarial Networks",
            "",
            "",
            "",
            "Arjovsky",
            "Chintala",
            "Bottou"
        ],
        "b2": [
            "On the sample complexity of reinforcement learning with a generative model",
            "",
            "",
            "",
            "Azar",
            "Munos",
            "Kappen"
        ],
        "b3": [
            "Distributional policy gradients",
            "",
            "",
            "",
            "Barth-Maron",
            "Hoffman",
            "Budden",
            "Dabney",
            "Horgan",
            "Tb",
            "Muldal",
            "Heess",
            "Lillicrap"
        ],
        "b4": [
            "The Arcade Learning Environment: an evaluation platform for general agents",
            "",
            "",
            "",
            "Bellemare",
            "Naddaf",
            "Veness",
            "Bowling"
        ],
        "b5": [
            "A distributional perspective on reinforcement learning",
            "",
            "",
            "",
            "Bellemare",
            "Dabney",
            "Munos"
        ],
        "b6": [
            "",
            "",
            "Dynamic Programming",
            ""
        ],
        "b7": [
            "",
            "",
            "From optimal transport to generative modeling: the vegan cookbook",
            ""
        ],
        "b8": [
            "Algorithms for CVaR optimization in MDPs",
            "",
            "",
            "",
            "Chow",
            "Ghavamzadeh"
        ],
        "b9": [
            "Distributional reinforcement learning with quantile regression",
            "",
            "",
            "",
            "Dabney",
            "Rowland",
            "Bellemare",
            "Munos"
        ],
        "b10": [
            "Remarks on quantiles and distortion risk measures",
            "",
            "",
            "",
            "Dhaene",
            "Kukush",
            "Linders",
            "Tang"
        ],
        "b11": [
            "",
            "",
            "Noisy networks for exploration",
            ""
        ],
        "b12": [
            "Kalman temporal differences",
            "",
            "",
            "",
            "Geist",
            "Pietquin"
        ],
        "b13": [
            "On the shape of the probability weighting function",
            "",
            "",
            "",
            "Gonzalez",
            "Wu"
        ],
        "b14": [
            "The Reactor: a fast and sampleefficient actor-critic agent for reinforcement learning",
            "",
            "",
            "",
            "Gruslys",
            "Dabney",
            "Azar",
            "Piot",
            "Bellemare",
            "Munos"
        ],
        "b15": [
            "Rainbow: combining improvements in deep reinforcement learning",
            "",
            "",
            "",
            "Hessel",
            "Modayil",
            "Van Hasselt",
            "Schaul",
            "Ostrovski",
            "Dabney",
            "Horgan",
            "Piot",
            "Azar",
            "Silver"
        ],
        "b16": [
            "Risk-sensitive markov decision processes",
            "",
            "",
            "",
            "Howard",
            "Matheson"
        ],
        "b17": [
            "Robust estimation of a location parameter",
            "",
            "",
            "",
            "Huber"
        ],
        "b18": [
            "Markov decision processes with a new optimality criterion: discrete time",
            "",
            "",
            "",
            "Jaquette"
        ],
        "b19": [
            "",
            "",
            "Quantile Regression",
            ""
        ],
        "b20": [
            "PAC bounds for discounted MDPs",
            "",
            "",
            "",
            "Lattimore",
            "Hutter"
        ],
        "b21": [
            "",
            "",
            "Particle value functions",
            ""
        ],
        "b22": [
            "How should a robot assess risk?",
            "",
            "",
            "",
            "Majumdar",
            "Pavone"
        ],
        "b23": [
            "Risk sensitive markov decision processes",
            "",
            "",
            "",
            "Marcus",
            "Fern\u00e1ndez-Gaucherand",
            "Hern\u00e1ndez-Hernandez",
            "Coraluppi",
            "Fard"
        ],
        "b24": [
            "Human-level control through deep reinforcement learning",
            "",
            "",
            "",
            "Mnih",
            "Kavukcuoglu",
            "Silver",
            "Rusu",
            "Veness",
            "Bellemare",
            "Graves",
            "Riedmiller",
            "Fidjeland",
            "Ostrovski"
        ],
        "b25": [
            "",
            "",
            "Efficient exploration with double uncertain value networks",
            ""
        ],
        "b26": [
            "Parametric return density estimation for reinforcement learning",
            "",
            "",
            "",
            "Morimura",
            "Hachiya",
            "Sugiyama",
            "Tanaka",
            "Kashima"
        ],
        "b27": [
            "Nonparametric return distribution approximation for reinforcement learning",
            "",
            "",
            "",
            "Morimura",
            "Sugiyama",
            "Kashima",
            "Hachiya",
            "Tanaka"
        ],
        "b28": [
            "Integral probability metrics and their generating classes of functions",
            "",
            "",
            "",
            "M\u00fcller"
        ],
        "b29": [
            "Massively parallel methods for deep reinforcement learning",
            "",
            "",
            "",
            "Nair",
            "Srinivasan",
            "Blackwell",
            "Alcicek",
            "Fearon",
            "De Maria",
            "Panneershelvam",
            "Suleyman",
            "Beattie",
            "Petersen"
        ],
        "b30": [
            "(more) efficient reinforcement learning via posterior sampling",
            "",
            "",
            "",
            "Osband",
            "Russo",
            "Van Roy"
        ],
        "b31": [
            "",
            "",
            "Decision Processes: Discrete Stochastic Dynamic Programming",
            ""
        ],
        "b32": [
            "An analysis of categorical distributional reinforcement learning",
            "",
            "",
            "",
            "Rowland",
            "Bellemare",
            "Dabney",
            "Munos",
            "Teh"
        ],
        "b33": [
            "Universal value function approximators",
            "",
            "",
            "",
            "Schaul",
            "Horgan",
            "Gregor",
            "Silver"
        ],
        "b34": [
            "Prioritized experience replay",
            "",
            "",
            "",
            "Schaul",
            "Quan",
            "Antonoglou",
            "Silver"
        ],
        "b35": [
            "The variance of discounted markov decision processes",
            "",
            "",
            "",
            "Sobel"
        ],
        "b36": [
            "Learning to predict by the methods of temporal differences",
            "",
            "",
            "",
            "Sutton"
        ],
        "b37": [
            "",
            "",
            "",
            ""
        ],
        "b38": [
            "Advances in prospect theory: cumulative representation of uncertainty",
            "",
            "",
            "",
            "Tversky",
            "Kahneman"
        ],
        "b39": [
            "Deep reinforcement learning with double Q-learning",
            "",
            "",
            "",
            "Van Hasselt",
            "Guez",
            "Silver"
        ],
        "b40": [
            "",
            "",
            "Theory of Games and Economic Behavior",
            ""
        ],
        "b41": [
            "Premium calculation by transforming the layer premium density",
            "",
            "",
            "",
            "Wang"
        ],
        "b42": [
            "A class of distortion operators for pricing financial and insurance risks",
            "",
            "",
            "",
            "Wang"
        ],
        "b43": [
            "Dueling network architectures for deep reinforcement learning",
            "",
            "",
            "",
            "Wang",
            "Schaul",
            "Hessel",
            "Van Hasselt",
            "Lanctot",
            "Freitas"
        ],
        "b44": [
            "",
            "",
            "Learning from delayed rewards",
            ""
        ],
        "b45": [
            "Mean, variance, and probabilistic criteria in finite markov decision processes: a review",
            "",
            "",
            "",
            "White"
        ],
        "b46": [
            "Curvature of the probability weighting function",
            "",
            "",
            "",
            "Wu",
            "Gonzalez"
        ],
        "b47": [
            "The dual theory of choice under risk. Econometrica",
            "",
            "",
            "",
            "Yaari"
        ],
        "b48": [
            "More than a million ways to be pushed. a high-fidelity experimental dataset of planar pushing",
            "",
            "",
            "",
            "Yu",
            "Bauza",
            "Fazeli",
            "Rodriguez"
        ]
    },
    {
        "tab_0": "Table 2 .2Mean Median Human Gap SeedsDQN228%79%0.3341PRIOR.434%124%0.1781C51701%178%0.1521RAINBOW 1189%230%0.1442QR-DQN864%193%0.1653IQN1019%218%0.1415",
        "tab_1": "Table 1 .1Mean and median of scores across 57 Atari 2600 games, measured as percentages of human baseline(Nair et al., 2015). Scores are averages over number of seeds.Human-starts (median)DQN PRIOR. A3CC51RAINBOWIQN68%128%116% 125%153%162%",
        "tab_2": "Table 2 .2Median human-normalized scores for human-starts.Implicit Quantile Networks for Distributional Reinforcement LearningMeanMedianHuman-Normalized ScoreDQN IQN Prioritized DQN QR-DQN RainbowTraining Frames (Million)Training Frames (Million)",
        "tab_3": "Complete Atari-57 training curves.Figure 7. Raw scores for a single seed across all games, starting with 30 no-op actions. Reference values from(Wang et al., 2016).RANDOM 227.8 5.8 222.4 210.0 719.1 47,388.7 HUMAN 7,127.7 1,719.5 742.0 8,503.3 12,850.0 29,028.1 279,987.0 DQN 1,620.0 978.0 4,280.4 4,359.0 1,364.5 14.2 753.1 455.0 2,360.0 37,187.5 29,900.0 363.9 16,926.5 8,627.5 123.7 2,630.4 585.6 23.1 160.7 50.4 0.1 12.1 88.0 1.7 30.5 385.5 2,090.9 12,017.0 4,657.7 811.0 7,387.8 6,126.0 10,780.5 35,829.4 110,763.0 2,874.5 18,688.9 23,633.0 152.1 1,971.0 12,149.4 -18.6 -16.4 -6.6 0.0 860.5 729.0 -91.7 -38.7 -4.9 0.0 29.6 30.8 65.2 4,334.7 797.4 257.6 2,412.5 8,777.4 173.0 3,351.4 473.0 1,027.0 30,826.4 20,437.8 -11.2 0.9 -1.9 29.0 302.8 768.5 52.0 3,035.0 7,259.0 1,598.0 2,665.5 8,422.3 258.5 22,736.3 26,059.0 0.0 4,753.3 0.0 307.3 6,951.6 3,085.6 2,292.3 8,049.0 8,207.8 761.4 7,242.6 8,485.2 -229.4 6,463.7 -286.1 -20.7 14.6 19.5 24.9 69,571.3 146.7 163.9 13,455.0 13,117.3 1,338.5 17,118.0 7,377.6 11.5 7,845.0 39,544.0 2.2 11.9 63.9 68.4 42,054.7 5,860.6 -17,098.1 -4,336.9 -13,062.3 1,236.3 12,326.7 3,482.8 148.0 1,668.7 1,692.3 664.0 10,250.0 54,282.0 -10.0 6.5 -5.6 -23.8 -8.3 12.2 3,568.0 5,229.2 4,870.0 11.4 167.6 68.1 533.4 11,693.2 9,989.9 0.0 1,187.5 163.0 16,256.9 17,667.9 196,760.4 563.5 4,756.5 2,704.0 3,092.9 54,576.9 18,098.9 Figure 6. GAMES Alien Amidar Assault Asterix Asteroids Atlantis Bank Heist Battle Zone Beam Rider Berzerk Bowling Boxing Breakout Centipede Chopper Command Crazy Climber Defender Demon Attack Double Dunk Enduro Fishing Derby Freeway Frostbite Gopher Gravitar H.E.R.O. Ice Hockey James Bond Kangaroo Krull Kung-Fu Master Montezumas Revenge Ms. Pac-Man Name This Game Phoenix Pitfall! Pong Private Eye Q*Bert River Raid Road Runner Robotank Seaquest Skiing Solaris Space Invaders Star Gunner Surround Tennis Time Pilot Tutankham Up and Down Venture Video Pinball Wizard Of Wor Yars Revenge Zaxxon 32.5 9,173.3 5,363.0IQN Rainbow PRIOR. DUEL. 3,941.0 2,296.8 11,477.0 375,080.0 1,192.7 395,762.0 1,503.1 35,520.0 30,276.5 3,409.0 46.7 98.9 366.0 7,687.5 13,185.0 162,224.0 41,324.5 72,878.6 -12.5 2,306.4 41.3 33.0 7,413.0 104,368.2 238.0 21,036.5 -0.4 812.0 1,792.0 10,374.4 48,375.0 0.0 3,327.3 15,572.5 70,324.3 0.0 20.9 206.0 18,760.3 20,607.6 62,151.0 27.5 931.6 -19,949.9 133.4 15,311.5 125,117.0 1.2 0.0 7,553.0 245.9 33,879.1 48.0 479,197.0 12,352.0 69,618.1 13,886.0DQN QR-DQN-1 QR-DQN 4,871 1,641 22,012 29,091 IQN 7,022 2,946 261,025 342,016 4,226 2,898 971,850 978,200 1,249 1,416 39,268 42,244 34,821 42,776 3,117 1,053 77.2 86.5 99.9 99.8 742 734 12,447 11,561 14,667 16,836 161,196 179,082 47,887 53,537 121,551 128,580 21.9 5.6 2,355 2,359 39.0 33.8 34.0 34.0 4,384 4,324 113,585 118,365 995 911 21,395 28,386 -1.7 0.2 4,703 35,108 15,356 15,487 11,447 10,707 76,642 73,512 0.0 0.0 5,821 6,349 21,890 22,682 16,585 56,599 0.0 0.0 21.0 21.0 350 200 572,510 25,750 17,571 17,765 64,262 57,900 59.4 62.5 8,268 30,140 -9,324 -9,289 6,740 8,007 20,972 28,888 77,495 74,677 8.2 9.4 23.6 23.6 10,345 12,236 297 293 71,260 88,148 43.9 1,318 705,662 698,045 25,061 31,190 26,447 28,379 13,112 21,772"
    }
]