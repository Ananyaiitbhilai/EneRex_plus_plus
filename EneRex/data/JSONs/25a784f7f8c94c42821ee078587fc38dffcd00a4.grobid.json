[
    {
        "basename": "25a784f7f8c94c42821ee078587fc38dffcd00a4.grobid",
        "fulltext": 24,
        "footnote_size": 2,
        "footnote_max": 2,
        "reference": 49,
        "authors": [
            "Zhang",
            "Shen",
            "Qiao",
            "Wang",
            "Wang",
            "Yuille"
        ]
    },
    {
        "title": "Robust Face Detection via Learning Small Faces on Hard Images",
        "abstract": "Recent anchor-based deep face detectors have achieved promising performance, but they are still struggling to detect hard faces, such as small, blurred and partially occluded faces. A reason is that they treat all images and faces equally, without putting more effort on hard ones; however, many training images only contain easy faces, which are less helpful to achieve better performance on hard images. In this paper, we propose that the robustness of a face detector against hard faces can be improved by learning small faces on hard images. Our intuitions are (1) hard images are the images which contain at least one hard face, thus they facilitate training robust face detectors; (2) most hard faces are small faces and other types of hard faces can be easily converted to small faces by shrinking. We build an anchor-based deep face detector, which only output a single feature map with small anchors, to specifically learn small faces and train it by a novel hard image mining strategy. Extensive experiments have been conducted on WIDER FACE, FDDB, Pascal Faces, and AFW datasets to show the effectiveness of our method. Our method achieves APs of 95.7, 94.9 and 89.7 on easy, medium and hard WIDER FACE val dataset respectively, which surpass the previous state-of-the-arts, especially on the hard subset. Code and model are available at https://github.com/bairdzhang/smallhardface.",
        "Introduction": "Face detection is a fundamental and important computer vision problem, which is critical for many face-related tasks, such as face alignment  #b2  #b36 , tracking  #b8  and recognition  #b18  #b23 . Stem from the recent successful development of deep neural networks, massive CNN-based face detection approaches  #b6  #b16  #b29  #b44  #b46  have been proposed and achieved the state-of-the-art performance. However, face detection remains a challenging task due to occlusion, illumination, makeup, as well as pose and scale variance, as shown in the benchmark dataset WIDER FACE  #b40 .Current state-of-the-art CNN-based face detectors at- tempt to address these challenges by employing more powerful backbone models  #b0 , exploiting feature pyramid-style architectures to combine features from multiple detection feature maps  #b29 , designing denser anchors  #b46  and utilizing larger contextual information  #b29 . These methods and techniques have been shown to be successful to build a robust face detector, and improve the performance towards human-level for most images.In spite of their success for most images, an evident performance gap still exists especially for those hard images which contain small, blurred and partially occluded faces. We realize that these hard images have become the main barriers for face detectors to achieve human-level detection performance. In Figure 1, we show that, even on the train set of WIDER FACE, the official pre-trained SSH 1 still fails on some of the images with extremely hard faces. We show two such hard training images in the upper right corner in Figure 1.On the other hand, most training images with easy faces can be almost perfectly detected (see the illustration in the right lower corner of Figure 1). As shown in left part of Figure 1, over two thirds of the training images already obtained perfect detection accuracy, which indicates that those easy images are less useful towards training a robust face detector. To address this issue, in this paper, we propose a robust face detector by putting more training focus on those hard images.This issue is most related to anchor-level hard example mining discussed in OHEM  #b25 . However, due to the sparsity of ground-truth faces and positive anchors, traditional anchor-level hard example mining mainly focuses on mining hard negative anchors, and mining hard anchors on well-detected images exhibits less effectiveness since there is no useful information that can be further exploited in these easy images. To address this issue, we propose to mine hard examples at image level in parallel with anchor level. More specifically, we propose to dynamically assign difficulty scores to training images during the learning process, which can determine whether an image is already well-detected or still useful for further training. This allows us to fully utilize the images which were not perfectly detected to better facilitate the following learning process. We show this strategy can make our detector more robust towards hard faces, without involving more complex network architecture and computation overhead.Apart from mining the hard images, we also propose to improve the detection quality by exclusively exploiting small faces. Small faces are typically hard and have attracted extensive research attention  #b0  #b6  #b46 . Existing methods aim at building a scale-invariant face detector to learn and infer on both small and big faces, with multiple levels of detection features and anchors of different sizes. Compared with these methods, our detector is more efficient since it is specially designed to aggressively leveraging the small faces during training. More specifically, large faces are automatically ignored during training due to our anchor design, so that the model can fully focus on the small hard faces. Additionally, experiments demonstrate that this design effectively achieves improvements on detecting all faces in spite of its simple and shallow architecture.To conclude, in this paper, we propose a novel face detector with the following contributions:\u2022 We propose a hard image mining strategy, to improve the robustness of our detector to those extremely hard faces. This is done without any extra modules, parameters or computation overhead added on the existing detector.\u2022 We design a single shot detector with only one detection feature map, which focuses on small faces with a specific range of sizes. This allows our model to be simple and focus on difficult small faces without struggling with scale variance.\u2022 Our face detector establishes state-of-the-art performance on all popular face detection datasets, including WIDER FACE, FDDB, Pascal Faces, and AFW. We achieve 95.7, 94.9 and 89.7 on easy, medium and hard WIDER FACE val dataset. Our method also achieves APs of 99.00 and 99.60 on Pascal Faces and AFW respectively, as well as a TPR of 98.7 on FDDB.The remainder of this paper is organized as follows. In Section 2, we discuss some studies have been done which are related to our paper. In Section 3, we dive into details of our proposed method, and we discuss experiment results and ablation experiments in Section 4. Finally, conclusions are drawn in Section 5.",
        "Related work": "Face detection has received extensive research attention  #b10  #b15  #b31 . With the emergence of modern CNN  #b5  #b9  #b26  and object detector  #b3  #b13  #b20  #b21  #b45 , there are many face detectors proposed to achieve promising performances  #b16  #b28  #b29  #b33  #b34  #b44 , by adapting general object detection framework into face detection domain. We briefly review hard example mining, face detection architecture, and anchor design & matching.",
        "Hard example mining": "Hard example mining is an important strategy to improve model quality, and has been studied extensively in image classification  #b14  and general object detection  #b12  #b25 . The main idea is to find some hard positive and hard negative examples at each step, and put more effort into training on those hard examples  #b22  #b30 . Recently, with modern detection frameworks proposed to boost the performance, OHEM  #b25  and Focal loss  #b12  have been proposed to select hard examples. OHEM computed the gradients of the networks by selecting the proposals with highest losses in every minibatch; while Focal loss aimed at naturally putting more focus on hard and misclassified examples by adding a factor to the standard cross entropy criterion. However, these algorithms mainly focused on anchor-level or proposal-level mining. It cannot handle the imbalance of easy and hard images in the dataset. In our paper, we propose to exploit hard example mining on image level, i.e. hard image mining, to improve the quality of face detector on extremely hard faces. More specifically, we assign difficulty scores to training images while training with an SGD mechanism, and re-sample the training images to build a new training subset at the next epoch.",
        "Face Detection Architecture": "Recent state-of-the-art face detectors are generally built based on Faster-RCNN  #b21 , R-FCN  #b3  or SSD  #b13 . SSH  #b16  exploited the RPN (Region Proposal Network) from Faster-RCNN to detect faces, by building three detection feature maps and designing six anchors with different sizes attached to the detection feature maps. S 3 FD  #b44  and PyramidBox  #b29 , on the other hand, adopted SSD as their detection architecture with six different detection feature The framework of our face detector. We take VGG16 as our backbone CNN, and we fuse two layers (conv4 3 and conv5 3) after dimension reduction and bilinear upsampling, to generate the final detection feature map. Based on that, we add a detection head for classification and bounding-box regression.maps. Different from S 3 FD, PyramidBox exploited a feature pyramid-style structure to combine features from different detection feature maps. Our proposed method, on the other hand, only builds single level detection feature map, based on VGG16, for classification and bounding-box regression, which is both simple and effective.",
        "Anchor design and matching": "Usually, anchors are designed to have different sizes to detect objects with different scales, in order to build a scaleinvariant detector. SSD as well as its follow-up detectors S 3 FD and PyramidBox, had six sets of anchors with different sizes, ranging from (16 \u00d7 16) to (512 \u00d7 512), and their network architectures had six levels of detection feature maps, with resolutions ranging from 1  4 to 1 128 , respectively. Similarly, SSH had the same anchor setting, and those anchors were attached to three levels of detection feature maps with resolutions ranging from 1  8 to 1 32 . The difference between SSH and S 3 DF is that in SSH, anchors with two neighboring sizes shared the same detection feature map, while in S 3 DF, anchors with different sizes are attached to different detection feature maps. SNIP  #b27  discussed an alternative approach to handle scales. It showed that CNNs are not robust to changes in scale, so training and testing on the same scales of an image pyramid can be a more optimal strategy. In our paper, we exploit this idea by limiting the anchor sizes to be (16\u00d716), (32 \u00d7 32) and (64 \u00d7 64). Then those faces with either too small or too big sizes will not be matched to any of the anchors, thus will be ignored during the training and testing. By removing those large anchors with sizes larger than (64 \u00d7 64), our network focuses more on small faces which are potentially more difficult. To deal with large faces, we use multiscale training and testing to resize them to match our anchors. Experiments show this design performs well on both small and big faces, although it has fewer detection feature maps and anchor sizes.",
        "Proposed method": "In this section, we introduce our proposed method for effective face detection. We first discuss the architecture of our detector in Section 3.1, then we elaborate our hard image mining strategy in Section 3.2, as well as some other useful training techniques in Section 3.3.",
        "Single-level small face detection framework": "The framework of our face detector is illustrated in Figure 2. We use VGG16 network as our backbone CNN, and combine conv4 3 and conv5 3 features, to build the detection feature map with both low-level and high-level semantic information. Similar to SSH  #b16 , we apply 1\u00d71 convolution layers after conv4 3 and conv5 3 to reduce dimension, and then apply a 3\u00d73 convolution layer on the concatenation of these two dimension reduced features. The output feature of the 3\u00d73 convolution layer is the final detection feature map, which will be fed into the detection head for classification and bounding-box regression.The detection feature map has a resolution of 1  8 of the original image (of size H \u00d7 W ). We attach three anchors at each point in the grid as default face detection boxes. Then we do classification and bounding-box regression on those 3 \u00d7 H 8 \u00d7 W 8 anchors. Unlike many other face detectors which build multiple feature maps to detect face with a variant range of scales, inspired by SNIP  #b27 , faces are trained and inferred with roughly the same scales. We only have one detection feature map, with three sets of anchors attached to it. The anchors have sizes of (16 \u00d7 16), (32 \u00d7 32) and (64 \u00d7 64), and the aspect ratio is set to be 1. By making this configuration, our network only trains and infers on small and medium size of faces; and we propose to handle large faces by shrinking the images in the test phase. We argue that there is no speed or accuracy degradation for large faces, since inferring on a tiny image (with short side containing 100 or 300 pixels) is very fast, and the shrinked large face will still have enough information to be recognized.To handle the difference of anchor sizes attached to the same detection feature map, we propose a detection head which uses different dilation rates for anchors with different sizes, as shown in Figure 3. The intuition is that in order to detect faces with different sizes, different effective receptive fields are required. This naturally requires the backbone feature map to be invariant to scales. To this end, we adopt different dilation rates for anchors with different sizes. For anchors with size (16 \u00d7 16), (32 \u00d7 32) and (64 \u00d7 64), we use a convolution with kernel size of 3 and dilation rate of 1, 2 and 4 to gather context features at different scales. These three convolution layers share weights to reduce the model size. With this design, the input of the 3 \u00d7 3 convolution, will be aligned to the same location of faces, regardless of the size of faces and anchors. Ablation experiments show the effectiveness of this multi-dilation design.",
        "Hard image mining": "Different from OHEM discussed in Section 3.3, which selects proposals or anchors with the highest losses, we propose a novel hard image mining strategy at image level. The intuition is that most images in the dataset are very easy, and we can achieve a very high AP even on the hard subset of the WIDER FACE val dataset with our baseline model.We believe not all training images should be treated equally, and well-recognized images will not help towards training a more robust face detector. To put more attention on training hard images instead of easy ones, we use a subset D of all training images D, to contain hard ones for training. At the beginning of each epoch, we build D based on the difficulty scores obtained in the previous epoch.We initially use all training images to train our model (i.e. D = D). This is due to the fact that our initial Im-ageNet pre-trained model will only give random guess towards face detection. In this case, there is no easy image. In other words, every image is considered as hard image and fed to the network for training at the first epoch. where A(I) + is the set of positive anchors for image I, with IoU over 0.5 against ground-truth boxes, l is the classification logit and l(I; \u0398) a,1 , l(I; \u0398) a,0 are the logits of anchor a for image I to be foreground face and background. All images are initially marked as hard, and any image with WPAS greater than a threshold th will be marked as easy image.At = [I ij i , I ij 2 , \u2022 \u2022 \u2022 , I ij k ],which focuses more on hard images, will be used for training at this epoch. Note that for multi-GPU training, each GPU will maintain its training list D independently. In our experiments, we set the probability p to be 0.7, and the threshold th to be 0.85.",
        "Training strategy": "",
        "Multi-scale training and anchor matching": "Since we only have anchors covering a limited range of face scales, we train our model by varying the sizes of training images. During the training phase, we resize the training images so that the short side of the image contains s pixels, where s is randomly selected from {400, 800, 1200}. We also set an upper bound of 2000 pixels to the long side of the image considering the GPU memory limitation.For each anchor, we assign a label {+1, 0, -1} based on how well it matches with any ground-truth face bounding box. If an anchor has an IoU (Intersection over Union) over 0.5 against a ground-truth face bounding box, we assign +1 to that anchor. On the other hand, if the IoU against any ground-truth face bounding box is lower than 0.3, we assign 0 to that anchor. All other anchors will be given -1 as the label, and thus will be ignored in the classification loss. By doing so, we only train on faces with designated scales. Those faces with no anchor matching will be simply ignored, since we do not assign the anchor with largest IoU to it (thus assign the corresponding anchor label +1) as Faster-RCNN does. This anchor matching strategy will ignore the large faces, and our model can put more capacity on learning different face patterns on hard small faces instead of memorizing the change in scales.For the regression loss, all anchors with IoU greater than 0.3 against ground-truth faces will be taken into account and contribute to the smooth 1 loss. We use a smaller threshold (i.e. 0.3) because (1) this will allow imperfectly matched anchors to be able to localize the face, which may be useful during the testing and (2) the regression task has less supervision since unlike classification, there are no negative anchors for computing loss and the positive anchors are usually sparse.",
        "Anchor-level hard example mining": "OHEM has been proven to be useful for object detection and face detection in  #b13  #b16  #b25 . During our training, in parallel with our newly proposed hard image mining, we also exploit the traditional hard anchor mining method to focus more on the hard and misclassificed anchors. Given a training image with size H \u00d7 W , there are 3 \u00d7 H 8 \u00d7 W 8 anchors at the detection head, and we only select 256 of them to be involved in computing the classification loss. For all positive anchors with IoU greater than 0.5 against ground-truth boxes, we select the top 64 of them with lowest confidences to be recognized as face. After selecting positive anchors, (256 -#pos anchor) negative anchors with highest face confidence are selected to compute the classification loss as the hard negative anchors. Note that we only perform OHEM for classification loss, and we keep all anchors with IoU greater than 0.3 for computing regression loss, without selecting a subset based on either classification loss or bounding-box regression loss.",
        "Data augmentation": "Data augmentation is extremely useful to make the model robust to light, scale changes and small shifts  #b13  #b29 . In our proposed method, we exploit cropping and photometric distortion as data augmentation. Given a training image after resizing, we crop a patch of it with a probability of 0.5. The patch has a height of H and a width of W which are independently drawn from U(0.6H, H) and U(0.6W, W ), where U is the uniform distribution and H, W are the height and width of the resized training image. All ground-truth boxes whose centers are located inside the patch are kept. After the random cropping, we apply photometric distortion following SSD by randomly modifying the brightness, contrast, saturation and hue of the cropped image randomly.",
        "Experiments": "To verify the effectiveness of our model and proposed method, we conduct extensive experiments on popular face detection datasets, including WIDER FACE  #b40 , FDDB  #b7 , Pascal Faces  #b37  and AFW  #b48 . It is worth noting that the training is only performed on the train set of WIDER FACE, and we use the same model for evaluation on all these datasets without further fine-tuning.",
        "Experimental settings": "We train our model on the train set of WIDER FACE, which has 12880 images with 159k faces annotated. We flip all images horizontally, to double the size of our training dataset to 25760. For each training image, we first randomly resize it, and then we use the cropping and photometric distortion data augmentation methods discussed in Section 3.3 to pre-process the resized image. We use an ImageNet pretrained VGG16  #b9  model to initialize our network backbone, and our newly introduced layers are randomly initialized with Gaussian initialization. We train the model with the itersize to be 2, for 46k iterations, with a learning rate of 0.004, and then for another 14k iterations with a smaller learning rate of 0.0004. During training, we use 4 GPUs to simultaneously to compute the gradient and update the weight by synchronized SGD with Momentum  #b19 . The first two blocks of VGG16 are frozen during the training, and the rest layers of VGG16 are set to have a double learning rate.Since our model is designed and trained on only small faces, we use a multiscale image pyramid for testing to deal with faces larger than our anchors. Specifically, we resize the testing image so that the short side contains 100, 300, 600, 1000 and 1400 pixels for evaluation on WIDER FACE dataset. We also follow the testing strategies used in Pyra-midBox  #b29  2 such as horizontal flip and bounding-box voting  #b4 . ",
        "Experiment results": "WIDER FACE dataset includes 3226 images and 39708 faces labelled in the val dataset, with three subsetseasy, medium and hard. In Figure 4, we show the precision-recall (PR) curve and average precision (AP) for our model compared with many other state-of-the-arts  #b0  #b1  #b16  #b17  #b29  #b32  #b33  #b34  #b35  #b38  #b39  #b40  #b41  #b42  #b43  #b44  #b46  #b47  on these three subsets. As we can see, our method achieves the best performance on the hard subset, and outperforms the current state-of-the-art by a large margin. Since the hard set is a super set of small and medium, which contains all faces taller than 10 pixels, the performance on hard set can represent the performance on the full testing dataset more accurately. Our performance on the medium subset is comparable to the most recent state-of-the-art and the performance on the easy subset is a bit worse since our method focuses on learning hard faces, and the architecture of our model is simpler compared with other state-of-thearts.There is also a WIDER FACE test dataset with no annotations provided publicly. It contains 16097 images, and is evaluated by WIDER FACE author team. We report the performance of our method at Figure 5 for the hard subset.FDDB dataset includes 5171 faces on a set of 2845 images, and we use our model trained on WIDER FACE train set to infer on the FDDB dataset. We use the raw boundingbox result without fitting it into ellipse to compute ROC. We show the discontinuous ROC curve at Figure 6a compared with  #b11  #b15  #b24  #b29  #b31  #b37  #b42  #b44  #b48 , and our method achieves the state-of-the-art performance of TPR=98.7% given 1000 false positives.Pascal Faces dataset includes 1335 labeled faces on a set of 851 images extracted for the Pascal VOC dataset. We show the PR curve at Figure 6b compared with  #b15  #b44 , and our method achieves a new the state-of-the-art performance of AP=99.0.AFW dataset includes 473 faces labelled in a set of 205 images. As shown in Figure 6c compared with  #b15  #b24  #b44  #b48 , our method achieves state-of-the-art and almost perfect performance, with an AP of 99.60.",
        "Ablation study and diagnosis": "",
        "Ablation experiments": "In order to verify the performance of our single level face detector, as well as the effectiveness of our proposed hard image mining, the dilated-head classification and regression structure, we conduct various ablation experiments on the WIDER FACE val dataset. All results are summarized in Table 1.From Table Baseline-Single is our proposed detector with single detection feature map shown in Figure 2. HIM and DH represents hard image mining (Subsection 3.2) and dilated head architecture (Figure 3).state-of-the-art face detector, especially on the hard subset.Our model with single detection feature map performs better than the one with three detection feature maps, despite its shallower structure, fewer parameters and anchors. This confirms the effectiveness of our simple face detector with single detection feature map focusing on small faces. We also separately verify our newly proposed hard image mining (HIM) and dilated head architecture (DH) described in Subsection 3.2 and Figure 3 respectively. HIM can improve the performance on hard subset significantly without involving more complex network architecture nor computation overhead. DH itself can also boost the performance, which shows the effectiveness of designing larger convolution for larger anchors. Combining HIM and DH together can improve further towards the state-of-the-art performance.",
        "Diagnosis of hard image mining": "We investigate the effects of our hard image mining mechanism. We show the ratio of |D | and |D -D | (i.e. the ratio of the number of selected training images to the number of  ignored training images) in Figure 7 for each epoch. We can see that at the first epoch, all training images are used to train the model. Meanwhile, as the training process continues, more and more training images will be ignored. At the last epoch, over a half images will be ignored and thus will not be included in D .",
        "Diagnosis of data augmentation": "We investigate the effectiveness of the photometric distortion as well as the cropping mechanisms as discussed in Subsection 3.3. The ablation results evaluated on WIDER FACE val dataset are shown in Table 2. Both photometric distortion and cropping can contribute to a more robust face detector.",
        "Diagnosis of multi-scale testing": "Our face detector with one detection feature map is design for small face detection, and our anchors are only capable of capturing faces with sizes ranging from ( (64 \u00d7 64). As a result, it is critical to adopt multi-scale testing to deal with large faces. Different from SSH, S 3 FD and PyramidBox, our testing pyramid includes some extreme small scales (i.e. short side contains only 100 or 300 pixels). In Table 3, we show the effectiveness of these extreme small scales to deal with easy and large images. Our full evaluation resizes the image so that the short side contains 100, 300, 600, 1000 and 1400 pixels respectively, to build an image pyramid. We diagnose the impact of the extra small scales (i.e. 100 and 300) by removing them from the image pyramid. As shown in Table 3, the extra small scales are crucial to detect easy faces. Without resizing the short side to contain 100 and 300 pixels, the performance on easy subset is only 78.2, which is even lower than the performance on medium and hard which contain much harder faces. We will show in the next subsection that these extra small scales (100 and 300) lead to negligible computation overhead, due to the lower resolution.",
        "Diagnosis of accuracy/speed trade-off": "We evaluate the speed of our method as well as some other popular face detectors in Table 4. For fair comparison, we run all methods on the same machine, with one Titan X (Maxwell) GPU, and Intel Core i7-4770K 3.50GHz. All methods except for PyramidBox are based on Caffe1 implementation, which is compiled with CUDA 9.0 and CUDNN 7. For PyramidBox, we follow the official fluid code and the default configurations 3 . We use the officially built Pad-dlePaddle with CUDA 9.0 and CUDNN 7 4 .For SSH, S 3 FD and Pyramid, we use the official inference code and configurations. For SSH, we use multi-scale testing with the short side containing 500, 800, 1200 and 1600 pixels, and for S 3 FD, we execute the official evaluation code with both multi-scale testing and horizontal flip. PyramidBox takes a similar testing configuration as S 3 FD. As shown in Table 4, our detector can outperform SSH, S 3 FD and PyramidBox significantly with a smaller inference time. Based on that, using horizontal flip can further improve the performance slightly. In terms of GPU memory usage, our method uses only a half of what PyramidBox occupies, while achieving better performance. Ours * in Table 4 indicates our method without extra small scales in inference, i.e., evaluated with scales [600, 1000, 1400]. It is only 6.5% faster than evaluation with [100, 300, 600, 1000, 1400] (1.59 compared with 1.70). This proves that although our face detector is only trained on small faces, it can perform well on large faces, by simply shrinking the testing image with negligible computation overhead.",
        "Conclusion": "To conclude, we propose a novel face detector to focus on learning small faces on hard images, which achieves the state-of-the-art performance on all popular face detection datasets. We propose a hard image mining strategy by dynamically assigning difficulty scores to training images, and re-sampling subsets with hard images for training before each epoch. We also design a single shot face detector with only one detection feature map, to train and test on small faces. With these designs, our model can put more attention on learning small hard faces instead of memorizing change of scales. Extensive experiments and ablations have been done to show the effectiveness of our method, and our face detector achieves the state-of-the-art performance on all popular face detection datasets, including WIDER FACE, FDDB, Pascal Faces and AFW. Our face detector also enjoys faster multi-scale inference speed and less GPU memory usage. Our proposed method are flexible and can be applied to other backbones and tasks, which we remain as future work."
    },
    {
        "1": "https://github.com/mahyarnajibi/SSH",
        "2": "https://github.com/PaddlePaddle/models/blob/develop/fluid/PaddleCV/ face detection/widerface eval.py"
    },
    {
        "b0": [
            "Finding tiny faces in the wild with generative adversarial network",
            "",
            "",
            "",
            "Bai",
            "Zhang",
            "Ding",
            "Ghanem"
        ],
        "b1": [
            "A unified multi-scale deep convolutional neural network for fast object detection",
            "",
            "",
            "",
            "Cai",
            "Fan",
            "Feris",
            "Vasconcelos"
        ],
        "b2": [
            "Face alignment by explicit shape regression",
            "",
            "",
            "",
            "Cao",
            "Wei",
            "Wen",
            "Sun"
        ],
        "b3": [
            "R-fcn: Object detection via region-based fully convolutional networks",
            "",
            "",
            "",
            "Dai",
            "Li",
            "He",
            "Sun"
        ],
        "b4": [
            "Object detection via a multiregion and semantic segmentation-aware cnn model",
            "",
            "",
            "",
            "Gidaris",
            "Komodakis"
        ],
        "b5": [
            "Deep residual learning for image recognition",
            "",
            "",
            "",
            "He",
            "Zhang",
            "Ren",
            "Sun"
        ],
        "b6": [
            "Finding tiny faces",
            "",
            "",
            "",
            "Hu",
            "Ramanan"
        ],
        "b7": [
            "",
            "",
            "Fddb: A benchmark for face detection in unconstrained settings",
            ""
        ],
        "b8": [
            "Face tracking and recognition with visual constraints in real-world videos",
            "",
            "",
            "",
            "Kim",
            "Kumar",
            "Pavlovic",
            "Rowley"
        ],
        "b9": [
            "Imagenet classification with deep convolutional neural networks",
            "",
            "",
            "",
            "Krizhevsky",
            "Sutskever",
            "Hinton"
        ],
        "b10": [
            "Efficient boosted exemplar-based face detection",
            "",
            "",
            "",
            "Li",
            "Lin",
            "Brandt",
            "Shen",
            "Hua"
        ],
        "b11": [
            "Learning surf cascade for fast and accurate object detection",
            "",
            "",
            "",
            "Li",
            "Zhang"
        ],
        "b12": [
            "Focal loss for dense object detection",
            "",
            "",
            "",
            "Lin",
            "Goyal",
            "Girshick",
            "He",
            "Doll\u00e1r"
        ],
        "b13": [
            "Ssd: Single shot multibox detector",
            "",
            "",
            "",
            "Liu",
            "Anguelov",
            "Erhan",
            "Szegedy",
            "Reed",
            "Fu",
            "Berg"
        ],
        "b14": [
            "",
            "",
            "Online batch selection for faster training of neural networks",
            ""
        ],
        "b15": [
            "Face detection without bells and whistles",
            "",
            "",
            "",
            "Mathias",
            "Benenson",
            "Pedersoli",
            "Van Gool"
        ],
        "b16": [
            "SSH: Single stage headless face detector",
            "",
            "",
            "",
            "Najibi",
            "Samangouei",
            "Chellappa",
            "Davis"
        ],
        "b17": [
            "To boost or not to boost? on the limits of boosted trees for object detection",
            "",
            "",
            "",
            "Ohn-Bar",
            "Trivedi"
        ],
        "b18": [
            "Deep face recognition",
            "",
            "",
            "",
            "Parkhi",
            "Vedaldi",
            "Zisserman"
        ],
        "b19": [
            "On the momentum term in gradient descent learning algorithms",
            "",
            "",
            "",
            "Qian"
        ],
        "b20": [
            "You only look once: Unified, real-time object detection",
            "",
            "",
            "",
            "Redmon",
            "Divvala",
            "Girshick",
            "Farhadi"
        ],
        "b21": [
            "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "",
            "",
            "",
            "Ren",
            "He",
            "Girshick",
            "Sun"
        ],
        "b22": [
            "Neural networkbased face detection",
            "",
            "",
            "",
            "Rowley",
            "Baluja",
            "Kanade"
        ],
        "b23": [
            "Facenet: A unified embedding for face recognition and clustering",
            "",
            "",
            "",
            "Schroff",
            "Kalenichenko",
            "Philbin"
        ],
        "b24": [
            "Detecting and aligning faces by image retrieval",
            "",
            "",
            "",
            "Shen",
            "Lin",
            "Brandt",
            "Wu"
        ],
        "b25": [
            "Training regionbased object detectors with online hard example mining",
            "",
            "",
            "",
            "Shrivastava",
            "Gupta",
            "Girshick"
        ],
        "b26": [
            "",
            "",
            "Very deep convolutional networks for large-scale image recognition",
            ""
        ],
        "b27": [
            "An analysis of scale invariance in object detection-snip",
            "",
            "",
            "",
            "Singh",
            "Davis"
        ],
        "b28": [
            "Multiple instance detection network with online instance classifier refinement",
            "",
            "",
            "",
            "Tang",
            "Wang",
            "Bai",
            "Liu"
        ],
        "b29": [
            "Pyramidbox: A context-assisted single shot face detector",
            "",
            "",
            "",
            "Tang",
            "Du",
            "He",
            "Liu"
        ],
        "b30": [
            "Rapid object detection using a boosted cascade of simple features",
            "",
            "",
            "",
            "Viola",
            "Jones"
        ],
        "b31": [
            "Robust real-time face detection",
            "",
            "",
            "",
            "Viola",
            "Jones"
        ],
        "b32": [
            "",
            "",
            "",
            ""
        ],
        "b33": [
            "",
            "",
            "Face attention network: An effective face detector for the occluded faces",
            ""
        ],
        "b34": [
            "",
            "",
            "Detecting faces using region-based fully convolutional networks",
            ""
        ],
        "b35": [
            "",
            "",
            "Detecting faces using region-based fully convolutional networks",
            ""
        ],
        "b36": [
            "Supervised descent method and its applications to face alignment",
            "",
            "",
            "",
            "Xiong",
            "De"
        ],
        "b37": [
            "",
            "",
            "Face detection by structural models. Image and Vision Computing",
            ""
        ],
        "b38": [
            "Aggregate channel features for multi-view face detection",
            "",
            "",
            "",
            "Yang",
            "Yan",
            "Lei",
            "Li"
        ],
        "b39": [
            "From facial parts responses to face detection: A deep learning approach",
            "",
            "",
            "",
            "Yang",
            "Luo",
            "Loy",
            "Tang"
        ],
        "b40": [
            "Wider face: A face detection benchmark",
            "",
            "",
            "",
            "Yang",
            "Luo",
            "Loy",
            "Tang"
        ],
        "b41": [
            "",
            "",
            "Face detection through scale-friendly deep convolutional networks",
            ""
        ],
        "b42": [
            "",
            "",
            "Face detection using improved faster rcnn",
            ""
        ],
        "b43": [
            "Joint face detection and alignment using multitask cascaded convolutional networks",
            "",
            "",
            "",
            "Zhang",
            "Zhang",
            "Li",
            "Qiao"
        ],
        "b44": [
            "S3 fd: Single shot scale-invariant face detector",
            "",
            "",
            "",
            "Zhang",
            "Zhu",
            "Lei",
            "Shi",
            "Wang",
            "Li"
        ],
        "b45": [
            "Single-shot object detection with enriched semantics",
            "",
            "",
            "",
            "Zhang",
            "Qiao",
            "Xie",
            "Shen",
            "Wang",
            "Yuille"
        ],
        "b46": [
            "Seeing small faces from robust anchors perspective",
            "",
            "",
            "",
            "Zhu",
            "Tao",
            "Luu",
            "Savvides"
        ],
        "b47": [
            "Cms-rcnn: contextual multi-scale region-based cnn for unconstrained face detection",
            "",
            "",
            "",
            "Zhu",
            "Zheng",
            "Luu",
            "Savvides"
        ],
        "b48": [
            "Face detection, pose estimation, and landmark localization in the wild",
            "",
            "",
            "",
            "Zhu",
            "Ramanan"
        ]
    },
    {
        "tab_0": "Table 1 :11, we can see that our single level baseline model can achieve performance comparable to the current Ablation experiments. Baseline-Three is a face detector similar to SSH with three detection feature maps.1.01.01.00.90.80.80.8True positive rate0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.70 250 500 750 1000 1250 1500 1750 2000 False positives Ours (0.987) PyramidBox (0.987) SFD (0.983) FD-CNN (0.926) HeadHunter (0.871) DPM (0.864) SquaresChnFtrs-5 (0.858) Structured Models (0.846) Olaworks (0.843) Face++ (0.839) SURF Cascade multiview (0.837) PEP-Adapt (0.809) XZJY (0.786) Shen et al. (0.777) TSM (0.766) Segui et al (0.761) Li et al (0.760) Illuxtech (0.718) Jain et al (0.677) Subburaman et al (0.630) Viola Jones (0.597) Mikolajczyk et al (0.548) Precision0.0 0.0 0.2 0.4 0.60.2 Ours (AP 99.00) 0.4 Recall SFD (AP 98.49) DPM (AP 90.29) Headhunter (AP 89.63) 0.6 SquaresChnFtrs-5 (AP 85.57) Face++0.81.0 Precision0.0 0.0 0.2 0.4 0.60.2 SFD (AP 99.85) 0.4 Recall Ours (AP 99.60) DPM (AP 97.21) Headhunter (AP 97.14) 0.6 SquaresChnFtrs-5 (AP 95.24) Shen et al. (AP 89.03) TSM (AP 87.99) Face.com Face++ Picasa0.81.0(a) FDDB(b) Pascal Faces(c) AFWFigure 6: Performance compared with state-of-the-arts on other face datasets.Methodeasy medium hardBaseline-Three 95.093.888.5Baseline-Single 95.194.289.1+ HIM95.494.889.6+ DH95.494.589.3+ DH + HIM95.794.989.7",
        "tab_2": "Table 2 :2Diagnosis of data augmentation. PD indicates photometric distortion. All entries are based on our Baseline-SingleLevel configuration without HIM and DH.16 \u00d7 16) to",
        "tab_3": "Table 3 :3Diagnosis of multi-scale testing.Testing Scaleseasy medium hard[600, 1000, 1400]78.285.786.1[300, 600, 1000, 1400]91.392.688.8[100, 300, 600, 1000, 1400] 95.794.989.7",
        "tab_4": "Table 4 :43 https://github.com/PaddlePaddle/models/blob/develop/fluid/PaddleCV/ face detection/widerface eval.py 4 pip install paddlepaddle-gpu Diagnosis of inference speed. MS and HF indicate multi-scale testing and horizontal flip; Time is the inference time (in second) for a single image; G-Mem is the GPU memory usage in gigabyte; AP-h is the average precision on the hard subset of WIDER FACE val set. Ours * indicates our detector without extra small scales. All entries are evaluated with a single nVIDIA Titan X (Maxwell).MethodMSHF Time G-Mem AP-hSSHYesNo 1.006.184.5S 3 FDYesYes 1.346.285.2PyramidBox YesYes 2.2411.988.9Ours  *Yes  *  Yes 1.595.386.1OursYesNo 0.845.389.3OursYesYes 1.705.389.7"
    }
]