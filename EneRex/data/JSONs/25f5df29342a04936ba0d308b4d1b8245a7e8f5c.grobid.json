[
    {
        "basename": "25f5df29342a04936ba0d308b4d1b8245a7e8f5c.grobid",
        "fulltext": 16,
        "footnote_size": 1,
        "footnote_max": 3,
        "reference": 44,
        "authors": [
            "Wei",
            "Ramakrishna"
        ]
    },
    {
        "title": "Convolutional Pose Machines",
        "abstract": "Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation. The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference. Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.",
        "Introduction": "We introduce Convolutional Pose Machines (CPMs) for the task of articulated pose estimation. CPMs inherit the benefits of the pose machine  #b28  architecture-the implicit learning of long-range dependencies between image and multi-part cues, tight integration between learning and inference, a modular sequential design-and combine them with the advantages afforded by convolutional architectures: the ability to learn feature representations for both image and spatial context directly from data; a differentiable architecture that allows for globally joint training with backpropagation; and the ability to efficiently handle large training datasets.CPMs consist of a sequence of convolutional networks that repeatedly produce 2D belief maps 1 for the location 1 We use the term belief in a slightly loose sense, however the belief  of each part. At each stage in a CPM, image features and the belief maps produced by the previous stage are used as input. The belief maps provide the subsequent stage an expressive non-parametric encoding of the spatial uncertainty of location for each part, allowing the CPM to learn rich image-dependent spatial models of the relationships between parts. Instead of explicitly parsing such belief maps either using graphical models  #b27  #b37  #b38  or specialized post-processing steps  #b37  #b39 , we learn convolutional networks that directly operate on intermediate belief maps and learn implicit image-dependent spatial models of the relationships between parts. The overall proposed multistage architecture is fully differentiable and therefore can be trained in an end-to-end fashion using backpropagation.At a particular stage in the CPM, the spatial context of part beliefs provide strong disambiguating cues to a subsequent stage. As a result, each stage of a CPM produces belief maps with increasingly refined estimates for the locations of each part (see Figure 1). In order to capture longrange interactions between parts, the design of the network in each stage of our sequential prediction framework is motivated by the goal of achieving a large receptive field on both the image and the belief maps. We find, through experiments, that large receptive fields on the belief maps are crucial for learning long range spatial relationships and remaps described are closely related to beliefs produced in message passing inference in graphical models. The overall architecture can be viewed as an unrolled mean-field message passing inference algorithm  #b30  that is learned end-to-end using backpropagation.sult in improved accuracy.Composing multiple convolutional networks in a CPM results in an overall network with many layers that is at risk of the problem of vanishing gradients  #b3  #b4  #b9  #b11  during learning. This problem can occur because backpropagated gradients diminish in strength as they are propagated through the many layers of the network. While there exists recent work 2 which shows that supervising very deep networks at intermediate layers aids in learning  #b19  #b35 , they have mostly been restricted to classification problems. In this work, we show how for a structured prediction problem such as pose estimation, CPMs naturally suggest a systematic framework that replenishes gradients and guides the network to produce increasingly accurate belief maps by enforcing intermediate supervision periodically through the network. We also discuss different training schemes of such a sequential prediction architecture.Our main contributions are (a) learning implicit spatial models via a sequential composition of convolutional architectures and (b) a systematic approach to designing and training such an architecture to learn both image features and image-dependent spatial models for structured prediction tasks, without the need for any graphical model style inference. We achieve state-of-the-art results on standard benchmarks including the MPII, LSP, and FLIC datasets, and analyze the effects of jointly training a multi-staged architecture with repeated intermediate supervision.",
        "Related Work": "The classical approach to articulated pose estimation is the pictorial structures model  #b1  #b2  #b8  #b13  #b25  #b26  #b29  #b42  in which spatial correlations between parts of the body are expressed as a tree-structured graphical model with kinematic priors that couple connected limbs. These methods have been successful on images where all the limbs of the person are visible, but are prone to characteristic errors such as double-counting image evidence, which occur because of correlations between variables that are not captured by a tree-structured model. The work of Kiefel et al.  #b16  is based on the pictorial structures model but differs in the underlying graph representation. Hierarchical models  #b34  #b36  represent the relationships between parts at different scales and sizes in a hierarchical tree structure. The underlying assumption of these models is that larger parts (that correspond to full limbs instead of joints) can often have discriminative image structure that can be easier to detect and consequently help reason about the location of smaller, harder-to-detect parts. Non-tree models  #b7  #b15  #b18  #b32  #b41  incorporate interactions that introduce loops to augment the tree structure with additional edges that capture symmetry, occlusion and long-range relation- 2 New results have shown that using skip connections with identity mappings  #b10  in so-called residual units also aids in addressing vanishing gradients in \"very deep\" networks. We view this method as complementary and it can be noted that our modular architecture easily allows us to replace each stage with the appropriate residual network equivalent.ships. These methods usually have to rely on approximate inference during both learning and at test time, and therefore have to trade off accurate modeling of spatial relationships with models that allow efficient inference, often with a simple parametric form to allow for fast inference. In contrast, methods based on a sequential prediction framework  #b28  learn an implicit spatial model with potentially complex interactions between variables by directly training an inference procedure, as in  #b21  #b24  #b30  #b40 .There has been a recent surge of interest in models that employ convolutional architectures for the task of articulated pose estimation  #b5  #b6  #b22  #b23  #b27  #b37  #b38 . Toshev et al.  #b39  take the approach of directly regressing the Cartesian coordinates using a standard convolutional architecture  #b17 . Recent work regresses image to confidence maps, and resort to graphical models, which require hand-designed energy functions or heuristic initialization of spatial probability priors, to remove outliers on the regressed confidence maps. Some of them also utilize a dedicated network module for precision refinement  #b27  #b37 . In this work, we show the regressed confidence maps are suitable to be inputted to further convolutional networks with large receptive fields to learn implicit spatial dependencies without the use of hand designed priors, and achieve state-of-the-art performance over all precision region without careful initialization and dedicated precision refinement. Pfister et al.  #b23  also used a network module with large receptive field to capture implicit spatial models. Due to the differentiable nature of convolutions, our model can be globally trained, where Tompson et al.  #b38  and Steward et al.  #b33  also discussed the benefit of joint training.Carreira et al.  #b5  train a deep network that iteratively improves part detections using error feedback but use a cartesian representation as in  #b39  which does not preserve spatial uncertainty and results in lower accuracy in the highprecision regime. In this work, we show how the sequential prediction framework takes advantage of the preserved uncertainty in the confidence maps to encode the rich spatial context, with enforcing the intermediate local supervisions to address the problem of vanishing gradients.",
        "Method": "",
        "Pose Machines": "We denote the pixel location of the p-th anatomical landmark (which we refer to as a part), Y p \u2208 Z \u2282 R 2 , where Z is the set of all (u, v) locations in an image. Our goal is to predict the image locations Y = (Y 1 , . . . , Y P ) for all P parts. A pose machine  #b28  (see Figure 2a and2b) consists of a sequence of multi-class predictors, g t (\u2022), that are trained to predict the location of each part in each level of the hierarchy. In each stage t \u2208 {1 . . . T }, the classifiers g t predict beliefs for assigning a location to each part Y p = z, \u2200z \u2208 Z, based on features extracted from the image at the location z denoted by x z \u2208 R d and contextual information from the preceding classifier in the neighbor- hood around each Y p in stage t. A classifier in the first stage t = 1, therefore produces the following belief values:9\u21e59 C 1\u21e51 C 1\u21e51 C 1\u21e51 C 1\u21e51 C 11\u21e511 C 11\u21e511 C Loss Loss f1 f2 (c) Stage 1 Input Image h\u21e5w\u21e53 Input Image h\u21e5w\u21e53 9\u21e59 C 9\u21e59 C 9\u21e59 C 2\u21e5 P 2\u21e5 P 5\u21e55 C 2\u21e5 P 9\u21e59 C 9\u21e59 C 9\u21e59 C 2\u21e5 P 2\u21e5 P 5\u21e55 C 2\u21e5 P 11\u21e511 C (e) E\u21b5ective Receptive Field x x 0 g 1 g 2 g T b 1 b 2 b T 2 T (a)g 1 (x z ) \u2192 {b p 1 (Y p = z)} p\u2208{0...P } ,(1)where b p 1 (Y p = z) is the score predicted by the classifier g 1 for assigning the p th part in the first stage at image location z. We represent all the beliefs of part p evaluated at every location z = (u, v) T in the image as b p t \u2208 R w\u00d7h , where w and h are the width and height of the image, respectively. That is,b p t [u, v] = b p t (Y p = z).(2)For convenience, we denote the collection of belief maps for all the parts as b t \u2208 R w\u00d7h\u00d7(P +1) (P parts plus one for background).In subsequent stages, the classifier predicts a belief for assigning a location to each part Y p = z, \u2200z \u2208 Z, based on (1) features of the image data x t z \u2208 R d again, and (2) contextual information from the preceeding classifier in the neighborhood around each Y p :g t (x z , \u03c8 t (z, b t-1 )) \u2192 {b p t (Y p = z)} p\u2208{0...P +1} ,(3)where \u03c8 t>1 (\u2022) is a mapping from the beliefs b t-1 to context features. In each stage, the computed beliefs provide an increasingly refined estimate for the location of each part. Note that we allow image features x z for subsequent stage to be different from the image feature used in the first stage x. The pose machine proposed in  #b28  used boosted random forests for prediction ({g t }), fixed hand-crafted image features across all stages (x = x), and fixed hand-crafted context feature maps (\u03c8 t (\u2022)) to capture spatial context across all stages.",
        "Convolutional Pose Machines": "We show how the prediction and image feature computation modules of a pose machine can be replaced by a deep convolutional architecture allowing for both image and contextual feature representations to be learned directly from data. Convolutional architectures also have the advantage of being completely differentiable, thereby enabling endto-end joint training of all stages of a CPM. We describe our design for a CPM that combines the advantages of deep convolutional architectures with the implicit spatial modeling afforded by the pose machine framework.",
        "Keypoint Localization Using Local Image Evidence": "The first stage of a convolutional pose machine predicts part beliefs from only local image evidence. Figure 2c shows the network structure used for part detection from local image evidence using a deep convolutional network. The evidence is local because the receptive field of the first stage of the network is constrained to a small patch around the output pixel location. We use a network structure composed of five convolutional layers followed by two 1 \u00d7 1 convolutional layers which results in a fully convolutional archi- tecture  #b20 . In practice, to achieve certain precision, we normalize input cropped images to size 368 \u00d7 368 (see Section 4.2 for details), and the receptive field of the network shown above is 160 \u00d7 160 pixels. The network can effectively be viewed as sliding a deep network across an image and regressing from the local image evidence in each 160 \u00d7 160 image patch to a P + 1 sized output vector that represents a score for each part at that image location.",
        "Sequential Prediction with Learned Spatial Context Features": "While the detection rate on landmarks with consistent appearance, such as the head and shoulders, can be favorable, the accuracies are often much lower for landmarks lower down the kinematic chain of the human skeleton due to their large variance in configuration and appearance. The landscape of the belief maps around a part location, albeit noisy, can, however, be very informative. Illustrated in Figure 3, when detecting challenging parts such as right elbow, the belief map for right shoulder with a sharp peak can be used as a strong cue. A predictor in subsequent stages (g t>1 ) can use the spatial context (\u03c8 t>1 (\u2022)) of the noisy belief maps in a region around the image location z and improve its predictions by leveraging the fact that parts occur in consistent geometric configurations. In the second stage of a pose machine, the classifier g 2 accepts as input the image features x 2 z and features computed on the beliefs via the feature function \u03c8 for each of the parts in the previous stage. The feature function \u03c8 serves to encode the landscape of the belief maps from the previous stage in a spatial region around the location z of the different parts. For a convolutional pose machine, we do not have an explicit function that computes context features. Instead, we define \u03c8 as being the receptive field of the predictor on the beliefs from the previous stage.The design of the network is guided by achieving a receptive field at the output layer of the second stage network that is large enough to allow the learning of potentially complex and long-range correlations between parts. By simply supplying features on the outputs of the previous stage (as opposed to specifying potential functions in a graphical model), the convolutional layers in the subsequent stage allow the classifier to freely combine contextual information by picking the most predictive features. The belief maps from the first stage are generated from a network that examined the image locally with a small receptive field. In the second stage, we design a network that drastically increases the equivalent receptive field. Large receptive fields can be achieved either by pooling at the expense of precision, increasing the kernel size of the convolutional filters at the expense of increasing the number of parameters, or by increasing the number of convolutional layers at the risk of encountering vanishing gradients during training. Our network design and corresponding receptive field for the subsequent stages (t \u2265 2) is shown in Figure 2d. We choose to use multiple convolutional layers to achieve large receptive field on the 8\u00d7 downscaled heatmaps, as it allows us to be parsimonious with respect to the number of parameters of the model. We found that our stride-8 network performs as well as a stride-4 one even at high precision region, while it makes us easier to achieve larger receptive fields. We also repeat similar structure for image feature maps to make the spatial context be image-dependent and allow error correction, following the structure of pose machine. We find that accuracy improves with the size of the receptive field. In Figure 4 we show the improvement in accuracy on the FLIC dataset  #b31  as the size of the receptive field on the original image is varied by varying the architecture without significantly changing the number of parameters, through a series of experimental trials on input images normalized to a size of 304 \u00d7 304. We see that the accuracy improves as the effective receptive field increases, and starts to saturate around 250 pixels, which also happens to be roughly the size of the normalized object. This improvement in accuracy with receptive field size suggests that the network does indeed encode long range interactions between parts and that doing so is beneficial. In our best performing setting in Figure 2, we normalize cropped images into a larger size of 368 \u00d7 368 pixels for better precision, and the receptive field of the second stage output on the belief maps of the first stage is set to 31 \u00d7 31, which is equivalently 400 \u00d7 400 pixels on the original image, where the radius can usually cover any pair of the parts. With more  Gradient (\u00d7 10 -3 ) stages, the effective receptive field is even larger. In the following section we show our results from up to 6 stages.",
        "Supervision Supervision Supervision": "",
        "Histograms of Gradient Magnitude During Training": "Figure",
        "Learning in Convolutional Pose Machines": "The design described above for a pose machine results in a deep architecture that can have a large number of layers. Training such a network with many layers can be prone to the problem of vanishing gradients  #b3  #b4  #b9  where, as observed by Bradley  #b4  and Bengio et al.  #b9 , the magnitude of back-propagated gradients decreases in strength with the number of intermediate layers between the output layer and the input layer.Fortunately, the sequential prediction framework of the pose machine provides a natural approach to training our deep architecture that addresses this problem. Each stage of the pose machine is trained to repeatedly produce the belief maps for the locations of each of the parts. We encourage the network to repeatedly arrive at such a representation by defining a loss function at the output of each stage t that minimizes the l 2 distance between the predicted and ideal belief maps for each part. The ideal belief map for a part p is written as b p * (Y p = z), which are created by putting Gaussian peaks at ground truth locations of each body part p. The cost function we aim to minimize at the output of each stage at each level is therefore given by:f t = P +1 p=1 z\u2208Z b p t (z) -b p * (z) 2 2 . (4)The overall objective for the full architecture is obtained by adding the losses at each stage and is given by:F = T t=1 f t .(5)We use standard stochastic gradient descend to jointly train all the T stages in the network. To share the image feature x across all subsequent stages, we share the weights of corresponding convolutional layers (see Figure 2) across stages t \u2265 2.",
        "Evaluation": "",
        "Analysis": "Addressing vanishing gradients. The objective in Equation 5 describes a decomposable loss function that operates on different parts of the network (see Figure 2). Specifically, each term in the summation is applied to the network after each stage t effectively enforcing supervision in intermediate stages through the network. Intermediate supervision has the advantage that, even though the full architecture can have many layers, it does not fall prey to the vanishing gradient problem as the intermediate loss functions replenish the gradients at each stage. We verify this claim by observing histograms of gradient magnitude (see Figure 5) at different depths in the architecture across training epochs for models with and without intermediate supervision. In early epochs, as we move from the output layer to the input layer, we observe on the model without intermediate supervision, the gradient distribution is tightly peaked around zero because of vanishing gradients. The model with intermediate supervision has a much  larger variance across all layers, suggesting that learning is indeed occurring in all the layers thanks to intermediate supervision. We also notice that as training progresses, the variance in the gradient magnitude distributions decreases pointing to model convergence.Benefit of end-to-end learning. We see in Figure 6a that replacing the modules of a pose machine with the appropriately designed convolutional architecture provides a large boost of 42.4 percentage points over the previous approach of  #b28  in the high precision regime (PCK@0.1) and 30.9 percentage points in the low precision regime (PCK@0.2).Comparison on training schemes. We compare different variants of training the network in Figure 6b on the LSP dataset with person-centric (PC) annotations. To demonstrate the benefit of intermediate supervision with joint training across stages, we train the model in four ways: (i) training from scratch using a global loss function that enforces intermediate supervision (ii) stage-wise; where each stage is trained in a feed-forward fashion and stacked (iii) as same as (i) but initialized with weights from (ii), and (iv) as same as (i) but with no intermediate supervision. We find that network (i) outperforms all other training methods, showing that intermediate supervision and joint training across stage is indeed crucial in achieving good performance. The stagewise training in (ii) saturate at suboptimal, and the jointly fine-tuning in (iii) improves from this sub-optimal to the accuracy level closed to (i), however with effectively longer training iterations.Performance across stages. We show a comparison of performance across each stage on the LSP dataset (PC) in Fig- ure 6c. We show that the performance increases monotonically until 5 stages, as the predictors in subsequent stages make use of contextual information in a large receptive field on the previous stage beliefs maps to resolve confusions between parts and background. We see diminishing returns at the 6th stage, which is the number we choose for reporting our best results in this paper for LSP and MPII datasets.",
        "Datasets and Quantitative Analysis": "In this section we present our numerical results in various standard benchmarks including the MPII, LSP, and FLIC datasets. To have normalized input samples of 368 \u00d7 368 for training, we first resize the images to roughly make the samples into the same scale, and then crop or pad the image according to the center positions and rough scale estimations provided in the datasets if available. In datasets such as LSP without these information, we estimate them according to joint positions or image sizes. For testing, we perform similar resizing and cropping (or padding), but estimate center position and scale only from image sizes when necessary. In addition, we merge the belief maps from different scales (perturbed around the given one) for final predictions, to handle the inaccuracy of the given scale estimation.We define and implement our model using the Caffe [13] libraries for deep learning. We publicly release the source code and details on the architecture, learning parameters, design decisions and data augmentation to ensure full reproducibility. 3MPII Human Pose Dataset. We show in Figure 8 our results on the MPII Human Pose dataset  #b0  which consists more than 28000 training samples. We choose to randomly augment the data with rotation degrees in [-40 \u2022 , 40 \u2022 ], scaling with factors in [0.7, 1.3], and horizonal flipping. The evaluation is based on PCKh metric  #b0  where the error tolerance is normalized with respect to head size of the target. Because there often are multiple people in the proximity of the interested person (rough center position is given in the dataset), we made two sets of ideal belief maps for training: one includes all the peaks for every person appearing in the proximity of the primary subject and the second type where we only place peaks for the primary subject. We supply the first set of belief maps to the loss layers in the first stage as the initial stage only relies on local image evidence to make predictions. We supply the second type of belief maps to theLeft Right t = 1 t = 2 t = 3 Wrists t = 1 t = 2 t = 3 Elbows t = 3 t = 1 t = 2 t = 1 t = 2 t = 3Wrists Elbows (a) (b)  loss layers of all subsequent stages. We also find that supplying to all subsequent stages an additional heat-map with a Gaussian peak indicating center of the primary subject is beneficial.Our total PCKh-0.5 score achieves state of the art at 87.95% (88.52% when adding LSP training data), which is 6.11% higher than the closest competitor, and it is noteworthy that on the ankle (the most challenging part), our PCKh-0.5 score is 78.28% (79.41% when adding LSP training data), which is 10.76% higher than the closest competitor. This result shows the capability of our model to capture long distance context given ankles are the farthest parts from head and other more recognizable parts. Figure 11 shows our accuracy is also consistently significantly higher than other methods across various view angles defined in  #b0 , especially in those challenging non-frontal views. In sum-mary, our method improves the accuracy in all parts, over all precisions, across all view angles, and is the first one achieving such high accuracy without any pre-training from other data, or post-inference parsing with hand-design priors or initialization of such a structured prediction task as in  #b27  #b38 . Our methods also does not need another module dedicated to location refinement as in  #b37  to achieve great high-precision accuracy with a stride-8 network.Leeds Sports Pose (LSP) Dataset. We evaluate our method on the Extended Leeds Sports Dataset  #b14  that consists of 11000 images for training and 1000 images for testing. We trained on person-centric (PC) annotations and evaluate our method using the Percentage Correct Keypoints (PCK) metric  #b43 . Using the same augmentation scheme as for the MPI dataset, our model again achieves state of the art at 84.32% (90.5% when adding MPII train-MPII FLIC LSP Figure 10: Qualitative results of our method on the MPII, LSP and FLIC datasets respectively. We see that the method is able to handle non-standard poses and resolve ambiguities between symmetric parts for a variety of different relative camera views.  ing data). Note that adding MPII data here significantly boosts our performance, due to its labeling quality being much better than LSP. Because of the noisy label in the LSP dataset, Pishchulin et al.  #b27  reproduced the dataset with original high resolution images and better labeling quality.FLIC Dataset. We evaluate our method on the FLIC Dataset  #b31  which consists of 3987 images for training and 1016 images for testing. We report accuracy as per the metric introduced in Sapp et al.  #b31  for the elbow and wrist joints in Figure 12. Again, we outperform all prior art at PCK@0.2 with 97.59% on elbows and 95.03% on wrists. In higher precision region our advantage is even more significant: 14.8 percentage points on wrists and 12.7 percentage points on elbows at PCK@0.05, and 8.9 percentage points on wrists and 9.3 percentage points on elbows at PCK@0.1. ",
        "Discussion": "Convolutional pose machines provide an end-to-end architecture for tackling structured prediction problems in computer vision without the need for graphical-model style inference. We showed that a sequential architecture composed of convolutional networks is capable of implicitly learning a spatial models for pose by communicating increasingly refined uncertainty-preserving beliefs between stages. Problems with spatial dependencies between variables arise in multiple domains of computer vision such as semantic image labeling, single image depth prediction and object detection and future work will involve extending our architecture to these problems. Our approach achieves state of the art accuracy on all primary benchmarks, however we do observe failure cases mainly when multiple people are in close proximity. Handling multiple people in a single end-to-end architecture is also a challenging problem and an interesting avenue for future work."
    },
    {
        "3": "https://github.com/CMU-Perceptual-Computing-Lab/ convolutional-pose-machines-release"
    },
    {
        "b0": [
            "human pose estimation: New benchmark and state of the art analysis",
            "",
            "",
            "",
            "Andriluka",
            "Pishchulin",
            "Gehler",
            "Schiele"
        ],
        "b1": [
            "Pictorial structures revisited: People detection and articulated pose estimation",
            "",
            "",
            "",
            "Andriluka",
            "Roth",
            "Schiele"
        ],
        "b2": [
            "Monocular 3D pose estimation and tracking by detection",
            "",
            "",
            "",
            "Andriluka",
            "Roth",
            "Schiele"
        ],
        "b3": [
            "Learning long-term dependencies with gradient descent is difficult",
            "",
            "",
            "",
            "Bengio",
            "Simard",
            "Frasconi"
        ],
        "b4": [
            "",
            "",
            "Learning In Modular Systems",
            ""
        ],
        "b5": [
            "",
            "",
            "Human pose estimation with iterative error feedback",
            ""
        ],
        "b6": [
            "Articulated pose estimation by a graphical model with image dependent pairwise relations",
            "",
            "",
            "",
            "Chen",
            "Yuille"
        ],
        "b7": [
            "Human pose estimation using body parts dependent joint regressors",
            "",
            "",
            "",
            "Dantone",
            "Gall",
            "Leistner",
            "Van Gool"
        ],
        "b8": [
            "Pictorial structures for object recognition",
            "",
            "",
            "",
            "Felzenszwalb",
            "Huttenlocher"
        ],
        "b9": [
            "Understanding the difficulty of training deep feedforward neural networks",
            "",
            "",
            "",
            "Glorot",
            "Bengio"
        ],
        "b10": [
            "",
            "",
            "Deep residual learning for image recognition",
            ""
        ],
        "b11": [
            "",
            "",
            "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies. A Field Guide to Dynamical Recurrent Neural Networks",
            ""
        ],
        "b12": [
            "",
            "",
            "Caffe: Convolutional architecture for fast feature embedding",
            ""
        ],
        "b13": [
            "Clustered pose and nonlinear appearance models for human pose estimation",
            "",
            "",
            "",
            "Johnson",
            "Everingham"
        ],
        "b14": [
            "Learning effective human pose estimation from inaccurate annotation",
            "",
            "",
            "",
            "Johnson",
            "Everingham"
        ],
        "b15": [
            "Using linking features in learning non-parametric part models",
            "",
            "",
            "",
            "Karlinsky",
            "Ullman"
        ],
        "b16": [
            "Human pose estimation with fields of parts",
            "",
            "",
            "",
            "Kiefel",
            "Gehler"
        ],
        "b17": [
            "Imagenet classification with deep convolutional neural networks",
            "",
            "",
            "",
            "Krizhevsky",
            "Sutskever",
            "Hinton"
        ],
        "b18": [
            "Beyond trees: Common-factor models for 2D human pose recovery",
            "",
            "",
            "",
            "Lan",
            "Huttenlocher"
        ],
        "b19": [
            "Deeplysupervised nets",
            "",
            "",
            "",
            "Lee",
            "Xie",
            "Gallagher",
            "Zhang",
            "Tu"
        ],
        "b20": [
            "Fully convolutional networks for semantic segmentation",
            "",
            "",
            "",
            "Long",
            "Shelhamer",
            "Darrell"
        ],
        "b21": [
            "Stacked hierarchical labeling",
            "",
            "",
            "",
            "Munoz",
            "Bagnell",
            "Hebert"
        ],
        "b22": [
            "Multi-source deep learning for human pose estimation",
            "",
            "",
            "",
            "Ouyang",
            "Chu",
            "Wang"
        ],
        "b23": [
            "Flowing convnets for human pose estimation in videos",
            "",
            "",
            "",
            "Pfister",
            "Charles",
            "Zisserman"
        ],
        "b24": [
            "Recurrent convolutional neural networks for scene labeling",
            "",
            "",
            "",
            "Pinheiro",
            "Collobert"
        ],
        "b25": [
            "Poselet conditioned pictorial structures",
            "",
            "",
            "",
            "Pishchulin",
            "Andriluka",
            "Gehler",
            "Schiele"
        ],
        "b26": [
            "Strong appearance and expressive spatial models for human pose estimation",
            "",
            "",
            "",
            "Pishchulin",
            "Andriluka",
            "Gehler",
            "Schiele"
        ],
        "b27": [
            "",
            "",
            "Deepcut: Joint subset partition and labeling for multi person pose estimation",
            ""
        ],
        "b28": [
            "Pose Machines: Articulated Pose Estimation via Inference Machines",
            "",
            "",
            "",
            "Ramakrishna",
            "Munoz",
            "Hebert",
            "Bagnell",
            "Sheikh"
        ],
        "b29": [
            "Strike a Pose: Tracking people by finding stylized poses",
            "",
            "",
            "",
            "Ramanan",
            "Forsyth",
            "Zisserman"
        ],
        "b30": [
            "Learning message-passing inference machines for structured prediction",
            "",
            "",
            "",
            "Ross",
            "Munoz",
            "Hebert",
            "Bagnell"
        ],
        "b31": [
            "MODEC: Multimodal Decomposable Models for Human Pose Estimation",
            "",
            "",
            "",
            "Sapp",
            "Taskar"
        ],
        "b32": [
            "Measure locally, reason globally: Occlusion-sensitive articulated pose estimation",
            "",
            "",
            "",
            "Sigal",
            "Black"
        ],
        "b33": [
            "",
            "",
            "End-to-end people detection in crowded scenes",
            ""
        ],
        "b34": [
            "Articulated part-based model for joint object detection and pose estimation",
            "",
            "",
            "",
            "Sun",
            "Savarese"
        ],
        "b35": [
            "",
            "",
            "Going deeper with convolutions",
            ""
        ],
        "b36": [
            "Exploring the spatial hierarchy of mixture models for human pose estimation",
            "",
            "",
            "",
            "Tian",
            "Zitnick",
            "Narasimhan"
        ],
        "b37": [
            "Efficient object localization using convolutional networks",
            "",
            "",
            "",
            "Tompson",
            "Goroshin",
            "Jain",
            "Lecun",
            "Bregler"
        ],
        "b38": [
            "Joint training of a convolutional network and a graphical model for human pose estimation",
            "",
            "",
            "",
            "Tompson",
            "Jain",
            "Lecun",
            "Bregler"
        ],
        "b39": [
            "DeepPose: Human pose estimation via deep neural networks",
            "",
            "",
            "",
            "Toshev",
            "Szegedy"
        ],
        "b40": [
            "Auto-context and its application to highlevel vision tasks and 3d brain image segmentation",
            "",
            "",
            "",
            "Tu",
            "Bai"
        ],
        "b41": [
            "Multiple tree models for occlusion and spatial constraints in human pose estimation",
            "",
            "",
            "",
            "Wang",
            "Mori"
        ],
        "b42": [
            "Articulated pose estimation with flexible mixtures-of-parts",
            "",
            "",
            "",
            "Yang",
            "Ramanan"
        ],
        "b43": [
            "Articulated human detection with flexible mixtures of parts",
            "",
            "",
            "",
            "Yang",
            "Ramanan"
        ]
    },
    {
        "tab_1": "5: Intermediate supervision addresses vanishing gradients. We track the change in magnitude of gradients in layers at different depths in the network, across training epochs, for models with and without intermediate supervision. We observe that for layers closer to the output, the distribution has a large variance for both with and without intermediate supervision; however as we move from the output layer towards the input, the gradient magnitude distribution peaks tightly around zero with low variance (the gradients vanish) for the model without intermediate supervision. For the model with intermediate supervision the distribution has a moderately large variance throughout the network. At later training epochs, the variances decrease for all layers for the model with intermediate supervision and remain tightly peaked around zero for the model without intermediate supervision. (Best viewed in color)",
        "tab_2": "Quantitative results on the LSP dataset using the PCK metric. Our method again achieves state of the art performance and has a significant advantage on challenging parts.PCK hip, LSP PCPCK wrist & elbow, LSP PCPCK knee, LSP PCPCK ankle, LSP PCDetection rate %00.050.10.150.200.050.10.150.200.050.10.150.200.050.10.150.2Normalized distanceNormalized distanceNormalized distanceNormalized distanceOurs 6-Stage + MPIOurs 6-StagePishchulin CVPR'16 (relabel) + MPITompson NIPS'14Chen NIPS'14Wang CVPR'13Figure 9:"
    }
]