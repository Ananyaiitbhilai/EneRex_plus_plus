[
    {
        "basename": "27aa0f3ec934925265f93fac7ff1cd1d70ceb618.grobid",
        "fulltext": 15,
        "footnote_size": 3,
        "footnote_max": 5,
        "reference": 55,
        "authors": [
            "Ruder",
            "Plank"
        ]
    },
    {
        "title": "Strong Baselines for Neural Semi-Supervised Learning under Domain Shift",
        "abstract": "Novel neural models have been proposed in recent years for learning under domain shift. Most models, however, only evaluate on a single task, on proprietary datasets, or compare to weak baselines, which makes comparison of models difficult. In this paper, we re-evaluate classic general-purpose bootstrapping approaches in the context of neural networks under domain shifts vs. recent neural approaches and propose a novel multi-task tri-training method that reduces the time and space complexity of classic tri-training. Extensive experiments on two benchmarks are negative: while our novel method establishes a new state-of-the-art for sentiment analysis, it does not fare consistently the best. More importantly, we arrive at the somewhat surprising conclusion that classic tri-training, with some additions, outperforms the state of the art. We conclude that classic approaches constitute an important and strong baseline.",
        "Introduction": "Deep neural networks (DNNs) excel at learning from labeled data and have achieved state of the art in a wide array of supervised NLP tasks such as dependency parsing  #b10 , named entity recognition  #b23 , and semantic role labeling  #b17 .In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. This is common in many real-world applications where the distribution of the training and test data differs. Many state-of-the-art domain adaptation approaches leverage task-specific characteristics such as sentiment words  #b3  #b48  or distributional features (Schn-abel and Sch\u00fctze, 2014; #b50  which do not generalize to other tasks. Other approaches that are in theory more general only evaluate on proprietary datasets (Kim et al., 2017) or on a single benchmark  #b51 , which carries the risk of overfitting to the task. In addition, most models only compare against weak baselines and, strikingly, almost none considers evaluating against approaches from the extensive semi-supervised learning (SSL) literature  #b6 .In this work, we make the argument that such algorithms make strong baselines for any task in line with recent efforts highlighting the usefulness of classic approaches  #b30  #b9 . We re-evaluate bootstrapping algorithms in the context of DNNs. These are general-purpose semi-supervised algorithms that treat the model as a black box and can thus be used easily-with a few additions-with the current generation of NLP models. Many of these methods, though, were originally developed with in-domain performance in mind, so their effectiveness in a domain adaptation setting remains unexplored.In particular, we re-evaluate three traditional bootstrapping methods, self-training  #b49 , tri-training  #b52 , and tritraining with disagreement  #b42  for neural network-based approaches on two NLP tasks with different characteristics, namely, a sequence prediction and a classification task (POS tagging and sentiment analysis). We evaluate the methods across multiple domains on two wellestablished benchmarks, without taking any further task-specific measures, and compare to the best results published in the literature.We make the somewhat surprising observation that classic tri-training outperforms task-agnostic state-of-the-art semi-supervised learning  #b22  and recent neural adaptation approaches  #b14  #b39 .In addition, we propose multi-task tri-training, which reduces the main deficiency of tri-training, namely its time and space complexity. It establishes a new state of the art on unsupervised domain adaptation for sentiment analysis but it is outperformed by classic tri-training for POS tagging.Contributions Our contributions are: a) We propose a novel multi-task tri-training method. b) We show that tri-training can serve as a strong and robust semi-supervised learning baseline for the current generation of NLP models. c) We perform an extensive evaluation of bootstrapping 1 algorithms compared to state-of-the-art approaches on two benchmark datasets. d) We shed light on the task and data characteristics that yield the best performance for each model.",
        "Neural bootstrapping methods": "We first introduce three classic bootstrapping methods, self-training, tri-training, and tri-training with disagreement and detail how they can be used with neural networks. For in-depth details we refer the reader to  #b0  #b6  #b54 . We introduce our novel multitask tri-training method in \u00a72.3.",
        "Self-training": "Self-training  #b49 McClosky et al., 2006b) is one of the earliest and simplest bootstrapping approaches. In essence, it leverages the model's own predictions on unlabeled data to obtain additional information that can be used during training. Typically the most confident predictions are taken at face value, as detailed next.Self-training trains a model m on a labeled training set L and an unlabeled data set U . At each iteration, the model provides predictions m(x) in the form of a probability distribution over classes for all unlabeled examples x in U . If the probability assigned to the most likely class is higher than a predetermined threshold \u03c4 , x is added to the labeled examples with p(x) = arg max m(x) as pseudo-label. This instantiation is the most widely used and shown in Algorithm 1.Calibration It is well-known that output probabilities in neural networks are poorly calibrated  #b16 . Using a fixed threshold \u03c4 is thus 1 We use the term bootstrapping as used in the semisupervised learning literature  #b53 , which should not be confused with the statistical procedure of the same name  #b11 .Algorithm 1 Self-training  #b0  1: repeat2: m \u2190 train_model(L) 3: for x \u2208 U do 4: if max m(x) > \u03c4 then 5: L \u2190 L \u222a {(x, p(x))}6: until no more predictions are confident not the best choice. While the absolute confidence value is inaccurate, we can expect that the relative order of confidences is more robust.For this reason, we select the top n unlabeled examples that have been predicted with the highest confidence after every epoch and add them to the labeled data. This is one of the many variants for self-training, called throttling  #b0 . We empirically confirm that this outperforms the classic selection in our experiments.Online learning In contrast to many classic algorithms, DNNs are trained online by default. We compare training setups and find that training until convergence on labeled data and then training until convergence using self-training performs best.Classic self-training has shown mixed success. In parsing it proved successful only with small datasets  #b36  or when a generative component is used together with a reranker in high-data conditions (McClosky et al., 2006b; #b46 . Some success was achieved with careful task-specific data selection  #b33 , while others report limited success on a variety of NLP tasks  #b34  #b47  #b15 . Its main downside is that the model is not able to correct its own mistakes and errors are amplified, an effect that is increased under domain shift.",
        "Tri-training": "Tri-training  #b52  is a classic method that reduces the bias of predictions on unlabeled data by utilizing the agreement of three independently trained models. Tri-training (cf. Algorithm 2) first trains three models m 1 , m 2 , and m 3 on bootstrap samples of the labeled data L. An unlabeled data point is added to the training set of a model m i if the other two models m j and m k agree on its label. Training stops when the classifiers do not change anymore.Tri-training with disagreement  #b42  Algorithm 2 Tri-training  #b52  1: for i \u2208 {1..3} do 2:S i \u2190 bootstrap_sample(L)3:m i \u2190 train_model(S i ) 4: repeat 5:for i \u2208 {1..3} do 6:L i \u2190 \u2205 7: for x \u2208 U do 8: if p j (x) = p k (x)(j, k = i) then 9: L i \u2190 L i \u222a {(x, p j (x))} m i \u2190 train_model(L \u222a L i )10: until none of m i changes 11: apply majority vote over m i is based on the intuition that a model should only be strengthened in its weak points and that the labeled data should not be skewed by easy data points. In order to achieve this, it adds a simple modification to the original algorithm (altering line 8 in Algorithm 2), requiring that for an unlabeled data point on which m j and m k agree, the other model m i disagrees on the prediction. Tri-training with disagreement is more data-efficient than tritraining and has achieved competitive results on part-of-speech tagging  #b42 .Sampling unlabeled data Both tri-training and tri-training with disagreement can be very expensive in their original formulation as they require to produce predictions for each of the three models on all unlabeled data samples, which can be in the millions in realistic applications. We thus propose to sample a number of unlabeled examples at every epoch. For all traditional bootstrapping approaches we sample 10k candidate instances in each epoch. For the neural approaches we use a linearly growing candidate sampling scheme proposed by  #b39 , increasing the candidate pool size as the models become more accurate.Confidence thresholding Similar to selftraining, we can introduce an additional requirement that pseudo-labeled examples are only added if the probability of the prediction of at least one model is higher than some threshold \u03c4 . We did not find this to outperform prediction without threshold for traditional tri-training, but thresholding proved essential for our method ( \u00a72.3).The most important condition for tri-training and tri-training with disagreement is that the models are diverse. Typically, bootstrap samples are used to create this diversity  #b52  #b42 . However, training separate models on bootstrap samples of a potentially large amount of training data is expensive and takes a lot of time. This drawback motivates our approach.",
        "Multi-task tri-training": "In order to reduce both the time and space complexity of tri-training, we propose Multi-task Tritraining (MT-Tri). MT-Tri leverages insights from multi-task learning (MTL)  #b5  to share knowledge across models and accelerate training. Rather than storing and training each model separately, we propose to share the parameters of the models and train them jointly using MTL.2 All models thus collaborate on learning a joint representation, which improves convergence.The output softmax layers are model-specific and are only updated for the input of the respective model. We show the model in Figure 1 (as instantiated for POS tagging). As the models leverage a joint representation, we need to ensure that the features used for prediction in the softmax layers of the different models are as diverse as possible, so that the models can still learn from each other's predictions. In contrast, if the parameters in all output softmax layers were the same, the method would degenerate to self-training.To guarantee diversity, we introduce an orthogonality constraint  #b4  as an additional loss term, which we define as follows:L orth = W m 1 W m 2 2 F (1)where | \u2022 2 F is the squared Frobenius norm and W m 1 and W m 2 are the softmax output parameters of the two source and pseudo-labeled output layers m 1 and m 2 , respectively. The orthogonality constraint encourages the models not to rely on the same features for prediction. As enforcing pairwise orthogonality between three matrices is not possible, we only enforce orthogonality between the softmax output layers of m 1 and m 2 , 3 while m 3 is gradually trained to be more target-specific. We parameterize L orth by \u03b3=0.01 following  #b24 . We do not further tune \u03b3.More formally, let us illustrate the model by taking the sequence prediction task (Figure 1) as illustration. Given an utterance with labels y 1 , .., y n , our Multi-task Tri-training loss consists of three task-specific (m 1 , m 2 , m 3 ) tagging loss functions (where h is the uppermost Bi-LSTM encoding):L(\u03b8) = - i 1,..,n log P m i (y| h) + \u03b3L orth (2)In contrast to classic tri-training, we can train the multi-task model with its three model-specific outputs jointly and without bootstrap sampling on the labeled source domain data until convergence, as the orthogonality constraint enforces different representations between models m 1 and m 2 . From this point, we can leverage the pair-wise agreement of two output layers to add pseudo-labeled examples as training data to the third model. We train the third output layer m 3 only on pseudo-labeled target instances in order to make tri-training more robust to a domain shift. For the final prediction, majority voting of all three output layers is used, which resulted in the best instantiation, together with confidence thresholding (\u03c4 = 0.9, except for highresource POS where \u03c4 = 0.8 performed slightly better). We also experimented with using a domainadversarial loss  #b14  on the jointly learned representation, but found this not to help. The full pseudo-code is given in Algorithm 3.",
        "Computational complexity": "The motivation for MT-Tri was to reduce the space and time complexity of tri-training. We thus give an estimate of its efficiency gains. MT-Tri is ~3\u00d7 more spaceefficient than regular tri-training; tri-training stores one set of parameters for each of the three models, while MT-Tri only stores one set of parameters (we use three output layers, but these make up a comparatively small part of the total parameter budget). In terms of time efficiency, tri-training first 3 We also tried enforcing orthogonality on a hidden layer rather than the output layer, but this did not help. MT-Tri can be seen as a self-ensembling technique, where different variations of a model are used to create a stronger ensemble prediction. Recent approaches in this line are snapshot ensembling  #b19  that ensembles models converged to different minima during a training run, asymmetric tri-training  #b39  (ASYM) that leverages agreement on two models as information for the third, and temporal ensembling  #b22 , which ensembles predictions of a model at different epochs. We tried to compare to temporal ensembling in our experiments, but were not able to obtain consistent results. 4 We compare to the closest most recent method, asymmetric tritraining  #b39 . It differs from ours in two aspects: a) ASYM leverages only pseudolabels from data points on which m 1 and m 2 agree, and b) it uses only one task (m 3 ) as final predictor. In essence, our formulation of MT-Tri is closer to the original tri-training formulation (agreements on two provide pseudo-labels to the third) thereby incorporating more diversity.   #b33  for POS tagging (above) and the Amazon Reviews dataset  #b3  for sentiment analysis (below).Algorithm 3 Multi-task Tri-training 1: m \u2190 train_model(L) 2: repeat 3: for i \u2208 {1..3} do 4: L i \u2190 \u2205 5: for x \u2208 U do 6: if p j (x) = p k (x)(j, k = i) then 7: L i \u2190 L i \u222a {(x, p j (x))} 8: if i = 3 then m i = train_model(L i ) 9: elsem i \u2190 train_model(L \u222a L i",
        "Experiments": "In order to ascertain which methods are robust across different domains, we evaluate on two widely used unsupervised domain adaptation datasets for two tasks, a sequence labeling and a classification task, cf. Table 1 for data statistics.",
        "POS tagging": "For POS tagging we use the SANCL 2012 shared task dataset  #b33  and compare to the top results in both low and high-data conditions  #b40  #b50 . Both are strong baselines, as the FLORS tagger has been developed for this challenging dataset and it is based on contextual distributional features (excluding the word's identity), and hand-crafted suffix and shape features (including some languagespecific morphological features). We want to gauge to what extent we can adopt a nowadays fairly standard (but more lexicalized) general neural tagger.Our POS tagging model is a state-of-the-art Bi-LSTM tagger  #b35  with word and 100-dim character embeddings. Word embeddings are initialized with the 100-dim Glove embeddings  #b32 . The BiLSTM has one hidden layer with 100 dimensions. The base POS model is trained on WSJ with early stopping on the WSJ development set, using patience 2, Gaussian noise with \u03c3 = 0.2 and word dropout with p = 0.25  #b21 .Regarding data, the source domain is the Ontonotes 4.0 release of the Penn treebank Wall Street Journal (WSJ) annotated for 48 fine-grained POS tags. This amounts to 30,060 labeled sen-tences. We use 100,000 WSJ sentences from 1988 as unlabeled data, following  #b40 . 5 As target data, we use the five SANCL domains (answers, emails, newsgroups, reviews, weblogs). We restrict the amount of unlabeled data for each SANCL domain to the first 100k sentences, and do not do any pre-processing. We consider the development set of ANSWERS as our only target dev set to set hyperparameters. This may result in suboptimal per-domain settings but better resembles an unsupervised adaptation scenario.",
        "Sentiment analysis": "We show results for sentiment analysis for all 12 domain adaptation scenarios in Figure 2. For clarity, we also show the accuracy scores averaged across each target domain as well as a global macro average in Table 2  around 100-300 in early epochs. This shows that the orthogonality constraint is useful for inducing diversity. In addition, adding fewer examples poses a smaller risk of swamping the learned representations with useless signals and is more akin to fine-tuning, the standard method for supervised domain adaptation  #b18 .We observe an asymmetry in the results between some of the domain pairs, e.g. B->D and D->B. We hypothesize that the asymmetry may be due to properties of the data and that the domains are relatively far apart e.g., in terms of A-distance. In fact, asymmetry in these domains is already reflected   #b40 .in the results of  #b2  and is corroborated in the results for asymmetric tri-training  #b39  and our method. We note a weakness of this dataset is high variance. Existing approaches only report the mean, which makes an objective comparison difficult. For this reason, we believe it is essential to evaluate proposed approaches also on other tasks.POS tagging Results for tagging in the low-data regime (10% of WSJ) are given in Table 3.Self-training does not work for the sequence prediction task. We report only the best instantia-tion (throttling with n=800). Our results contribute to negative findings regarding self-training  #b34  #b47 .In the low-data setup, tri-training with disagreement works best, reaching an overall average accuracy of 89.70, closely followed by classic tritraining, and significantly outperforming the baseline on 4/5 domains. The exception is newsgroups, a difficult domain with high OOV rate where none of the approches beats the baseline (see \u00a73.4). Our proposed MT-Tri is   the baseline significantly on only 2/5 domains (answers and emails). The FLORS tagger  #b50  fares better. Its contextual distributional features are particularly helpful on unknown word-tag combinations (see \u00a7 3.4), which is a limitation of the lexicalized generic bi-LSTM tagger.For the high-data setup (Table 4) results are similar. Disagreement, however, is only favorable in the low-data setups; the effect of avoiding easy points no longer holds in the full data setup. Classic tritraining is the best method. In particular, traditional tri-training is complementary to word embedding initialization, pushing the non-pre-trained baseline to the level of SRC with Glove initalization. Tritraining pushes performance even further and results in the best model, significantly outperforming the baseline again in 4/5 cases, and reaching FLORS performance on weblogs. Multi-task tritraining is often slightly more effective than asymmetric tri-training  #b39 ; however, improvements for both are not robust across domains, sometimes performance even drops. The model likely is too simplistic for such a high-data POS setup, and exploring shared-private models might prove more fruitful  #b24 . On the test sets, tri-training performs consistently the best.",
        "Baselines": "Besides comparing to the top results published on both datasets, we include the following baselines:   #b39 .Our proposed model is multi-task tri-training (MT-Tri). We implement our models in DyNet  #b31 . Reporting single evaluation scores might result in biased results  #b37 . Throughout the paper, we report mean accuracy and standard deviation over five runs for POS tagging and over ten runs for sentiment analysis. Significance is computed using bootstrap test. The code for all experiments is released at: https://github.com/bplank/ semi-supervised-baselines.",
        "Results": "",
        "POS analysis": "We analyze POS tagging accuracy with respect to word frequency 6 and unseen word-tag combinations (UWT) on the dev sets. Table 5 (top rows) provides percentage of un-6 The binned log frequency was calculated with base 2 (bin 0 are OOVs, bin 1 are singletons and rare words etc). known tags, OOVs and unknown word-tag (UWT) rate. The SANCL dataset is overall very challenging: OOV rates are high (6.8-11% compared to 2.3% in WSJ), so is the unknown word-tag (UWT) rate (answers and emails contain 2.91% and 3.47% UWT compared to 0.61% on WSJ) and almost all target domains even contain unknown tags  #b40  (unknown tags: ADD,GW,NFP,XX), except for weblogs. Email is the domain with the highest OOV rate and highest unknown-tag-for-known-words rate. We plot accuracy with respect to word frequency on email in Figure 3, analyzing how the three methods fare in comparison to the baseline on this difficult domain.Regarding OOVs, the results in Table 5 (second part) show that classic tri-training outperforms the source model (trained on only source data) on 3/5 domains in terms of OOV accuracy, except on two domains with high OOV rate (newsgroups and weblogs). In general, we note that tri-training works best on OOVs and on low-frequency tokens, which is also shown in Figure 3 (leftmost bins). Both other methods fall typically below the baseline in terms of OOV accuracy, but MT-Tri still outperforms Asym in 4/5 cases. Table 5 (last part) also shows that no bootstrapping method works well on unknown word-tag combinations. UWT tokens are very difficult to predict correctly using an unsupervised approach; the less lexicalized and more context-driven approach taken by FLORS is clearly superior for these cases, resulting in higher UWT accuracies for 4/5 domains.proaches include adversarial learning  #b14  and fine-tuning  #b41 . There is almost no work on bootstrapping approaches for recent neural NLP, in particular under domain shift. Tri-training is less studied, and only recently re-emerged in the vision community  #b39 , albeit is not compared to classic tri-training.Neural network ensembling Related work on self-ensembling approaches includes snapshot ensembling  #b19  or temporal ensembling  #b22 . In general, the line between \"explicit\" and \"implicit\" ensembling  #b19 , like dropout  #b44  or temporal ensembling  #b39 , is more fuzzy. As we noted earlier our multi-task learning setup can be seen as a form of self-ensembling.Multi-task learning in NLP Neural networks are particularly well-suited for MTL allowing for parameter sharing  #b5 . Recent NLP conferences witnessed a \"tsunami\" of deep learning papers  #b27 , followed by what we call a multi-task learning \"wave\": MTL has been successfully applied to a wide range of NLP tasks  #b8  #b7  #b26  #b35  #b12  #b43  #b38  #b1 . Related to it is the pioneering work on adversarial learning (DANN)  #b14 . For sentiment analysis we found tri-training and our MT-Tri model to outperform DANN. Our MT-Tri model lends itself well to shared-private models such as those proposed recently  #b24 Kim et al., 2017), which extend upon  #b14  by having separate source and target-specific encoders.",
        "Conclusions": "We re-evaluate a range of traditional generalpurpose bootstrapping algorithms in the context of neural network approaches to semi-supervised learning under domain shift. For the two examined NLP tasks classic tri-training works the best and even outperforms a recent state-of-the-art method. The drawback of tri-training it its time and space complexity. We therefore propose a more efficient multi-task tri-training model, which outperforms both traditional tri-training and recent alternatives in the case of sentiment analysis. For POS tagging, classic tri-training is superior, performing especially well on OOVs and low frequency to-kens, which suggests it is less affected by error propagation. Overall we emphasize the importance of comparing neural approaches to strong baselines and reporting results across several runs."
    },
    {
        "2": "Note: we use the term multi-task learning here albeit all tasks are of the same kind, similar to work on multi-lingual modeling treating each language (but same label space) as separate task e.g.,(Fang and Cohn, 2017). It is interesting to point out that our model is further doing implicit multi-view learning by way of the orthogonality constraint.",
        "4": "We suspect that the sparse features in NLP and the domain shift might be detrimental to its unsupervised consistency loss.",
        "5": "Note that our unlabeled data might slightly differ from theirs. We took the first 100k sentences from the 1988 WSJ dataset from the BLLIP 1987-89 WSJ Corpus Release 1."
    },
    {
        "b0": [
            "",
            "",
            "Semisupervised learning for computational linguistics",
            ""
        ],
        "b1": [
            "Multi-task Learning of Pairwise Sequence Classification Tasks Over Disparate Label Spaces",
            "",
            "",
            "",
            "Augenstein",
            "Ruder",
            "S\u00f8gaard"
        ],
        "b2": [
            "",
            "",
            "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. Annual Meeting-Association for Computational Linguistics",
            ""
        ],
        "b3": [
            "Domain Adaptation with Structural Correspondence Learning",
            "",
            "",
            "",
            "Blitzer",
            "Mcdonald",
            "Pereira"
        ],
        "b4": [
            "",
            "",
            "",
            ""
        ],
        "b5": [
            "Multitask learning: A knowledgebased source of inductive bias",
            "",
            "",
            "",
            "Caruana"
        ],
        "b6": [
            "",
            "",
            "Semi-Supervised Learning",
            ""
        ],
        "b7": [
            "Open-domain name error detection using a multitask rnn",
            "",
            "",
            "",
            "Cheng",
            "Fang",
            "Ostendorf"
        ],
        "b8": [
            "Modelling annotator bias with multi-task gaussian processes: An application to machine translation quality estimation",
            "",
            "",
            "",
            "Cohn",
            "Specia"
        ],
        "b9": [
            "",
            "",
            "Stronger baselines for trustable results in neural machine translation",
            ""
        ],
        "b10": [
            "Deep Biaffine Attention for Neural Dependency Parsing",
            "",
            "",
            "",
            "Dozat",
            "Manning"
        ],
        "b11": [
            "",
            "",
            "An introduction to the bootstrap",
            ""
        ],
        "b12": [
            "Learning when to trust distant supervision: An application to lowresource pos tagging using cross-lingual projection",
            "",
            "",
            "",
            "Fang",
            "Cohn"
        ],
        "b13": [
            "Model transfer for tagging low-resource languages using a bilingual dictionary",
            "",
            "",
            "",
            "Fang",
            "Cohn"
        ],
        "b14": [
            "Domain-Adversarial Training of Neural Networks",
            "",
            "",
            "",
            "Ganin",
            "Ustinova",
            "Ajakan",
            "Germain",
            "Larochelle",
            "Laviolette",
            "Marchand",
            "Lempitsky"
        ],
        "b15": [
            "To normalize, or not to normalize: The impact of normalization on part-of-speech tagging",
            "",
            "",
            "",
            "Van Der Goot",
            "Plank",
            "Nissim"
        ],
        "b16": [
            "On Calibration of Modern Neural Networks",
            "",
            "",
            "",
            "Guo",
            "Pleiss",
            "Sun",
            "Weinberger"
        ],
        "b17": [
            "Deep semantic role labeling: What works and what's next",
            "",
            "",
            "",
            "He",
            "Lee",
            "Lewis",
            "Zettlemoyer"
        ],
        "b18": [
            "Universal Language Model Fine-tuning for Text Classification",
            "",
            "",
            "",
            "Howard",
            "Ruder"
        ],
        "b19": [
            "Snapshot Ensembles: Train 1, get M for free",
            "",
            "",
            "",
            "Huang",
            "Li",
            "Pleiss",
            "Liu",
            "Hopcroft",
            "Weinberger"
        ],
        "b20": [
            "Adversarial adaptation of synthetic or stale data",
            "",
            "",
            "",
            "Jiang",
            "Zhai"
        ],
        "b21": [
            "Simple and accurate dependency parsing using bidirectional lstm feature representations",
            "",
            "",
            "",
            "Kiperwasser",
            "Goldberg"
        ],
        "b22": [
            "Temporal Ensembling for Semi-Supervised Learning",
            "",
            "",
            "",
            "Laine",
            "Aila"
        ],
        "b23": [
            "Neural Architectures for Named Entity Recognition",
            "",
            "",
            "",
            "Lample",
            "Ballesteros",
            "Subramanian",
            "Kawakami",
            "Dyer"
        ],
        "b24": [
            "Adversarial multi-task learning for text classification",
            "",
            "",
            "",
            "Liu",
            "Qiu",
            "Huang"
        ],
        "b25": [
            "",
            "",
            "The variational fair autoencoder",
            ""
        ],
        "b26": [
            "",
            "",
            "Multi-task sequence to sequence learning",
            ""
        ],
        "b27": [
            "Computational linguistics and deep learning",
            "",
            "",
            "",
            "Christopher",
            "Manning"
        ],
        "b28": [
            "Effective self-training for parsing",
            "",
            "",
            "",
            "Mcclosky",
            "Charniak",
            "Johnson"
        ],
        "b29": [
            "Reranking and Self-Training for Parser Adaptation",
            "",
            "",
            "",
            "Mcclosky",
            "Charniak",
            "Johnson"
        ],
        "b30": [
            "",
            "",
            "On the State of the Art of Evaluation in Neural Language Models",
            ""
        ],
        "b31": [
            "",
            "",
            "The dynamic neural network toolkit",
            ""
        ],
        "b32": [
            "Glove: Global vectors for word representation",
            "",
            "",
            "",
            "Pennington",
            "Socher",
            "Manning"
        ],
        "b33": [
            "",
            "",
            "Overview of the 2012 shared task on parsing the web. Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL",
            ""
        ],
        "b34": [
            "",
            "",
            "Domain adaptation for parsing",
            ""
        ],
        "b35": [
            "Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss",
            "",
            "",
            "",
            "Plank",
            "S\u00f8gaard",
            "Goldberg"
        ],
        "b36": [
            "Self-training for enhancement and domain adaptation of statistical parsers trained on small datasets",
            "",
            "",
            "",
            "Reichart",
            "Rappoport"
        ],
        "b37": [
            "Reporting score distributions makes a difference: Performance study of lstm-networks for sequence tagging",
            "",
            "",
            "",
            "Reimers",
            "Gurevych"
        ],
        "b38": [
            "",
            "",
            "Learning what to share between loosely related tasks",
            ""
        ],
        "b39": [
            "",
            "",
            "Asymmetric Tri-training for Unsupervised Domain Adaptation",
            ""
        ],
        "b40": [
            "FLORS: Fast and Simple Domain Adaptation for Part-of-Speech Tagging",
            "",
            "",
            "",
            "Schnabel",
            "Sch\u00fctze"
        ],
        "b41": [
            "Improving neural machine translation models with monolingual data",
            "",
            "",
            "",
            "Sennrich",
            "Haddow",
            "Birch"
        ],
        "b42": [
            "Simple semi-supervised training of part-of-speech taggers",
            "",
            "",
            "",
            "S\u00f8gaard"
        ],
        "b43": [
            "Deep multitask learning with low level tasks supervised at lower layers",
            "",
            "",
            "",
            "S\u00f8gaard",
            "Goldberg"
        ],
        "b44": [
            "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
            "",
            "",
            "",
            "Srivastava",
            "Hinton",
            "Krizhevsky",
            "Sutskever",
            "Salakhutdinov"
        ],
        "b45": [
            "Example selection for bootstrapping statistical parsers",
            "",
            "",
            "",
            "Steedman",
            "Hwa",
            "Clark",
            "Osborne",
            "Sarkar",
            "Hockenmaier",
            "Ruhlen",
            "Baker",
            "Crim"
        ],
        "b46": [
            "",
            "",
            "Semi-supervised sequential labeling and segmentation using gigaword scale unlabeled data",
            ""
        ],
        "b47": [
            "",
            "",
            "Predicting the effectiveness of self-training: Application to sentiment classification",
            ""
        ],
        "b48": [
            "",
            "",
            "Sentiment Domain Adaptation with Multiple Sources. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016)",
            ""
        ],
        "b49": [
            "Unsupervised Word Sense Disambiguation Rivaling Supervised Methods",
            "",
            "",
            "",
            "Yarowsky"
        ],
        "b50": [
            "Online Updating of Word Representations for Part-of-Speech Tagging",
            "",
            "",
            "",
            "Yin",
            "Schnabel",
            "Sch\u00fctze"
        ],
        "b51": [
            "Bi-transferring deep neural networks for domain adaptation",
            "",
            "",
            "",
            "Zhou",
            "Xie",
            "Huang",
            "He"
        ],
        "b52": [
            "Tri-Training: Exploiting Unlabeled Data Using Three Classifiers",
            "",
            "",
            "",
            "Zhou",
            "Li"
        ],
        "b53": [
            "Semi-Supervised Learning Literature Survey",
            "",
            "",
            "",
            "Zhu"
        ],
        "b54": [
            "Introduction to semi-supervised learning",
            "",
            "",
            "",
            "Zhu",
            "Goldberg"
        ]
    },
    {
        "tab_0": "Table 1 :1Number of labeled and unlabeled sentences for each domain in the SANCL 2012 datasetDomain# labeled # unlabeledPOS taggingAnswers Emails Newsgroups Reviews Weblogs WSJ3,489 4,900 2,391 3,813 2,031 30,06027,274 1,194,173 1,000,000 1,965,350 524,834 100,000SentimentBook DVD Electronics Kitchen2,000 2,000 2,000 2,0004,465 3,586 5,681 5,945",
        "tab_1": ".ModelDBEKAvgVFAE* 76.57 73.40 80.53 82.93 78.36DANN* 75.40 71.43 77.67 80.53 76.26Asym*76.17 72.97 80.47 83.97 78.39Src75.91 73.47 75.61 79.58 76.14Self78.00 74.55 76.54 80.30 77.35Tri78.72 75.64 78.60 83.26 79.05Tri-D76.99 74.44 78.30 80.59 77.58MT-Tri78.14 74.86 81.45 82.14 79.15Table 2: Average accuracy scores for each SA tar-get domain. *: result from Saito et al. (2017).Self-training achieves surprisingly good resultsbut is not able to compete with tri-training. Tri-training with disagreement is only slightly betterthan self-training, showing that the disagreementcomponent might not be useful when there is astrong domain shift. Tri-training achieves the best",
        "tab_2": "Table 3 :3\u00b1.37 86.49 \u00b1.35 88.60 \u00b1.22 90.12 \u00b1.32 92.85 \u00b1.17 89.14 \u00b1.28 95.49 \u00b1.09 -Self (5) 87.64 \u00b1.18 86.58 \u00b1.30 88.42 \u00b1.24 90.03 \u00b1.11 92.80 \u00b1.19 89.09 \u00b1.20 95.36 \u00b1.07 .5k Tri (4) 88.42 \u00b1.16 87.46 \u00b1.20 87.97 \u00b1.09 90.72 \u00b1.14 93.40 \u00b1.15 89.56 \u00b1.16 95.94 \u00b1.07 20.5k Tri-D (7) 88.50 \u00b1.04 87.63 \u00b1.15 88.12 \u00b1.05 90.76 \u00b1.10 93.51 \u00b1.06 89.70 \u00b1.08 95.99 \u00b1.03 7.7K Asym (3) 87.81 \u00b1.19 86.97 \u00b1.17 87.74 \u00b1.24 90.16 \u00b1.17 92.73 \u00b1.16 89.08 \u00b1.19 95.55 \u00b1.12 Accuracy scores on dev set of target domain for POS tagging for 10% labeled data. Avg: average over the 5 SANCL domains. Hyperparameter ep (epochs) is tuned on Answers dev. \u00b5 pseudo : average amount of added pseudo-labeled data. FLORS: results for Batch (u:big) from (Yin et al., 2015) (see \u00a73). \u00b1.15 88.24 \u00b1.12 89.45 \u00b1.23 91.24 \u00b1.03 93.92 \u00b1.17 90.34 \u00b1.14 96.69 \u00b1.08 Tri 89.34 \u00b1.18 88.83 \u00b1.07 89.32 \u00b1.21 91.62 \u00b1.06 94.40 \u00b1.06 90.70 \u00b1.12 96.84 \u00b1.04 Tri-D 89.35 \u00b1.16 88.66 \u00b1.09 89.29 \u00b1.12 91.58 \u00b1.05 94.32 \u00b1.05 90.62 \u00b1.09 96.85 \u00b1.06 Src (+glove) 89.35 \u00b1.16 88.55 \u00b1.14 90.12 \u00b1.31 91.48 \u00b1.15 94.48 \u00b1.07 90.80 \u00b1.17 96.90 \u00b1.04 Tri 90.00 \u00b1.03 89.06 \u00b1.16 90.04 \u00b1.25 91.98 \u00b1.11 94.74 \u00b1.06 91.16 \u00b1.12 96.99 \u00b1.02 Tri-D 89.80 \u00b1.19 88.85 \u00b1.10 90.03 \u00b1.22 91.98 \u00b1.09 94.70 \u00b1.05 91.01 \u00b1.13 96.95 \u00b1.05 Asym 89.51 \u00b1.15 88.47 \u00b1.19 89.26 \u00b1.16 91.60 \u00b1.20 94.28 \u00b1.15 90.62 \u00b1.17 96.56 \u00b1.01 MT-Tri 89.45 \u00b1.05 88.65 \u00b1.04 89.40 \u00b1.22 91.63 \u00b1.23 94.41 \u00b1.05 90.71 \u00b1.12 97.37 \u00b1.07Target domainsModelepAnswersEmailsNewsgroupsReviewsWeblogsAvgWSJ\u00b5 pseudoSrc (+glove)87.63 1.5kMT-Tri(4) 87.92 \u00b1.18 87.20 \u00b1.23 87.73 \u00b1.37 90.27 \u00b1.10 92.96 \u00b1.07 89.21 \u00b1.19 95.50 \u00b1.067.6kFLORS89.7188.4689.8292.1094.2090.8695.80-Target domains dev setsAvg onModelAnswersEmailsNewsgroupsReviewsWeblogstargetsWSJTnT*88.5588.1488.6690.4093.3389.8295.75Stanford*88.9288.6889.1191.4394.1590.4696.83Src 88.84 FLORS* 90.3089.4490.8692.9594.7191.6696.59Target domains test setsAvg onModelAnswersEmailsNewsgroupsReviewsWeblogstargetsWSJTnT*89.3687.3890.8589.6791.3789.7396.57Stanford*89.7487.7791.2590.3092.3290.2897.43Src (+glove) 90.43 \u00b1.13 87.95 \u00b1.18 91.83 \u00b1.20 90.04 \u00b1.11 92.44 \u00b1.14 90.54 \u00b1.15 97.50 \u00b1.03Tri91.21 \u00b1.06 88.30 \u00b1.19 92.18 \u00b1.19 90.06 \u00b1.10 92.85 \u00b1.02 90.92 \u00b1.11 97.45 \u00b1.03Asym90.62 \u00b1.26 87.71 \u00b1.07 91.40 \u00b1.05 89.89 \u00b1.22 92.37 \u00b1.27 90.39 \u00b1.17 97.19 \u00b1.03MT-Tri90.53 \u00b1.15 87.90 \u00b1.07 91.45 \u00b1.19 89.77 \u00b1.26 92.35 \u00b1.09 90.40 \u00b1.15 97.37 \u00b1.07FLORS*91.1788.6792.4192.2593.1491.5397.11",
        "tab_3": "Table 4 :4Accuracy for POS tagging on the dev and test sets of the SANCL domains, models trained on full source data setup. Values for methods with * are from",
        "tab_4": "better than asymmetric tritraining, but falls below classic tri-training. It beatsAns Email NewsgRev Webl% unk tag0.250.800.310.060.0% OOV8.53 10.5610.346.848.45% UWT2.913.472.432.211.46Accuracy on OOV tokensSrc54.26 57.4861.80 59.26 80.37Tri55.53 59.1161.36 61.16 79.32Asym52.86 56.7856.58 59.59 76.84MT-Tri52.88 57.2257.28 58.99 77.77Accuracy on unknown word-tag (UWT) tokensSrc17.68 11.1417.88 17.31 24.79Tri16.88 10.0417.58 16.35 23.65Asym17.16 10.4317.84 16.92 22.74MT-Tri16.43 11.0817.29 16.72 23.13FLORS*17.19 15.1321.97 21.06 21.65",
        "tab_5": "Table 5 :5Accuracy scores on dev sets for OOV and unknown word-tag (UWT) tokens."
    }
]