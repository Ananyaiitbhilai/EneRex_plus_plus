[
    {
        "basename": "081531984770a74e87dbd68907061b4b0f3631bf.grobid",
        "fulltext": 24,
        "footnote_size": 2,
        "footnote_max": 2,
        "reference": 40,
        "authors": [
            "Caballero",
            "Ledig",
            "Aitken",
            "Acosta",
            "Totz",
            "Wang",
            "Shi Twitter"
        ]
    },
    {
        "title": "Real-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation",
        "abstract": "Convolutional neural networks have enabled accurate image super-resolution in real-time. However, recent attempts to benefit from temporal correlations in video superresolution have been limited to naive or inefficient architectures. In this paper, we introduce spatio-temporal subpixel convolution networks that effectively exploit temporal redundancies and improve reconstruction accuracy while maintaining real-time speed. Specifically, we discuss the use of early fusion, slow fusion and 3D convolutions for the joint processing of multiple consecutive video frames. We also propose a novel joint motion compensation and video super-resolution algorithm that is orders of magnitude more efficient than competing methods, relying on a fast multi-resolution spatial transformer module that is endto-end trainable. These contributions provide both higher accuracy and temporally more consistent videos, which we confirm qualitatively and quantitatively. Relative to singleframe models, spatio-temporal networks can either reduce the computational cost by 30% whilst maintaining the same quality or provide a 0.2dB gain for a similar computational cost. Results on publicly available datasets demonstrate that the proposed algorithms surpass current state-of-theart performance in both accuracy and efficiency.",
        "Introduction": "Image and video super-resolution (SR) are long-standing challenges of signal processing. SR aims at recovering a high-resolution (HR) image or video from its low-resolution (LR) version, and finds direct applications ranging from medical imaging  #b37  #b33  to satellite imaging  #b4 , as well as facilitating tasks such as face recognition  #b12 . The reconstruction of HR data from a LR input is however a highly ill-posed problem that requires additional constraints to be solved. While those constraints are often applicationdependent, they usually rely on data redundancy. In single image SR, where only one LR image is provided, methods exploit inherent image redundancy in the form of local correlations to recover lost high-frequency details by imposing sparsity constraints  #b38  or assuming other types of image statistics such as multi-scale patch recurrence  #b11 . In multi-image SR  #b27  it is assumed that different observations of the same scene are available, hence the shared explicit redundancy can be used to constrain the problem and attempt to invert the downscaling process directly. Transitioning from images to videos implies an additional data dimension (time) with a high degree of correlation that can also be exploited to improve performance in terms of accuracy as well as efficiency.",
        "Related work": "Video SR methods have mainly emerged as adaptations of image SR techniques. Kernel regression methods  #b34  have been shown to be applicable to videos using 3D kernels instead of 2D ones  #b35 . Dictionary learning approaches, which define LR images as a sparse linear combination of dictionary atoms coupled to a HR dictionary, have also been adapted from images  #b37  to videos  #b3 . Another approach is example-based patch recurrence, which assumes patches in a single image or video obey multi-scale relationships, and therefore missing high-frequency content at a given scale can be inferred from coarser scale patches.",
        "arXiv:1611.05250v2 [cs.CV] 10 Apr 2017": "This was successfully presented by Glasner et al.  #b11  for image SR and has later been extended to videos  #b31 .When adapting a method from images to videos it is usually beneficial to incorporate the prior knowledge that frames of the same scene of a video can be approximated by a single image and a motion pattern. Estimating and compensating motion is a powerful mechanism to further constrain the problem and expose temporal correlations. It is therefore very common to find video SR methods that explicitly model motion through frames. A natural choice has been to preprocess input frames by compensating interframe motion using displacement fields obtained from offthe-shelf optical flow algorithms  #b35 . This nevertheless requires frame preprocessing and is usually expensive. Alternatively, motion compensation can also be performed jointly with the SR task, as done in the Bayesian approach of Liu et al.  #b26  by iteratively estimating motion as part of its wider modeling of the downscaling process.The advent of neural network techniques that can be trained from data to approximate complex nonlinear functions has set new performance standards in many applications including SR. Dong et al.  #b5  proposed to use a convolutional neural network (CNN) architecture for single image SR that was later extended by Kappeler et al.  #b21  in a video SR network (VSRnet) which jointly processes multiple input frames. Additionally, compensating the motion of input images with a total variation (TV)-based optical flow algorithm showed an improved accuracy. Joint motion compensation for SR with neural networks has also been studied through recurrent bidirectional networks  #b16 .The common paradigm for CNN based approaches has been to upscale the LR image with bicubic interpolation before attempting to solve the SR problem  #b5  #b21 . However, increasing input image size through interpolation considerably impacts the computational burden for CNN processing. A solution was proposed by Shi et al. with an efficient sub-pixel convolution network (ESPCN)  #b32 , where an upscaling operation directly mapping from LR to HR space is learnt by the network. This technique reduces runtime by an order of magnitude and enables real-time video SR by independently processing frames with a single frame model. Similar solutions to improve efficiency have also been proposed based on transposed convolutions  #b6  #b19 .",
        "Motivation and contributions": "Existing solutions for high definition (HD) video SR have not been able to effectively exploit temporal correlations while performing in real-time. On the one hand, ES-PCN  #b32  leverages sub-pixel convolution for a very efficient operation, but its naive extension to videos treating frames independently fails to exploit inter-frame redundancies and does not enforce a temporally consistent result. VSRnet  #b21 , on the other hand, can improve reconstruction quality by jointly processing multiple input frames. However, the preprocessing of LR images with bicubic upscaling and the use of an inefficient motion compensation mechanism slows runtime to about 0.016 frames per second even on videos smaller than standard definition resolution.Spatial transformer networks  #b18  provide a means to infer parameters for a spatial mapping between two images. These are differentiable networks that can be seamlessly combined and jointly trained with networks targeting other objectives to enhance their performance. For instance, spatial transformer networks were initially shown to facilitate image classification by transforming images onto the same frame of reference  #b18 . Recently, it has been shown how spatial transformers can encode optical flow features with unsupervised training  #b10  #b0  #b28  #b13 , but they have nevertheless not yet been investigated for video motion compensation. Related approaches have emerged for view synthesis assuming rigid transformations  #b20 .In this paper, we combine the efficiency of sub-pixel convolution with the performance of spatio-temporal networks and motion compensation to obtain a fast and accurate video SR algorithm. We study different treatments of the temporal dimension with early fusion, slow fusion and 3D convolutions, which have been previously suggested to extend classification from images to videos  #b22  #b36 . Additionally, we build a motion compensation scheme based on spatial transformers, which is combined with spatio-temporal models to lead to a very efficient solution for video SR with motion compensation that is end-to-end trainable. A high-level diagram of the proposed approach is show in Fig. 1.The main contributions of this paper are:\u2022 Presenting a real-time approach for video SR based on sub-pixel convolution and spatio-temporal networks that improves accuracy and temporal consistency.\u2022 Comparing early fusion, slow fusion and 3D convolutions as alternative architectures for discovering spatio-temporal correlations.\u2022 Proposing an efficient method for dense inter-frame motion compensation based on a multi-scale spatial transformer network.\u2022 Combining the proposed motion compensation technique with spatio-temporal models to provide an efficient, end-to-end trainable motion compensated video SR algorithm.",
        "Methods": "Our starting point is the real-time image SR method ES-PCN  #b32 . We restrict our analysis to standard architectural choices and do not further investigate potentially beneficial extensions such as recurrence  #b23 , residual connections  #b14  #b15  or training networks based on perceptual loss  functions  #b19  #b25  #b2  #b7 . Throughout the paper we assume all image processing is performed on the y-channel in colour space, and thus we represent all images as 2D matrices.",
        "Sub-pixel convolution SR": "For a given LR image I LR \u2208 R H\u00d7W which is assumed to be the result of low-pass filtering and downscaling by a factor r the HR image I HR \u2208 R rH\u00d7rW , the CNN superresolved solution I SR \u2208 R rH\u00d7rW can be expressed asI SR = f I LR ; \u03b8 .(1)Here, \u03b8 are model parameters and f (.) represents the mapping function from LR to HR. A convolutional network models this function as a concatenation of L layers defined by sets of weights and biases \u03b8 l = (W l , b l ), each followed by non-linearities \u03c6 l , with l \u2208 [0, L -1]. Formally, the output of each layer is written asf l I LR ; \u03b8 l = \u03c6 l W l * f l-1 I LR ; \u03b8 l-1 + b l , \u2200l,(2)with f 0 I LR ; \u03b8 0 = I LR . We assume the shape of filtering weights to be n l-1 \u00d7 n l \u00d7 k l \u00d7 k l , where n l and k l represent the number and size of filters in layer l, with the single frame input meaning n 0 = 1. Model parameters are optimised minimising a loss given a set of LR and HR example image pairs, commonly mean squared error (MSE):\u03b8 * = arg min \u03b8 I HR -f (I LR ; \u03b8) 2 2 .(3)Methods preprocessing I LR with bicubic upsampling before mapping from LR to HR impose that the output number of filters is n L-1 = 1  #b5  #b21 . Using sub-pixel convolution allows to process I LR directly in the LR space and then use n L-1 = r 2 output filters to obtain an HR output tensor with shape 1 \u00d7 r 2 \u00d7 H \u00d7 W that can be reordered to obtain I SR  #b32 . This implies that if there exists an upscaling operation that is better suited for the problem than bicubic upsampling, the network can learn it. Moreover, and most importantly, all convolutional processing is performed in LR space, making this approach very efficient.",
        "Spatio-temporal networks": "Spatio-temporal networks assume input data to be a block of spatio-temporal information, such that instead of a single input frame I LR , a sequence of consecutive frames is considered. This can be represented in the network by introducing an additional dimension for temporal depth D l , with the input depth D 0 representing an odd number of consecutive input frames. If we denote the temporal radius of a spatio-temporal block to be R = D0-1 2 , we define the group of input frames centered at time t as I LR [t-R:t+R] \u2208 R H\u00d7W \u00d7D0 , and the problem in Eq. ( 1) becomesI SR t = f I LR [t-R:t+R] ; \u03b8 .(4)The shape of weighting filters W l is also extended by their temporal size d l , and their tensor shape becomesd l \u00d7n l-1 \u00d7 n l \u00d7 k l \u00d7 k l .We note that it is possible to consider solutions that aim to jointly reconstruct more than a single output frame, which could have advantages at least in terms of computational efficiency. However, in this work we focus on the reconstruction of only a single output frame.",
        "Early fusion": "One of the most straightforward approaches for a CNN to process videos is to match the temporal depth of the input layer to the number of frames d 0 = D 0 . This will collapse all temporal information in the first layer and the remaining operations are identical to those in a single image SR network, meaning d l = 1, l \u2265 1. An illustration of early fusion is shown in Fig. 2a for D 0 = 5, where the temporal dimension has been colour coded and the output mapping to 2D space is omitted. This design has been studied for video classification and action recognition  #b22  #b36 , and was also one of the architectures proposed in VSRnet  #b21 . However, VSRnet requires bicubic upsampling as opposed to sub-pixel convolution, making the framework computationally much less efficient in comparison.",
        "Slow fusion": "Another option is to partially merge temporal information in a hierarchical structure, so it is slowly fused as information progresses through the network. In this case, the temporal depth of network layers is configured to be 1 \u2264 d l < D 0 , and therefore some layers also have a temporal extent until all information has been merged and the depth of the network reduces to 1. This architecture, termed slow fusion, has shown better performance than early fusion for video classification  #b22 . In Fig. 2b we show a slow fusion network where D 0 = 5 and the rate of fusion is defined by d l = 2 for l \u2264 3 or d l = 1 otherwise, meaning that at each layer only two consecutive frames or filter activations are merged until the network's temporal depth shrinks to 1. Note that early fusion is an special case of slow fusion.",
        "3D convolutions": "Another variation of slow fusion is to force layer weights to be shared across the temporal dimension, which has computational advantages. Assuming an online processing of frames, when a new frame becomes available the result of some layers for the previous frame can be reused. For instance, refering to the diagram in Fig. 2b and assuming the bottom frame to be the latest frame received, all activations above the dashed line are readily available because they were required for processing the previous frame. This architecture is equivalent to using 3D convolutions, initially proposed as an effective tool to learn spatio-temporal features that can help for video action recognition  #b36 . An illustration of this design from a 3D convolution perspective is shown in Fig. 2c, where the arrangement of the temporal and filter features is swapped relative to Fig. 2b.",
        "Spatial transformer motion compensation": "We propose the use of an efficient spatial transformer network to compensate the motion between frames fed to the SR network. It has been shown how spatial transformers can effectively encode optical flow to describe motion  #b28  #b0  #b13 , and are therefore suitable for motion compensation. We will compensate blocks of three consecutive frames to combine the compensation module with the SR network as shown in Fig. 1, but for simplicity we first introduce motion compensation between two frames. No-tice that the data used contains inherent motion blur and (dis)occlusions, and even though an explicit modelling for these effects is not used it could potentially improve results.The task is to find the best optical flow representation relating a new frame I t+1 with a reference current frame I t . The flow is assumed pixel-wise dense, allowing to displace each pixel to a new position, and the resulting pixel arrangement requires interpolation back onto a regular grid. We use bilinear interpolation I{.} as it is much more efficient than the thin-plate spline interpolation originally proposed in  #b18 . Optical flow is a function of parameters \u03b8 \u2206,t+1 and is represented with two feature maps \u2206 t+1 = (\u2206 t+1 x, \u2206 t+1 y; \u03b8 \u2206,t+1 ) corresponding to displacements for the x and y dimensions, thus a compensated image can be expressed as I t+1 (x, y) = I{I t+1 (x + \u2206 t+1 x, y + \u2206 t+1 y)}, or more conciselyI t+1 = I{I t+1 (\u2206 t+1 )}.(5)We adopt a multi-scale design to represent the flow, which has been shown to be effective in classical methods  #b9  #b1  and also in more recently proposed spatial transformer techniques  #b10  #b0  #b8 . A schematic of the design is shown in Fig. 3 and flow estimation modules are detailed in Table 1. First, a \u00d74 coarse estimate of the flow is obtained by early fusing the two input frames and downscaling spatial dimensions with \u00d72 strided convolutions. The estimated flow is upscaled with sub-pixel convolution and the result \u2206 c t+1 is applied to warp the target frame producing I c t+1 . The warped image is then processed together with the coarse flow and the original images through a fine flow estimation module. This uses a single strided convolution with stride 2 and a final \u00d72 upscaling stage to obtain a finer flow map \u2206 f . The final motion compensated frame is obtained by warping the target frame with the total flowI t+1 = I{I t+1 (\u2206 c t+1 + \u2206 f t+1 )}.Output activations use tanh to represent pixel displacement in normalised space, such that a displacement of \u00b11 means maximum displacement from the center to the border of the image.To train the spatial transformer to perform motion compensation we optimise its parameters \u03b8 \u2206,t+1 to minimise the MSE between the transformed frame and the reference frame. Similary to classical optical flow methods, we found that it is generally helpful to constrain the flow to behave smoothly in space, and so we penalise the Huber loss of the flow map gradients, namely \u03b8 * \u2206,t+1 = arg min \u03b8\u2206,t+1I t -I t+1 2 2 + \u03bbH (\u2202 x,y \u2206 t+1 ) .(6)In practice we approximate the Huber loss withH (\u2202 x,y \u2206) = + i=x,y (\u2202 x \u2206i 2 + \u2202 y \u2206i 2 ), where = 0.01. This function has a smooth L2 behaviour near the origin and is sparsity promoting far from it.  The spatial transformer module is advantageous relative to other motion compensation mechanisms as it is straightforward to combine with a SR network to perform joint motion compensation and video SR. Referring to Fig. 1, the same parameters \u03b8 \u2206 can be used to model motion of the outer two frames relative to the central frame. The spatial transformer and SR modules are both differentiable and therefore end-to-end trainable. As a result, they can be jointly optimised to minimise a composite loss combining the accuracy of the reconstruction in Eq. ( 3) with the fidelity of motion compensation in Eq. ( 6), namely(\u03b8 * , \u03b8 * \u2206 ) = arg min \u03b8,\u03b8\u2206 I HR t -f (I LR t-1:t+1 ; \u03b8) 2 2 + i=\u00b11 [\u03b2 I LR t+i -I LR t 2 2 + \u03bbH (\u2202 x,y \u2206 t+i )].(7)",
        "Experiments and results": "In this section, we first analyse spatio-temporal networks for video SR in isolation and later evaluate the benefits of introducing motion compensation. We restrict our experiments to tackle \u00d73 and \u00d74 upscaling of full HD video resolution (1080\u00d71920), and no compression is applied. To ensure a fair comparison of methods, the number of network parameters need to be comparable so that gains in perfor-mance can be attributed to specific choices of network resource allocation and not to a trivial increase in capacity. For a layer l, the number of floating-point operations to reconstruct a frame is approximated byHW D l+1 n l+1 convolutions (2k 2 l d l -1)n l + 2 bias & activation . (8)In measuring the complexity of slow fusion networks with weight sharing we look at steady-state operation where the output of some layers is reused from one frame to the following. We note that the analysis of VSRnet variants in  #b21  does not take into account model complexity.",
        "Experimental setup 3.1.1 Data": "We use the CDVL database  #b17 , which contains 115 uncompressed full HD videos excluding repeated videos, and choose a subset of 100 videos for training. The videos are downscaled and 30 random samples are extracted from each HR-LR video pair to obtain 3000 training samples, 5% of which are used for validation. Depending on the network architecture, we refer to a sample as a single inputoutput frame pair for single frame networks, or as a block of consecutive LR input frames and the corresponding central HR frame for spatio-temporal networks. The remaining 15 videos are used for testing. Although the total number of training frames is large, we foresee that the methods presented could benefit from a richer, more diverse set of videos. Additionally, we present a benchmark against various SR methods on publicly available videos that are recurrently used in the literature and we refer to as Vid41 .",
        "Network training and parameters": "All SR models are trained following the same protocol and share similar hyperparameters. Filter sizes are set to k l = 3 \u2200l, and all non-linearities \u03c6 l are rectified linear units except for the output layer, which uses a linear activation. Biases are initialised to 0 and weights use orthogonal initialisation with gain \u221a 2 following recommendations in  #b29 . All hidden layers are set to have the same number of features. Video samples are broken into non-overlapping subsamples of spatial dimensions 33 \u00d7 33, which are randomly grouped in batches for stochastic optimisation. We employ Adam  #b24  with a learning rate 10 -4 and an initial batch size 1. Every 10 epochs the batch size is doubled until it reaches a maximum size of 128. We choose n l = 24 for layers where the network temporal depth is 1 (layers in gray in Figs. 2a to 2c), and to maintain comparable network sizes we choose n l = 24/D l , l > 0. This ensures that the number of features per hidden layer in early and slow fusion networks is always the same. For instance, the network shown in Fig. 2b, for which D 0 = 5 and d l = 2 for l \u2264 3, the number of features in a 6 layer network for \u00d7r SR would be 6, 8, 12, 24, 24, r 2 .",
        "Spatio-temporal video SR": "",
        "Single vs multi frame early fusion": "First, we investigate the impact of the number of input frames on complexity and accuracy without motion compensation. We compare single frame models (SF) against early fusion spatio-temporal models using 3, 5 and 7 input frames (E3, E5 and E7). Peak signal-to-noise ratio (PSNR) results on the CDVL dataset for networks of 6 to 11 layers are plotted in Fig. 4. Exploiting spatio-temporal correlations provides a more accurate result relative to an independent processing of frames. The increase in complexity from early fusion is marginal because only the first layer contributes to an increase of operations.Although the accuracy of spatio-temporal models is relatively similar, we find that E7 slightly underperforms. It is likely that temporal dependencies beyond 5 frames become too complex for networks to learn useful information and act as noise degrading their performance. Notice also that, whereas the performance increase from network depth is minimal after 8 layers for single frame networks, this increase is more consistent for spatio-temporal models.",
        "Early vs slow fusion": "Here we compare the different treatments of the temporal dimension discussed in Section 2.2. We assume networks with an input of 5 frames and slow fusion models with fil- ter temporal depths 2 as in Fig. 2. Using SF, E5, S5, and S5-SW to refer to single frame networks and 5 frame input networks using early fusion, slow fusion, and slow fusion with shared weights, we show in Table 2 results for 7 and 9 layer networks.As seen previously, early fusion networks attain a higher accuracy at a marginal 3% increase in operations relative to the single frame models, and as expected, slow fusion architectures provide efficiency advantages. Slow fusion is faster than early fusion because it uses fewer features in the initial layers. Referring to Eq. ( 8), slow fusion uses d l = 2 in the first layers and n l = 24/D l , which results in fewer operations than d l = 1, n l = 24 as used in early fusion.While the 7 layer network sees a considerable decrease in accuracy using slow fusion relative to early fusion, the 9 layer network can benefit from the same accuracy while reducing its complexity with slow fusion by about 30%. This suggests that in shallow networks the best use of network resources is to utilise the full network capacity to jointly process all temporal information as done by early fusion, but that in deeper networks slowly fusing the temporal dimension is beneficial, which is in line with the results presented by  #b22  for video classification.Additionally, weight sharing decreases accuracy because of the reduction in network parameters, but the reusability of network features means fewer operations are needed per frame. For instance, the 7 layer S5-SW network shows a reduction of almost 30% of operations with a minimal decrease in accuracy relative to SF. Using 7 layers with E5 nevertheless shows better performance and faster operation than S5-SW with 9 layers, and in all cases we found that early or slow fusion consistently outperformed slow fusion with shared weights in this performance and efficiency trade-off. Convolutions in spatio-temporal domain were shown in  #b36  to work well for video action recognition, but with larger capacity and many more frames processed jointly. We speculate this could be the reason why the conclusions drawn from this high-level vision task do not extrapolate to the SR problem.",
        "Motion compensated video SR": "In this section, the proposed frame motion compensation is combined with an early fusion network of temporal depth D 0 = 3. First, the motion compensation module is trained independently using Eq.  #b6 , where the first term is ignored and \u03b2 = 1, \u03bb = 0.01. This results in a network that will    compensate the motion of three consecutive frames by estimating the flow maps of outer frames relative to the middle frame. An example of a flow map obtained for one frame is shown in Fig. 5, where we also show the effect the motion compensation module has on three consecutive frames. The early fusion motion compensated SR network (E3-MC) is initialised with a compensation and a SR network pretrained separately, and the full model is then jointly op-timised with Eq. ( 7) (\u03b2 = 0.01, \u03bb = 0.001). Results for \u00d73 SR on CDVL are compared in Table 3 against a single frame (SF) model and early fusion without motion compensation (E3). E3-MC results in a PSNR that is sometimes almost twice the improvement of E3 relative to SF, which we attribute to the fact that the network adapts the SR input to maximise temporal redundancy. In Fig. 6 we show how this improvement is reflected in better structure preservation.",
        "Comparison to state-of-the-art": "We show in Table 4 the performance on Vid4 for SRCNN  #b5 , ESPCN  #b32 , VSRnet  #b21  and the proposed method, which we refer to as video ESPCN (VESPCN). To demonstrate its benefits in efficiency and quality we evaluate two early fusion models: a 5 layer 3 frame network (5L-E3) and a 9 layer 3 frame network with motion compensation (9L-E3-MC). The metrics compared are PSNR, structural similarity (SSIM)  #b39  and MOVIE  #b30  indices. The MOVIE index was designed as a metric measuring video quality that correlates with human perception and incorporates a notion of temporal consistency. We also directly compare the number of operations per frame of all CNN-based approaches for upscaling a generic 1080p frame.Reconstructions for SRCNN, ESPCN and VSRnet use models provided by the authors. SRCNN, ESPCN and VESPCN were tested on Theano and Lasagne, and for VS-Rnet we used available Caffe Matlab code. We crop spatial borders as well as initial and final frames on all reconstructions for fair comparison against VSRnet2 .",
        "Quality comparison": "An example of visual differences is shown in Fig. 7 against the motion compensated network. From the close-up images, we see how the structural detail of the original video is better recovered by the proposed VESPCN method. This is reflected in Table 4, where it surpasses any other method in PSNR and SSIM by a large margin. Figure 7 also shows temporal profiles on the row highlighted by a dashed line through 25 consecutive frames, demonstrating a better temporal coherence of the reconstruction proposed. The great temporal coherence of VESPCN also explains the significant reduction in the MOVIE index.",
        "Efficiency comparison": "The complexity of methods in  VSRnet is particularly expensive because it processes 5 input frames in 64 and 320 feature layers, whereas sub-pixel convolution greatly reduces the number of operations required in ESPCN and VESPCN. As a reference, ESPCN \u00d74 runs at 29ms per frame on a K2 GPU  #b32 . The enhanced capabilities of spatio-temporal networks allow to reduce the network operations of VESPCN relative to ESPCN while still matching its accuracy. As an example we show VESPCN with 5L-E3, which reduces the number of operations by about 20% relative to ESPCN while maintaining a similar performance in all evaluated quality metrics.The operations for motion compensation in VESPCN with 9L-E3-MC, included in Table 4 results, amount to 3.6 and 2.0 GOps for \u00d73 and \u00d74 upscaling, applied twice for each input frame requiring motion compensation. This makes the proposed motion compensated video SR very efficient relative to other approaches. For example, motion compensation in VSRnet is said to require 55 seconds per frame and is the computational bottleneck  #b21 . This is not accounted for in Table 4 but is \u00d710 3 slower than VESPCN with 9L-E3-MC, which can run in the order of 10 -2 sec-onds. The optical flow method in VSRnet was originally shown to run at 29ms on GPU for each frame of dimensions 512 \u00d7 383, but this is still considerably slower than the proposed solution considering motion compensation is required for more than a single frame of HD dimensions.",
        "Conclusion": "In this paper we combine the efficiency advantages of sub-pixel convolutions with temporal fusion strategies to present real-time spatio-temporal models for video SR. The spatio-temporal models used are shown to facilitate an improvement in reconstruction accuracy and temporal consistency or reduce computational complexity relative to independent single frame processing. The models investigated are extended with a motion compensation mechanism based on spatial transformer networks that is efficient and jointly trainable for video SR. Results obtained with approaches that incorporate explicit motion compensation are demonstrated to be superior in terms of PSNR and temporal consistency compared to spatio-temporal models alone, and outperform the current state of the art in video SR."
    },
    {
        "1": "Vid4 is composed of walk, city, calendar and foliage, and has sizes 720 \u00d7 480 or 720 \u00d7 576. The sequence city has dimensions 704 \u00d7 576, which we crop to 702 \u00d7 576 for \u00d73 upscaling. Results on Vid4 can be downloaded from https://twitter.box.com/v/vespcn-vid4",
        "2": "We used our own implementation of SSIM and use video PSNR instead of averaging individual frames PSNR as done in[22], thus values may slightly deviate from those reported in original papers."
    },
    {
        "b0": [
            "Unsupervised convolutional neural networks for motion estimation",
            "",
            "",
            "",
            "Ahmadi",
            "Patras"
        ],
        "b1": [
            "High accuracy optical flow estimation based on a theory for warping",
            "",
            "",
            "",
            "Brox",
            "Papenberg",
            "Weickert"
        ],
        "b2": [
            "Super-resolution with deep convolutional sufficient statistics",
            "",
            "",
            "",
            "Bruna",
            "Sprechmann",
            "Lecun"
        ],
        "b3": [
            "Dictionary-based multiple frame video super-resolution",
            "",
            "",
            "",
            "Dai",
            "Yoo",
            "Kappeler",
            "Katsaggelos"
        ],
        "b4": [
            "Discrete wavelet transformbased satellite image resolution enhancement",
            "",
            "",
            "",
            "Demirel",
            "Anbarjafari"
        ],
        "b5": [
            "Image super-resolution using deep convolutional networks",
            "",
            "",
            "",
            "Dong",
            "Loy",
            "He",
            "Tang"
        ],
        "b6": [
            "Accelerating the super-resolution convolutional neural network",
            "",
            "",
            "",
            "Dong",
            "Loy",
            "Tang"
        ],
        "b7": [
            "",
            "",
            "Generating images with perceptual similarity metrics based on deep networks",
            ""
        ],
        "b8": [
            "FlowNet: Learning optical flow with convolutional networks",
            "",
            "",
            "",
            "Dosovitskiy",
            "Fischery",
            "Ilg",
            "Hazirbas",
            "Golkov",
            "Van Der Smagt",
            "Cremers",
            "Brox"
        ],
        "b9": [
            "Two-frame motion estimation based on polynomial expansion",
            "",
            "",
            "",
            "Farneback"
        ],
        "b10": [
            "DeepWarp: Photorealistic Image Resynthesis for Gaze Manipulation",
            "",
            "",
            "",
            "Ganin",
            "Kononenko",
            "Sungatullina",
            "Lempitsky"
        ],
        "b11": [
            "Super-resolution from a single image",
            "",
            "",
            "",
            "Glasner",
            "Bagon",
            "Irani"
        ],
        "b12": [
            "Eigenface-domain super-resolution for face recognition",
            "",
            "",
            "",
            "Gunturk",
            "Batur",
            "Altunbasak",
            "Hayes",
            "Mersereau"
        ],
        "b13": [
            "",
            "",
            "Neural Network Library for Geometric Computer Vision. European Conference on Computer Vision (ECCV) Workshop on Deep Geometry",
            ""
        ],
        "b14": [
            "Deep residual learning for image recognition",
            "",
            "",
            "",
            "He",
            "Zhang",
            "Ren",
            "Sun"
        ],
        "b15": [
            "Identity mappings in deep residual networks",
            "",
            "",
            "",
            "He",
            "Zhang",
            "Ren",
            "Sun"
        ],
        "b16": [
            "Bidirectional recurrent convolutional networks for multi-frame super-resolution",
            "",
            "",
            "",
            "Huang",
            "Wang",
            "Wang"
        ],
        "b17": [
            "",
            "",
            "Consumer Digital Video Library",
            "http://www.cdvl.org/"
        ],
        "b18": [
            "",
            "",
            "Spatial transformer networks. Advances in Neural Information Processing Systems (NIPS)",
            ""
        ],
        "b19": [
            "Perceptual losses for real-time style transfer and super-resolution",
            "",
            "",
            "",
            "Johnson",
            "Alahi",
            "Fei-Fei"
        ],
        "b20": [
            "Learning-based view synthesis for light field cameras",
            "",
            "",
            "",
            "Kalantari",
            "Wang",
            "Ramamoorthi"
        ],
        "b21": [
            "Video super-resolution with convolutional neural networks",
            "",
            "",
            "",
            "Kappeler",
            "Yoo",
            "Dai",
            "Katsaggelos"
        ],
        "b22": [
            "Large-scale video classification with convolutional neural networks",
            "",
            "",
            "",
            "Karpathy",
            "Toderici",
            "Shetty",
            "Leung",
            "Sukthankar",
            "Fei-Fei"
        ],
        "b23": [
            "Deeply-recursive convolutional network for image super-resolution",
            "",
            "",
            "",
            "Kim",
            "Lee",
            "Lee"
        ],
        "b24": [
            "",
            "",
            "Adam: A method for stochastic optimization. International Conference On Learning Representations (ICLR)",
            ""
        ],
        "b25": [
            "",
            "",
            "Photo-realistic single image super-resolution using a generative adversarial network",
            ""
        ],
        "b26": [
            "A bayesian approach to adaptive video super resolution",
            "",
            "",
            "",
            "Liu",
            "England",
            "Sun"
        ],
        "b27": [
            "Super-resolution image reconstruction: A technical overview",
            "",
            "",
            "",
            "Park",
            "Park",
            "Kang"
        ],
        "b28": [
            "",
            "",
            "Spatio-temporal video autoencoder with differentiable memory. International Conference On Learning Representations (ICLR) Workshop",
            ""
        ],
        "b29": [
            "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
            "",
            "",
            "",
            "Saxe",
            "Mcclelland",
            "Ganguli"
        ],
        "b30": [
            "Motion tuned spatio-temporal quality assessment of natural videos motion tuned spatio-temporal quality assessment of natural videos",
            "",
            "",
            "",
            "Seshadrinathan",
            "Member",
            "Bovik"
        ],
        "b31": [
            "Space-time superresolution from a single video",
            "",
            "",
            "",
            "Shahar",
            "Faktor",
            "Irani"
        ],
        "b32": [
            "Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network",
            "",
            "",
            "",
            "Shi",
            "Caballero",
            "Ferenc",
            "Johannes",
            "Aitken",
            "Bishop",
            "Daniel",
            "Wang"
        ],
        "b33": [
            "Cardiac image super-resolution with global correspondence using multi-atlas PatchMatch",
            "",
            "",
            "",
            "Shi",
            "Caballero",
            "Ledig",
            "Zhuang",
            "Bai",
            "Bhatia",
            "De Marvao",
            "Dawes",
            "O'regan",
            "Rueckert"
        ],
        "b34": [
            "Kernel regression for image processing and reconstruction",
            "",
            "",
            "",
            "Takeda",
            "Farsiu",
            "Milanfar"
        ],
        "b35": [
            "Superresolution without explicit subpixel motion estimation",
            "",
            "",
            "",
            "Takeda",
            "Milanfar",
            "Protter",
            "Elad"
        ],
        "b36": [
            "Learning spatiotemporal features with 3D convolutional networks",
            "",
            "",
            "",
            "Tran",
            "Bourdev",
            "Fergus",
            "Torresani",
            "Paluri"
        ],
        "b37": [
            "Coupled dictionary training for image super-resolution",
            "",
            "",
            "",
            "Yang",
            "Member",
            "Wang"
        ],
        "b38": [
            "Image superresolution via sparse representation",
            "",
            "",
            "",
            "Yang",
            "Wright",
            "Huang",
            "Ma"
        ],
        "b39": [
            "Image quality assessment: form error visibility to structural similarity",
            "",
            "",
            "",
            "Zhou",
            "Bovik",
            "Sheikh",
            "Simoncelli"
        ]
    },
    {
        "tab_0": "Table 1 :1Motion compensation transformer architecture.Convolutional layers are described by kernel size (k), number of features (n) and stride (s).",
        "tab_1": "Table 2 :2Comparison of spatio-temporal architectures# LayersSFE5S5S5-SW7PSNR 37.78 37.92 37.83 GOps 12.29 12.69 10.6537.74 8.949PSNR 37.80 37.99 37.99 GOps 16.83 17.22 15.1937.90 13.47",
        "tab_3": "Table 3 :3PSNR for CDVL \u00d73 SR using single frame (SF) and 3 frame early fusion without and with motion compensation (E3, E3-MC).",
        "tab_4": "Table 4 :4Table 4 is determined by network and input image sizes. SRCNN and VSRnet upsample LR images before attempting to super-resolve them, which considerably increases the required number of operations. Performance on Vid4 videos. *VSRnet does not include operations needed for motion compensation.Image and video SRProposed VESPCNScaleBicubic SRCNNESPCNVSRnet5L-E39L-E3-MC3PSNR25.3826.5626.9726.6427.0527.25SSIM0.76130.81870.83640.82380.83880.8447MOVIE (\u00d710 -3 )5.363.583.223.503.122.86GOps / 1080p frame-233.119.921108.73*7.9624.234PSNR23.8224.6825.0624.4325.1225.35SSIM0.65480.71580.73940.73720.74220.7557MOVIE (\u00d710 -3 )9.316.906.546.826.185.82GOps / 1080p frame-233.116.081108.73*4.8514.00"
    }
]