[
    {
        "basename": "38cc89399dd6f5aaab1654f27ab3c9eeade12a36.grobid",
        "fulltext": 11,
        "footnote_size": 0,
        "reference": 57,
        "authors": [
            "Rayat",
            "Hossain",
            "Little"
        ]
    },
    {
        "title": "Exploiting temporal information for 3D human pose estimation",
        "abstract": "In this work, we address the problem of 3D human pose estimation from a sequence of 2D human poses. Although the recent success of deep networks has led many state-of-the-art methods for 3D pose estimation to train deep networks end-to-end to predict from images directly, the top-performing approaches have shown the effectiveness of dividing the task of 3D pose estimation into two steps: using a state-of-the-art 2D pose estimator to estimate the 2D pose from images and then mapping them into 3D space. They also showed that a low-dimensional representation like 2D locations of a set of joints can be discriminative enough to estimate 3D pose with high accuracy. However, estimation of 3D pose for individual frames leads to temporally incoherent estimates due to independent error in each frame causing jitter. Therefore, in this work we utilize the temporal information across a sequence of 2D joint locations to estimate a sequence of 3D poses. We designed a sequence-to-sequence network composed of layer-normalized LSTM units with shortcut connections connecting the input to the output on the decoder side and imposed temporal smoothness constraint during training. We found that the knowledge of temporal consistency improves the best reported result on Human3.6M dataset by approximately 12.2% and helps our network to recover temporally consistent 3D poses over a sequence of images even when the 2D pose detector fails.",
        "Introduction": "The task of estimating 3D human pose from 2D representations like monocular images or videos is an open research problem among the computer vision and graphics community for a long time. An understanding of human posture and limb articulation is important for high level computer vision tasks such as human action or activity recognition, sports analysis, augmented and virtual reality. A 2D representation of human pose, which is considered to be much easier to estimate, can be used for these tasks. However, 2D poses can be ambiguous because of occlusion and foreshortening. Additionally poses that are totally different can appear to be similar in 2D because of the way they are projected as shown in Figure 1. The depth information in 3D representation of human pose makes it free from such ambiguities and hence can improve performance for higher level tasks. Moreover, 3D pose can be very useful in computer animation, where the articulated pose of a person in 3D can be used to accurately model human posture and movement. However, 3D pose estimation is an ill-posed problem because of the inherent ambiguity in back-projecting a 2D view of an object to the 3D space maintaining its structure. Since the 3D pose of a person can be projected in an infinite number of ways on a 2D plane, the mapping from a 2D pose to 3D is not unique. Moreover, obtaining a dataset for 3D pose is difficult and expensive. Unlike the 2D pose datasets where the users can manually label the keypoints by mouse clicks, 3D pose datasets require a complicated laboratory setup with motion capture sensors and cameras. Hence, there is a lack of motion capture datasets for images in-the-wild. Over the years, different techniques have been used to address the problem of 3D pose estimation. Earlier methods used to focus on extracting features, invariant to factors such as background scenes, lighting, and skin color from images and mapping them into 3D human pose  #b5  #b6  #b7  #b8 . With the success of deep networks, recent methods tend to focus on training a deep convolutional neural network (CNN) end-to-end to estimate 3D poses from images directly [6, #b10  #b11  #b12  #b13  #b14  #b15 13, #b17  #b18  #b19 . Some approaches divided the 3D pose estimation task into first predicting the joint locations in 2D using 2D pose estimators  #b20  #b21  and then back-projecting them to estimate the 3D joint locations  #b22  #b23  #b24  #b25  #b26  #b27 . These results suggest the effectiveness of decoupling the task of 3D pose estimation where 2D pose estimator abstracts the complexities in the image. In this paper, we also adopt the decoupled approach to 3D pose estimation. However, predicting 3D pose for each frame individually can lead to jitter in videos because the errors in each frame are independent of each other. Therefore, we designed a sequence-to-sequence network [1] with shortcut connections on the decoder side  #b28  that predicts a sequence of temporally consistent 3D poses given a sequence of 2D poses. Each unit of our network is a Long Short-Term Memory (LSTM)  #b29  unit with layer normalization  #b30  and recurrent dropout  #b31 . We also imposed a temporal smoothness constraint on the predicted 3D poses during training to ensure that our predictions are smooth over a sequence. Our network achieves the state-of-the-art result on the Human3.6M dataset improving the previous best result by approximately 12.2%. We also obtained the lowest error for every action class in Human3.6M dataset  #b32 . Moreover, we observed that our network predicted meaningful 3D poses on Youtube videos, even when the detections from the 2D pose detector were extremely noisy or meaningless. This shows the effectiveness of using temporal information. In short our contributions in this work are:-Designing an efficient sequence-to-sequence network that achieves the stateof-the-art results for every action class of Human3.6M dataset  #b32  and can be trained very fast. -Exploiting the ability of sequence-to-sequence networks to take into account the events in the past, to predict temporally consistent 3D poses. -Effectively imposing temporal consistency constraint on the predicted 3D poses during training so that the errors in the predictions are distributed smoothly over the sequence.-Using only the previous frames to understand temporal context so that it can be deployed online and real-time.",
        "Related Work": "Representation of 3D pose Both model-based and model-free representations of 3D human pose have been used in the past. The most common model-based representation is a skeleton defined by a kinematic tree of a set of joints, parameterized by the offset and rotational parameters of each joint relative to its parent. Several 3D pose methods have used this representation  #b33  #b34  #b25  #b13 . Others model 3D pose as a sparse linear combination of an over-complete dictionary of basis poses  #b24  #b23  #b22 . However, we have chosen a model-free representation of 3D pose, where a 3D pose is simply a set of 3D joint locations relative to the root node like several recent approaches  #b27  #b26  #b11  #b12 . This representation is much simpler and low-dimensional.Estimating 3D pose from 2D joints Lee and Chen  #b35  were the first to infer 3D joint locations from their 2D projections given the bone lengths using a binary decision tree where each branch corresponds to two possible states of a joint relative to its parent. Jiang  #b36  used the 2D joint locations to estimate a set of hypothesis 3D poses using Taylor's algorithm  #b37  and used them to query a large database of motion capture data to find the nearest neighbor. Gupta et al.  #b38  and Chen and Ramanan  #b39  also used this idea of using the detected 2D pose to query a large database of exemplar poses to find the nearest nearest neighbor 3D pose. Another common approach to estimating 3D joint locations given the 2D pose is to separate the camera pose variability from the intrinsic deformation of the human body, the latter of which is modeled by learning an over-complete dictionary of basis 3D poses from a large database of motion capture data  #b22  #b23  #b25  #b24  #b40 . A valid 3D pose is defined by a sparse linear combination of the bases and by transforming the points using transformation matrix representing camera extrinsic parameters. Moreno-Nouguer  #b26  used the pair-wise distance matrix of 2D joints to learn a distance matrix for 3D joints, which they found invariant up to a rigid similarity transform with the ground truth 3D and used multi-dimensional scaling (MDS) with pose-priors to rule out the ambiguities. Martinez et al.  #b27  designed a fully connected network with shortcut connections every two linear layers to estimate 3D joint locations relative to the root node in the camera coordinate space.Deep network based methods With the success of deep networks, many have designed networks that can be trained end-to-end to predict 3D poses from images directly  #b10  #b11 6, #b17  #b12  #b18  #b13  #b41  #b42  #b43 . Li et al.  #b11  and Park et al.  #b17  designed CNNs to jointly predict 2D and 3D poses. Mehta et al.  #b12  and Sun et al.  #b18  used transfer learning to transfer the knowledge learned for 2D human pose estimation to the task of 3D pose estimation. Pavlakos et al.  #b10  extended the stacked-hourglass network  #b21  originally designed to predict 2D heatmaps of each joint to make it predict 3D volumetric heatmaps. Tome et al.  #b43  also extended a 2D pose estimator called Convolutional Pose Machine (CPM)  #b20  to make it predict 3D pose. Rogesz and Schmid  #b42  and Varol et al.  #b41  augmented the training data with synthetic images and trained CNNs to predict 3D poses from real images. Sun et al.  #b18  designed a unified network that can regress both 2D and 3D poses at the same time given an image. Hence during training time, in-the-wild images which do not have any ground truth 3D poses can be combined with the data with ground truth 3D poses. A similar idea of exploiting in-the-wild images to learn pose structure was used by Fang et al.  #b44 . They learned a pose grammar that encodes the possible human pose configurations.Using temporal information Since estimating poses for each frame individually leads to incoherent and jittery predictions over a sequence, many approaches tried to exploit temporal information  #b45  #b46  #b23  #b47  #b14 . Andriluka et al.  #b45  used tracking-by-detection to associate 2D poses detected in each frame individually and used them to retrieve 3D pose. Tekin et al.  #b46  used a CNN to first align bounding boxes of successive frames so that the person in the image is always at the center of the box and then extracted 3D HOG features densely over the spatio-temporal volume from which they regress the 3D pose of the central frame. Mehta et al.  #b14  implemented a real-time system for 3D pose estimation that applies temporal filtering across 2D and 3D poses from previous frames to predict a temporally consistent 3D pose. Lin et al.[13] performed a multi-stage sequential refinement using LSTMs to predict 3D pose sequences using previously predicted 2D pose representations and 3D pose. We focus on predicting temporally consistent 3D poses by learning the temporal context of a sequence using a form of sequence-to-sequence network. Unlike Lin et al.[13] our method does not need multiple stages of refinement. It is simpler and requires fewer parameters to train, leading to much improved performance.",
        "Our Approach": "Network Design We designed a sequence-to-sequence network with LSTM units and residual connections on the decoder side to predict a temporally coherent sequence of 3D poses given a sequence of 2D joint locations. Figure 2 shows the architecture of our network. The motivation behind using a sequence-to-sequence network comes from its application on the task of Neural Machine Translation (NMT) by Sutskever et al. [1], where their model translates a sentence in one language to a sentence in another language e.g. English to French. In a language translation model, the input and output sentences can have different lengths.Although our case is analogous to the NMT, the input and output sequences always have the same length while the input vectors to the encoder and decoder have different dimensions.The encoder side of our network takes a sequence of 2D poses and encodes them in a fixed size high dimensional vector in the hidden state of its final LSTM unit. Since the LSTMs are excellent in memorizing events and information from the past, the encoded vector stores the 2D pose information of all the frames. The initial state of the decoder is initialized by the final state of the encoder. A ST ART token is passed as initial input to the decoder, which in our case is a vector of ones, telling it to start decoding. Given a 3D pose estimate y t at a time step t each decoder unit predicts the 3D pose for next time step y t+1 . Note that the order of the input sequence is reversed as recommended by Sutskever et al. [1]. The shortcut connections on the decoder side cause each decoder unit to estimate the amount of perturbation in the 3D pose from the previous frame instead of having to estimate the actual 3D pose for each frame. As suggested by He et al.  #b28 , such a mapping is easier to learn for the network.We use layer normalization  #b30  and recurrent dropout  #b31  to regularize our network. Ba et al.  #b30  came up with the idea of layer normalization which estimates the normalization statistics (mean and standard deviation) from the summed inputs to the recurrent neurons of hidden layer on a single training example to regularize the RNN units. Similarly, Zaremba et al.  #b31  proposed the idea of applying dropout only on the non-recurrent connections of the network with a certain probability p while always keeping the recurrent connections intact because they are necessary for the recurrent units to remember the information from the past.Loss function Given a sequence of 2D joint locations as input, our network predicts a sequence of 3D joint locations relative to the root node (central hip). We predict each 3D pose in the camera coordinate space instead of predicting them in an arbitrary global frame as suggested by Martinez et al.  #b27 .We impose a temporal smoothness constraint on the predicted 3D joint locations to ensure that the prediction of each joint in one frame does not differ too much from its previous frame. Because the 2D pose detectors work on individual frames, even with the minimal movement of the subject in the image, the detections from successive frames may vary, particularly for the joints which move fast or are prone to occlusion. Hence, we made an assumption that the subject does not move too much in successive frames given the frame rate is high enough. Therefore, we added the L2 norm of the first order derivative on the 3D joint locations with respect to time to our loss function during training. This constraint helps us to estimate 3D poses reliably even when the 2D pose detector fails for a few frames within the temporal window without any post-processing.Empirically we found that certain joints are more difficult to estimate accurately e.g. wrist, ankle, elbow compared to others. To address this issue, we partitioned the joints into three disjoint sets torso head, limb leg and limb arm based on their contribution to overall error. We observed that the joints connected to the torso and the head e.g. hips, shoulders, neck are always predicted with high accuracy compared to those joints belonging to the limbs and therefore put them in the set torso head. The joints of the limbs, especially the joints on the arms, are always more difficult to predict due to their high range of motion and occlusion. We put the knees and the ankles in the set limb leg and the elbow and wrist in limb arm. We multiply the derivatives of each set of joints with different scalar values based on their contribution to the overall error.Therefore our loss function consists of the sum of two separate terms: Mean Squared Error (MSE) of N different sequences of 3D joint locations; and the mean of the L2 norm of the first order derivative of N sequences of 3D joint locations with respect to time, where the joints are divided into three disjoint sets.The MSE over N sequences, each of T time-steps, of 3D joint locations is given byL( \u0176, Y) = 1 N T N i=1 T t=1 \u0176i,t -Y i,t2 2.(1)Here, \u0176 denotes the estimated 3D joint locations while Y denotes 3D ground truth.The mean of L2 norm of the first order derivative of N sequences of 3D joint locations, each of length T , with respect to time is given by\u2207 t \u0176 2 2 = 1 N (T -1) N i=1 T t=2 \u03b7 \u0176TH i,t -\u0176TH i,t-1 2 2 + \u03c1 \u0176LL i,t -\u0176LL i,t-1 2 2 + \u03c4 \u0176LA i,t -\u0176LA i,t-1 2 2 .(2)In the above equation, \u0176TH , \u0176LL and \u0176LA denotes the predicted 3D locations of joints belonging to the sets torso head, limb leg and limb arm respectively. The \u03b7, \u03c1 and \u03c4 are scalar hyper-parameters to control the significance of the derivatives of 3D locations of each of the three set of joints. A higher weight is assigned to the set of joints which are generally predicted with higher error. The overall loss function for our network is given asL = min \u0176 \u03b1L( \u0176, Y) + \u03b2 \u2207 t \u0176 2 2 .(3)Here \u03b1 and \u03b2 are scalar hyper-parameters regulating the importance of each of the two terms in the loss function.",
        "Experimental Evaluation": "Datasets and protocols We perform quantitative evaluation on the Human 3.6M  #b32  dataset and on the HumanEva dataset  #b48 . Human 3.6M, to the best of our knowledge, is the largest publicly available dataset for human 3D pose estimation. The dataset contains 3.6 million images of 7 different professional actors performing 15 everyday activities like walking, eating, sitting, making a phone call. The dataset consists of 2D and 3D joint locations for each corresponding image. Each video is captured using 4 different calibrated high resolution cameras. In addition to 2D and 3D pose ground truth, the dataset also provides ground truth for bounding boxes, the camera parameters, the body proportion of all the actors and high resolution body scans or meshes of each actor. Hu-manEva, on the other hand, is a much smaller dataset. It has been largely used to benchmark previous work over the last decade. Most of the methods report results on two different actions and on three actors. For qualitative evaluation, we used the some videos from Youtube and the Human3.6M dataset.We follow the standard protocols of the Human3.6M dataset used in the literature. We used subjects 1, 5, 6, 7, and 8 for training, and subjects 9 and 11 for testing and the error is evaluated on the predicted 3D pose without any transformation. We refer this as protocol #1. Another common approach used by many to evaluate their methods is to align the predicted 3D pose with the ground truth using a similarity transformation (Procrustes analysis). We refer this as protocol #2. We use the average error per joint in millimeters between the estimated and the ground truth 3D pose relative to the root node as the error metric. For the HumanEva dataset, we report results on each subject and action separately after performing rigid alignment with the ground truth data, following the protocol used by the previous methods.",
        "2D detections": "We fine-tuned a model of stacked-hourglass network  #b21 , initially trained on the MPII dataset  #b49  (a benchmark dataset for 2D pose estimation), on the images of the Human3.6M dataset to obtain 2D pose estimations for each image. We used the bounding box information provided with the dataset to first compute the center of the person in the image and then cropped a 440 \u00d7 440 region across the person and resized it to 256 \u00d7 256. We fine-tuned the network for 250 iterations and used a batch size of 3 and a learning rate of 2.5e -4. Baselines Since many of the previous methods are based on single frame predictions, we used two baselines for comparison. To show that our method is much better than naive post processing, we applied a mean filter and a median filter on the 3D pose predictions of Martinez et al.  #b27 . We used a window size of 5 frames and a stride length of 1 to apply the filters. Although non-rigid structure from motion (NRSFM) is one of the most general approaches for any 3D reconstruction problem from a sequence of 2D correspondences, we did not use it as a baseline because Zhou et al.  #b23  did not find NRSFM techniques to be effective for 3D human pose estimation.They found that the NRSFM techniques do not work well with slow camera motion. Since the videos in the Human3.6M dataset  #b32  are captured by stationary cameras,the subjects in the dataset do not rotate that much to provide alternative views for NRSFM algorithm to perform well. Another reason is that human pose reconstruction is a specialized problem in which constraints from human body structure apply.Data pre-processing We normalized the 3D ground truth poses, the noisy 2D pose estimates from stacked-hourglass network and the 2D ground truth  #b21  by subtracting the mean and dividing by standard deviation. We do not predict the 3D location of the root joint i.e. central hip joint and hence zero center the 3D joint locations relative to the global position of the root node. To obtain the ground truth 3D poses in camera coordinate space, an inverse rigid body transformation is applied on the the ground truth 3D poses in global coordinate space using the given camera parameters. To generate both training and test sequences, we translated a sliding window of length T by one frame. Hence there is an overlap between the sequences. This gives us more data to train on, which is always an advantage for deep learning systems. During test time, we initially predict the first T frames of the sequence and slide the window by a stride length of 1 to predict the next frame using the previous frames.Training details We trained our network for 100 epochs, where each epoch makes a complete pass over the entire Human 3.6M dataset. We used the Adam  #b50  optimizer for training the network with a learning rate of 1e -5 which is decayed exponentially per iteration. The weights of the LSTM units are initialized by Xavier uniform initializer  #b51 . We used a mini-batch batch size of 32 i.e. 32 sequences. For most of our experiments we used a sequence length of 5, because it allows faster training with high accuracy. We experimented with different sequence lengths and found sequence length 4, 5 and 6 to generally give better results, which we will discuss in detail in the results section. We trained a single model for all the action classes. Our code is implemented in Tensorflow. We perform cross-validation on the training set to select the hyper-parameter values \u03b1 and \u03b2 of our loss function to 1 and 5 respectively. Similarly, using crossvalidation, the three hyper-parameters of the temporal consistency constraint \u03b7, \u03c1 and \u03c4 , are set to 1, 2.5 and 4 respectively. A single training step for sequences of length 5 takes only 34 ms approximately, while a forward pass takes only about 16ms on NVIDIA Titan X GPU. Therefore given the 2D joint locations from a pose detector, our network takes about 3.2ms to predict 3D pose per frame.",
        "Quantitative results": "Evaluation on estimated 2D pose As mentioned before, we used a sequence length of 5 to perform both qualitative and quantitative evaluation of our network. The results on Human3.6M dataset  #b32  under protocol #1 are shown in Table 1. From the table we observe that our model achieves the lowest error for every action class under protocol #1, unlike many of the previous state-of-the-art methods.Note that we train a single model for all the action classes unlike many other methods which trained a model for each action class. Our network significantly improves the state-of-the-art result of Sun et al.  #b18  by approximately 12.1% (by 7.2 mm). The results under protocol #2, which aligns the predictions to the ground truth using a rigid body similarity transform before computing the error, is reported in Table 2. Our network improves the reported state-of-the-art results by 8.09% (by 3.7 mm) and achieves the lowest error for each action in protocol #2 as well. From the results, we observe the effectiveness of exploiting temporal information across multiple sequences. By using the information of temporal context, our network reduced the overall error in estimating 3D joint locations, especially on actions like phone, photo, sit and sitting down on which most previous methods did not perform well due to heavy occlusion. We also observe that our method outperforms both the baselines by a large margin on both the protocols. This shows that our method learned the temporal context of the sequences and predicted temporally consistent 3D poses, which naive postprocessing techniques like temporal mean and median filters over frame-wise prediction failed to do. Like most previous methods, we report the results on action classes Walking and Jogging of the HumanEva  #b48  dataset in Table 3. We obtained the lowest error in four of the six cases and the lowest average error for the two actions. We also obtained the second best result on subject 2 of action Walking. However, HumanEva is a smaller dataset than Human3.6M and the same subjects appear in both training and testing. Evaluation on 2D ground truth As suggested by Martinez et al.  #b27 , we also found that the more accurate the 2D joint locations are, the better are the estimates for 3D pose. We trained our model on ground truth 2D poses for a sequence length of 5. The results under protocol #1 are reported in Table 1. As seen from the table, our model improves the lower bound error of Martinez et al.  #b27  by almost 13.8%. The results on ground truth 2D joint input for protocol #2 are reported in Table 4. When there is no noise in 2D joint locations, our network performs better than the models by Martinez et al.  #b27  and Moreno-Nouguer  #b26 . These results suggest that the information of temporal consistency from previous frames is a valuable cue for the task of estimating 3D pose even when the detections are noise free.Moreno-Nouguer  #b26   Table 4. Performance of our system trained with ground truth 2D pose of Hu-man3.6M  #b32  dataset and tested with different levels of additive Gaussian noise (Top) and on 2D pose predictions from stacked-hourglass  #b21  pose detector (Bottom)under protocol #2.",
        "Robustness to noise": "We carried out some experiments to test the tolerance of our model to different levels of noise in the input data by training our network on 2D ground truth poses and testing on inputs corrupted by different levels of Gaussian noise. Table 4 shows how our final model compares against the models by Moreno-Nouguer  #b26  and Martinez et al.  #b27 . Our network is significantly more robust than Moreno-Nouguer's model  #b26 . When compared against Martinez et al.  #b27  our network performs better when the level of input noise is low i.e. standard deviation less than or equal to 10. However, for higher levels of noise our network performs slightly worse than Martinez et al.  #b27 . We would like to attribute the cause of this to the temporal smoothness constraint imposed during training which distributes the error of individual frames over the entire sequence. However, its usefulness can be observed in the qualitative results (See Figure 4.1 and Figure 3).   Ablative analysis To show the usefulness of each component and design decision of our network, we perform an ablative analysis. We follow protocol #1 for performing ablative analysis and trained a single model for all the actions. The results are reported in Table 5. We observe that the biggest improvement in result is due the the residual connections on the decoder side, which agrees with the hypothesis of He et al.  #b28 . Removing the residual connections massively increases the error by 50.5 mm. When we do not apply layer normalization on LSTM units, the error increases by 9.2 mm. On the other hand when dropout is not performed, the error raises by 6.4 mm. When both layer normalization and recurrent dropout are not used the results get worse by 7.6 mm. Although the temporal consistency constraint may seem to have less impact (only 0.8 mm) quantitatively on the performance of our network, it ensures that the predictions over a sequence are smooth and temporally consistent which is apparent from our qualitative results as seen in Figure 4.1 and Figure 3.GT ESTIMATED ESTIMATED GTTo show the effectiveness of our model on detections from different 2D pose detectors, we also experimented with the detections from CPM  #b20  and from stacked-hourglass  #b21  (SH) module which is not fine-tuned on Human3.6M dataset. We observe that even for the non-fine tuned stacked hourglass detections, our model achieves the state-of-the-art results. For detections from CPM, our model achieves competitive accuracy for the predictions.Performance on different sequence lengths The results reported so far have been for input and output sequences of length 5. We carried out experiments to see how our network performs for different sequence lengths ranging from 2 to 10. The results are shown in Figure 4. As can be seen, the performance of our network remains stable for sequences of varying lengths. Even for a sequence length of 2, which only considers the previous and the current frame, our model generates very good results. Particularly the best results were obtained for length 4, 5 and 6. However, we chose sequence length 5 for carrying out our experiments as a compromise between training time and accuracy. ",
        "Qualitative Analysis": "We provide qualitative results on some videos of Human3.6M and Youtube. We apply the model trained on the Human3.6M dataset on some videos gathered from Youtube, The bounding box for each person in the Youtube video is labeled manually and for Human3.6M the ground truth bounding box is used. The 2D poses are detected using the stacked-hourglass model fine-tuned on Human3.6M data. The qualitative result for Youtube videos is shown in Figure 4.1 and for Human3.6M in Figure 3. The real advantage of using the temporal smoothness constraint during training is apparent in these figures. For Figure 4.1, we can see that even when the 2D pose estimator breaks or generates extremely noisy detections, our system can recover temporally coherent 3D poses by exploiting the temporal consistency information. A similar trend can also be found for Human3.6M videos in Figure 3, particularly for the action sitting down of subject 11. We have provided more qualitative results in the supplementary material.",
        "Conclusion": "Both the quantitative and qualitative results for our network show the effectiveness of exploiting temporal information over multiple sequences to estimate 3D poses which are temporally smooth. Our network achieved the best accuracy till date on all of the 15 action classes in the Human3.6M dataset  #b32 . Particularly, most of the previous methods struggled with actions which have a high degree of occlusion like taking photo, talking on the phone, sitting and sitting down. Our network has significantly better results on these actions. Additionally we found that our network is reasonably robust to noisy 2D poses. Although the contribution of temporal smoothness constraint is not apparent in the ablative analysis in Table 5, its effectiveness is clearly visible in the qualitative results, particularly on challenging Youtube videos (see Figure 4.1).Our network effectively demonstrates the power of using temporal context information which we achieved using a sequence-to-sequence network that can be trained efficiently in a reasonably quick time. Also our network makes predictions from 2D poses at 3ms per frame on average which suggests that, given the 2D pose detector is real time, our network can be applied in real-time scenarios."
    },
    {},
    {
        "b0": [
            "Discuss Eating Greet Phone Photo Pose Purch",
            "",
            "",
            "",
            "Direct"
        ],
        "b1": [
            "",
            "",
            "Baseline 1 ( [24] + median filter)",
            ""
        ],
        "b2": [
            "",
            "",
            "Baseline 2 ( [24] + mean filter)",
            ""
        ],
        "b3": [
            "",
            "",
            "",
            ""
        ],
        "b4": [
            "Sequence to sequence learning with neural networks",
            "",
            "",
            "",
            "Sutskever",
            "Vinyals",
            "Le"
        ],
        "b5": [
            "3D human pose from silhouettes by relevance vector regression",
            "",
            "",
            "",
            "Agarwal",
            "Triggs"
        ],
        "b6": [
            "Recovering 3D human body configurations using shape contexts",
            "",
            "",
            "",
            "Mori",
            "Malik"
        ],
        "b7": [
            "Fast algorithms for large scale conditional 3D prediction",
            "",
            "",
            "",
            "Bo",
            "Sminchisescu",
            "Kanaujia",
            "Metaxas"
        ],
        "b8": [
            "Fast pose estimation with parametersensitive hashing",
            "",
            "",
            "",
            "Shakhnarovich",
            "Viola",
            "Darrell"
        ],
        "b9": [
            "Structured prediction of 3d human pose with deep neural networks",
            "",
            "",
            "",
            "Tekin",
            "Katircioglu",
            "Salzmann",
            "Lepetit",
            "Fua"
        ],
        "b10": [
            "Coarse-to-fine volumetric prediction for single-image 3D human pose",
            "",
            "",
            "",
            "Pavlakos",
            "Zhou",
            "Derpanis",
            "Daniilidis"
        ],
        "b11": [
            "d human pose estimation from monocular images with deep convolutional neural network",
            "",
            "",
            "",
            "Li",
            "Chan"
        ],
        "b12": [
            "",
            "",
            "Monocular 3d human pose estimation using transfer learning and improved cnn supervision",
            ""
        ],
        "b13": [
            "Deep kinematic pose regression",
            "",
            "",
            "",
            "Zhou",
            "Sun",
            "Zhang",
            "Liang",
            "Wei"
        ],
        "b14": [
            "Vnect: Real-time 3d human pose estimation with a single rgb camera",
            "",
            "",
            "",
            "Mehta",
            "Sridhar",
            "Sotnychenko",
            "Rhodin",
            "Shafiei",
            "Seidel",
            "Xu",
            "Casas",
            "Theobalt"
        ],
        "b15": [
            "Monocular 3d human pose estimation by predicting depth on joints",
            "",
            "",
            "",
            "Nie",
            "Wei",
            "Zhu"
        ],
        "b16": [
            "Recurrent 3d pose sequence machines",
            "",
            "",
            "",
            "Lin",
            "Lin",
            "Liang",
            "Wang",
            "Chen"
        ],
        "b17": [
            "3d human pose estimation using convolutional neural networks with 2d pose information",
            "",
            "",
            "",
            "Park",
            "Hwang",
            "Kwak"
        ],
        "b18": [
            "Compositional human pose regression",
            "",
            "",
            "",
            "Sun",
            "Shang",
            "Liang",
            "Wei"
        ],
        "b19": [
            "Learning to fuse 2d and 3d image cues for monocular body pose estimation",
            "",
            "",
            "",
            "Tekin",
            "Marquez Neila",
            "Salzmann",
            "Fua"
        ],
        "b20": [
            "Convolutional pose machines",
            "",
            "",
            "",
            "Wei",
            "Ramakrishna",
            "Kanade",
            "Sheikh"
        ],
        "b21": [
            "Stacked hourglass networks for human pose estimation",
            "",
            "",
            "",
            "Newell",
            "Yang",
            "Deng"
        ],
        "b22": [
            "Reconstructing 3d human pose from 2d image landmarks",
            "",
            "",
            "",
            "Ramakrishna",
            "Kanade",
            "Sheikh"
        ],
        "b23": [
            "Sparseness meets deepness: 3d human pose estimation from monocular video",
            "",
            "",
            "",
            "Zhou",
            "Zhu",
            "Leonardos",
            "Derpanis",
            "Daniilidis"
        ],
        "b24": [
            "Pose-conditioned joint angle limits for 3d human pose reconstruction",
            "",
            "",
            "",
            "Akhter",
            "Black"
        ],
        "b25": [
            "Keep it smpl: Automatic estimation of 3d human pose and shape from a single image",
            "",
            "",
            "",
            "Bogo",
            "Kanazawa",
            "Lassner",
            "Gehler",
            "Romero",
            "Black"
        ],
        "b26": [
            "3d human pose estimation from a single image via distance matrix regression",
            "",
            "",
            "",
            "Moreno-Noguer"
        ],
        "b27": [
            "A simple yet effective baseline for 3d human pose estimation",
            "",
            "",
            "",
            "Martinez",
            "Hossain",
            "Romero",
            "Little"
        ],
        "b28": [
            "Deep residual learning for image recognition",
            "",
            "",
            "",
            "He",
            "Zhang",
            "Ren",
            "Sun"
        ],
        "b29": [
            "Long short-term memory",
            "",
            "",
            "",
            "Hochreiter",
            "Schmidhuber"
        ],
        "b30": [
            "",
            "",
            "Layer normalization",
            ""
        ],
        "b31": [
            "",
            "",
            "Recurrent neural network regularization",
            ""
        ],
        "b32": [
            "Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments",
            "",
            "",
            "",
            "Ionescu",
            "Papava",
            "Olaru",
            "Sminchisescu"
        ],
        "b33": [
            "Estimating anthropometry and pose from a single uncalibrated image",
            "",
            "",
            "",
            "Barron",
            "Kakadiaris"
        ],
        "b34": [
            "View independent human body pose estimation from a single perspective image",
            "",
            "",
            "",
            "Parameswaran",
            "Chellappa"
        ],
        "b35": [
            "",
            "",
            "Determination of 3D human body postures from a single view. Computer Vision, Graphics and Image Processing",
            ""
        ],
        "b36": [
            "3d human pose reconstruction using millions of exemplars",
            "",
            "",
            "",
            "Jiang"
        ],
        "b37": [
            "Reconstruction of articulated objects from point correspondences in a single uncalibrated image",
            "",
            "",
            "",
            "Taylor"
        ],
        "b38": [
            "3D Pose from Motion for Cross-view Action Recognition via Non-linear Circulant Temporal Encoding",
            "",
            "",
            "",
            "Gupta",
            "Martinez",
            "Little",
            "Woodham"
        ],
        "b39": [
            "3d human pose estimation= 2d pose estimation+ matching",
            "",
            "",
            "",
            "Chen",
            "Ramanan"
        ],
        "b40": [
            "Robust estimation of 3d human poses from a single image",
            "",
            "",
            "",
            "Wang",
            "Wang",
            "Lin",
            "Yuille",
            "Gao"
        ],
        "b41": [
            "Learning from synthetic humans",
            "",
            "",
            "",
            "Varol",
            "Romero",
            "Martin",
            "Mahmood",
            "Black",
            "Laptev",
            "Schmid"
        ],
        "b42": [
            "Mocap-guided data augmentation for 3D pose estimation in the wild",
            "",
            "",
            "",
            "Rogez",
            "Schmid"
        ],
        "b43": [
            "Lifting from the deep: Convolutional 3d pose estimation from a single image",
            "",
            "",
            "",
            "Tome",
            "Russell",
            "Agapito"
        ],
        "b44": [
            "",
            "",
            "Learning knowledge-guided pose grammar machine for 3d human pose estimation",
            ""
        ],
        "b45": [
            "Monocular 3d pose estimation and tracking by detection",
            "",
            "",
            "",
            "Andriluka",
            "Roth",
            "Schiele"
        ],
        "b46": [
            "Direct prediction of 3d body poses from motion compensated sequences",
            "",
            "",
            "",
            "Tekin",
            "Rozantsev",
            "Lepetit",
            "Fua"
        ],
        "b47": [
            "Marker-less 3d human motion capture with monocular image sequence and height-maps",
            "",
            "",
            "",
            "Du",
            "Wong",
            "Liu",
            "Han",
            "Gui",
            "Wang",
            "Kankanhalli",
            "Geng"
        ],
        "b48": [
            "Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion",
            "",
            "",
            "",
            "Sigal",
            "Balan",
            "Black"
        ],
        "b49": [
            "2d human pose estimation: New benchmark and state of the art analysis",
            "",
            "",
            "",
            "Andriluka",
            "Pishchulin",
            "Gehler",
            "Schiele"
        ],
        "b50": [
            "Adam: A method for stochastic optimization",
            "",
            "",
            "",
            "Kingma",
            "Ba"
        ],
        "b51": [
            "Understanding the difficulty of training deep feedforward neural networks",
            "",
            "",
            "",
            "Glorot",
            "Bengio"
        ],
        "b52": [
            "Monocular image 3d human pose estimation under self-occlusion",
            "",
            "",
            "",
            "Radwan",
            "Dhall",
            "Goecke"
        ],
        "b53": [
            "A joint model for 2d and 3d pose estimation from a single image",
            "",
            "",
            "",
            "Simo-Serra",
            "Quattoni",
            "Torras",
            "Moreno-Noguer"
        ],
        "b54": [
            "Twin Gaussian processes for structured prediction",
            "",
            "",
            "",
            "Bo",
            "Sminchisescu"
        ],
        "b55": [
            "Depth sweep regression forests for estimating 3d human pose from images",
            "",
            "",
            "",
            "Kostrikov",
            "Gall"
        ],
        "b56": [
            "A dual-source approach for 3d pose estimation from a single image",
            "",
            "",
            "",
            "Yasin",
            "Iqbal",
            "Kruger",
            "Weber",
            "Gall"
        ]
    },
    {
        "tab_1": "Table 1 .1Results showing the errors action-wise on Human3.6M[29] under Protocol #1 (no rigid alignment or similarity transform applied in post-processing). Note that our results reported here are for sequence of length 5. SA indicates that a model was trained for each action, and MA indicates that a single model was trained for all actions. GT indicates that the network was trained on ground truth 2D pose. The bold-faced numbers represent the best result while underlined numbers represent the second best.",
        "tab_2": "Table 2 .2Results showing the errors action-wise on Human3.6M[29] dataset under protocol #2 (Procrustes alignment to the ground truth in post-processing). Note that the results reported here are for sequence of length 5. The 14j annotation indicates that the body model considers 14 body joints while 17j means considers 17 body joints. (SA) annotation indicates per-action model while (MA) indicates single model used for all actions. The bold-faced numbers represent the best result while underlined numbers represent the second best. The results of the methods are obtained from the original papers, except for (*), which were obtained from[22].Protocol #2Direct. Discuss Eating Greet Phone Photo Pose Purch. Sitting SitingD Smoke Wait WalkD Walk WalkT AvgAkhter & Black [21]* (MA) 14j199.2 177.6 161.8 197.8 176.2 186.5 195.4 167.3 160.7 173.7 177.8 181.9 176.2 198.6 192.7 181.1Ramakrishna et al [19]* (MA) 14j 137.4 149.3 141.6 154.3 157.7 158.9 141.8 158.1 168.6 175.6 160.4 161.7 150.0 174.8 150.2 157.3Zhou et al [20]* (MA) 14j99.795.887.9 116.8 108.3 107.3 93.595.3 109.1 137.5 106.0 102.2 106.5 110.4 115.2 106.7Rogez et al [9] (MA)---------------87.3Nie et al [12] (MA)62.869.279.6 78.8 80.8 86.9 72.573.996.1 106.9 88.0 70.776.5 71.976.5 79.5Mehta et al [9] (MA) 14j---------------54.6Bogo et al [22] (MA) 14j62.060.267.8 76.5 92.1 77.0 73.075.3 100.3 137.3 83.4 77.386.8 79.787.7 82.3Moreno-Noguer [23] (MA) 14j66.161.784.5 73.7 65.2 67.2 60.967.3 103.574.6 92.6 69.671.5 78.073.2 74.0Tekin et al [16] (MA) 17j---------------50.1Pavlakos et al [7] (MA) 17j---------------51.9Martinez et al. [24] (MA) 17j39.543.246.4 47.0 51.0 56.0 41.440.656.569.4 49.2 45.049.5 38.043.1 47.7Fang et al. [41] (MA) 17j38.241.743.7 44.9 48.5 55.3 40.238.254.564.4 47.2 44.347.3 36.741.7 45.7Baseline 1 ( [24] + median filter)44.146.349.6 50.3 53.2 60.9 43.743.561.274.4 53.0 48.654.7 43.048.5 51.7Baseline 2 ( [24] + mean filter)43.145.048.8 49.0 52.1 59.4 43.542.459.770.9 51.2 46.952.4 40.346.0 50.0Our network (MA) 17j36.937.9 42.8 40.3 46.8 46.7 37.7 36.5 48.952.6 45.6 39.6 43.5 35.2 38.5 42.0",
        "tab_3": "Table 3 .3Results on the HumanEva[45] dataset, and comparison with previous work. The bold-faced numbers represent the best result while underlined numbers represent the second best.WalkingJoggingS1S2S3S1S2S3AvgRadwan et al. [49]75.1 99.8 93.8 79.2 89.8 99.4 89.5Wang et al. [37]71.9 75.7 85.3 62.6 77.7 54.4 71.3Simo-Serra et al. [50] 65.1 48.6 73.5 74.2 46.6 32.2 56.7Bo et al. [51]46.4 30.3 64.9 64.5 48.0 38.2 48.7Kostrikov et al. [52] 44.0 30.9 41.7 57.2 35.0 33.3 40.3Yasin et al. [53]35.8 32.4 41.6 46.6 41.4 35.4 38.9Moreno-Noguer [23] 19.7 13.0 24.9 39.7 20.0 21.0 26.9Pavlakos et al. [7]22.1 21.9 29.0 29.8 23.6 26.0 25.5Lin et al [13]26.5 20.7 38.0 41.0 29.7 29.1 30.8Martinez et al. [24]19.7 17.4 46.8 26.9 18.2 18.6 24.6Fang et al. [41]19.4 16.8 37.4 30.4 17.6 16.3 22.9Ours19.1 13.6 43.9 23.2 16.9 15.5 22.0",
        "tab_6": "Table 5 .5Ablative and hyperparameter sensitivity analysis."
    }
]