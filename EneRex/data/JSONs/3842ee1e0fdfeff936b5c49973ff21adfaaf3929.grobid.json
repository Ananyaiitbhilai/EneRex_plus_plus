[
    {
        "basename": "3842ee1e0fdfeff936b5c49973ff21adfaaf3929.grobid",
        "fulltext": 13,
        "footnote_size": 0,
        "reference": 24,
        "authors": [
            "Tzeng",
            "Hoffman",
            "Saenko",
            "Darrell"
        ]
    },
    {
        "title": "Adversarial Discriminative Domain Adaptation",
        "abstract": "Adversarial learning methods are a promising approach to training robust deep networks, and can generate complex samples across diverse domains. They also can improve recognition despite the presence of domain shift or dataset bias: several adversarial approaches to unsupervised domain adaptation have recently been introduced, which reduce the difference between the training and test domain distributions and thus improve generalization performance. Prior generative approaches show compelling visualizations, but are not optimal on discriminative tasks and can be limited to smaller shifts. Prior discriminative approaches could handle larger domain shifts, but imposed tied weights on the model and did not exploit a GAN-based loss. We first outline a novel generalized framework for adversarial adaptation, which subsumes recent state-of-the-art approaches as special cases, and we use this generalized view to better relate the prior approaches. We propose a previously unexplored instance of our general framework which combines discriminative modeling, untied weight sharing, and a GAN loss, which we call Adversarial Discriminative Domain Adaptation (ADDA). We show that ADDA is more effective yet considerably simpler than competing domain-adversarial methods, and demonstrate the promise of our approach by exceeding state-of-the-art unsupervised adaptation results on standard cross-domain digit classification tasks and a new more difficult cross-modality object classification task.",
        "Introduction": "Deep convolutional networks, when trained on large-scale datasets, can learn representations which are generically usefull across a variety of tasks and visual domains  #b0  #b1 . However, due to a phenomenon known as dataset bias or domain shift  #b2 , recognition models trained along with these rep-  resentations on one large dataset do not generalize well to novel datasets and tasks  #b3  #b0 . The typical solution is to further fine-tune these networks on task-specific datasetshowever, it is often prohibitively difficult and expensive to obtain enough labeled data to properly fine-tune the large number of parameters employed by deep multilayer networks. Domain adaptation methods attempt to mitigate the harmful effects of domain shift. Recent domain adaptation meth-ods learn deep neural transformations that map both domains into a common feature space. This is generally achieved by optimizing the representation to minimize some measure of domain shift such as maximum mean discrepancy  #b4  #b5  or correlation distances  #b6  #b7 . An alternative is to reconstruct the target domain from the source representation  #b8 .Adversarial adaptation methods have become an increasingly popular incarnation of this type of approach which seeks to minimize an approximate domain discrepancy distance through an adversarial objective with respect to a domain discriminator. These methods are closely related to generative adversarial learning  #b9 , which pits two networks against each other-a generator and a discriminator. The generator is trained to produce images in a way that confuses the discriminator, which in turn tries to distinguish them from real image examples. In domain adaptation, this principle has been employed to ensure that the network cannot distinguish between the distributions of its training and test domain examples  #b10  #b11  #b12 . However, each algorithm makes different design choices such as whether to use a generator, which loss function to employ, or whether to share weights across domains. For example,  #b10  #b11  share weights and learn a symmetric mapping of both source and target images to the shared feature space, while  #b12  decouple some layers thus learning a partially asymmetric mapping.In this work, we propose a novel unified framework for adversarial domain adaptation, allowing us to effectively examine the different factors of variation between the existing approaches and clearly view the similarities they each share. Our framework unifies design choices such as weightsharing, base models, and adversarial losses and subsumes previous work, while also facilitating the design of novel instantiations that improve upon existing ones.In particular, we observe that generative modeling of input image distributions is not necessary, as the ultimate task is to learn a discriminative representation. On the other hand, asymmetric mappings can better model the difference in low level features than symmetric ones. We therefore propose a previously unexplored unsupervised adversarial adaptation method, Adversarial Discriminative Domain Adaptation (ADDA), illustrated in Figure 1. ADDA first learns a discriminative representation using the labels in the source domain and then a separate encoding that maps the target data to the same space using an asymmetric mapping learned through a domain-adversarial loss. Our approach is simple yet surprisingly powerful and achieves state-of-the-art visual adaptation results on the MNIST, USPS, and SVHN digits datasets. We also test its potential to bridge the gap between even more difficult cross-modality shifts, without requiring instance constraints, by transferring object classifiers from RGB color images to depth observations.",
        "Related work": "There has been extensive prior work on domain transfer learning, see e.g.,  #b2 . Recent work has focused on transferring deep neural network representations from a labeled source datasets to a target domain where labeled data is sparse or non-existent. In the case of unlabeled target domains (the focus of this paper) the main strategy has been to guide feature learning by minimizing the difference between the source and target feature distributions  #b10  #b11  #b4  #b5  #b7  #b8  #b12 .Several methods have used the Maximum Mean Discrepancy (MMD)  #b2  loss for this purpose. MMD computes the norm of the difference between two domain means. The DDC method  #b4  used MMD in addition to the regular classification loss on the source to learn a representation that is both discriminative and domain invariant. The Deep Adaptation Network (DAN)  #b5  applied MMD to layers embedded in a reproducing kernel Hilbert space, effectively matching higher order statistics of the two distributions. In contrast, the deep Correlation Alignment (CORAL)  #b7  method proposed to match the mean and covariance of the two distributions.Other methods have chosen an adversarial loss to minimize domain shift, learning a representation that is simultaneously discriminative of source labels while not being able to distinguish between domains.  #b11  proposed adding a domain classifier (a single fully connected layer) that predicts the binary domain label of the inputs and designed a domain confusion loss to encourage its prediction to be as close as possible to a uniform distribution over binary labels. The gradient reversal algorithm (ReverseGrad) proposed in  #b10  also treats domain invariance as a binary classification problem, but directly maximizes the loss of the domain classifier by reversing its gradients. DRCN  #b8  takes a similar approach but also learns to reconstruct target domain images.In related work, adversarial learning has been explored for generative tasks. The Generative Adversarial Network (GAN) method  #b9  is a generative deep model that pits two networks against one another: a generative model G that captures the data distribution and a discriminative model D that distinguishes between samples drawn from G and images drawn from the training data by predicting a binary label. The networks are trained jointly using backprop on the label prediction loss in a mini-max fashion: simultaneously update G to minimize the loss while also updating D to maximize the loss (fooling the discriminator). The advantage of GAN over other generative methods is that there is no need for complex sampling or inference during training; the downside is that it may be difficult to train. GANs have been applied to generate natural images of objects, such as digits and faces, and have been extended in several ways. The BiGAN approach  #b13  extends GANs to also learn the inverse mapping from the image data back into the latent Recently the CoGAN  #b12  approach applied GANs to the domain transfer problem by training two GANs to generate the source and target images respectively. The approach achieves a domain invariant feature space by tying the highlevel layer parameters of the two GANs, and shows that the same noise input can generate a corresponding pair of images from the two distributions. Domain adaptation was performed by training a classifier on the discriminator output and applied to shifts between the MNIST and USPS digit datasets. However, this approach relies on the generators finding a mapping from the shared high-level layer feature space to full images in both domains. This can work well for say digits which can be difficult in the case of more distinct domains. In this paper, we observe that modeling the image distributions is not strictly necessary to achieve domain adaptation, as long as the latent feature space is domain invariant, and propose a discriminative approach.",
        "Generalized adversarial adaptation": "We present a general framework for adversarial unsupervised adaptation methods. In unsupervised adaptation, we assume access to source images X s and labels Y s drawn from a source domain distribution p s (x, y), as well as target images X t drawn from a target distribution p t (x, y), where there are no label observations. Our goal is to learn a target representation, M t and classifier C t that can correctly classify target images into one of K categories at test time, despite the lack of in domain annotations. Since direct su-pervised learning on the target is not possible, domain adaptation instead learns a source representation mapping, M s , along with a source classifier, C s , and then learns to adapt that model for use in the target domain.In adversarial adaptive methods, the main goal is to regularize the learning of the source and target mappings, M s and M t , so as to minimize the distance between the empirical source and target mapping distributions: M s (X s ) and M t (X t ). If this is the case then the source classification model, C s , can be directly applied to the target representations, elimating the need to learn a separate target classifier and instead setting,C = C s = C t .The source classification model is then trained using the standard supervised loss below:min Ms,C L cls (X s , Y t ) = E (xs,ys)\u223c(Xs,Yt) - K k=1 1 [k=ys] log C(M s (x s )) (1)We are now able to describe our full general framework view of adversarial adaptation approaches. We note that all approaches minimize source and target representation distances through alternating minimization between two functions. First a domain discriminator, D, which classifies whether a data point is drawn from the source or the target domain. Thus, D is optimized according to a standard supervised loss, L adv D (X s , X t , M s , M t ) where the labels indicate the origin domain, defined below:L adv D (X s , X t , M s , M t ) = -E xs\u223cXs [log D(M s (x s ))] -E xt\u223cXt [log(1 -D(M t (x t )))](2)Second, the source and target mappings are optimized according to a constrained adversarial objective, whose particular instantiation may vary across methods. Thus, we can derive a generic formulation for domain adversarial techniques below:min D L adv D (X s , X t , M s , M t ) min Ms,Mt L adv M (X s , X t , D) s.t. \u03c8(M s , M t )(3)In the next sections, we demonstrate the value of our framework by positioning recent domain adversarial approaches within our framework. We describe the potential mapping structure, mapping optimization constraints (\u03c8(M s , M t )) choices and finally choices of adversarial mapping loss, L adv M .",
        "Method Base model Weight sharing Adversarial loss": "Gradient reversal  #b15  discriminative shared minimax Domain confusion  #b11  discriminative shared confusion CoGAN  #b12  generative unshared GAN ADDA (Ours) discriminative unshared GAN Table 1: Overview of adversarial domain adaption methods and their various properties. Viewing methods under a unified framework enables us to easily propose a new adaptation method, adversarial discriminative domain adaptation (ADDA).",
        "Source and target mappings": "In the case of learning a source mapping M s alone it is clear that supervised training through a latent space discriminative loss using the known labels Y s results in the best representation for final source recognition. However, given that our target domain is unlabeled, it remains an open question how best to minimize the distance between the source and target mappings. Thus the first choice to be made is in the particular parameterization of these mappings.Because unsupervised domain adaptation generally considers target discriminative tasks such as classification, previous adaptation methods have generally relied on adapting discriminative models between domains  #b11  #b15 . With a discriminative base model, input images are mapped into a feature space that is useful for a discriminative task such as image classification. For example, in the case of digit classification this may be the standard LeNet model. However, Liu and Tuzel achieve state of the art results on unsupervised MNIST-USPS using two generative adversarial networks  #b12 . These generative models use random noise as input to generate samples in image space-generally, an intermediate feature of an adversarial discriminator is then used as a feature for training a task-specific classifier.Once the mapping parameterization is determined for the source, we must decide how to parametrize the target mapping M t . In general, the target mapping almost always matches the source in terms of the specific functional layer (architecture), but different methods have proposed various regularization techniques. All methods initialize the target mapping parameters with the source, but different methods choose different constraints between the source and target mappings, \u03c8(M s , M t ). The goal is to make sure that the target mapping is set so as to minimize the distance between the source and target domains under their respective mappings, while crucially also maintaining a target mapping that is category discriminative.Consider a layered representations where each layer parameters are denoted as, M s or M t , for a given set of equivalent layers, { 1 , . . . , n }. Then the space of constraints explored in the literature can be described through layerwise equality constraints as follows:\u03c8(M s , M t ) {\u03c8 i (M i s , M i t )} i\u2208{1...n}(4)where each individual layer can be constrained independently. A very common form of constraint is source and target layerwise equality:\u03c8 i (M i s , M i t ) = (M i s = M i t ).(5)It is also common to leave layers unconstrained. These equality constraints can easily be imposed within a convolutional network framework through weight sharing.For many prior adversarial adaptation methods  #b15  #b11 , all layers are constrained, thus enforcing exact source and target mapping consistency. Learning a symmetric transformation reduces the number of parameters in the model and ensures that the mapping used for the target is discriminative at least when applied to the source domain. However, this may make the optimization poorly conditioned, since the same network must handle images from two separate domains.An alternative approach is instead to learn an asymmetric transformation with only a subset of the layers constrained, thus enforcing partial alignment. Rozantsev et al.  #b16  showed that partially shared weights can lead to effective adaptation in both supervised and unsupervised settings. As a result, some recent methods have favored untying weights (fully or partially) between the two domains, allowing models to learn parameters for each domain individually.",
        "Adversarial losses": "Once we have decided on a parametrization of M t , we employ an adversarial loss to learn the actual mapping. There are various different possible choices of adversarial loss functions, each of which have their own unique use cases. All adversarial losses train the adversarial discriminator using a standard classification loss, L adv D , previously stated in Equation 2. However, they differ in the loss used to train the mapping, L adv M .The gradient reversal layer of  #b15  optimizes the mapping to maximize the discriminator loss directly:L adv M = -L adv D .(6)This optimization corresponds to the true minimax objective for generative adversarial networks. However, this objective can be problematic, since early on during training the discriminator converges quickly, causing the gradient to vanish.When training GANs, rather than directly using the minimax loss, it is typical to train the generator with the standard loss function with inverted labels  #b9 . This splits the optimization into two independent objectives, one for the generator and one for the discriminator, where L adv D remains unchanged, but L adv M becomes:L adv M (X s , X t , D) = -E xt\u223cXt [log D(M t (x t ))]. (7)This objective has the same fixed-point properties as the minimax loss but provides stronger gradients to the target mapping. We refer to this modified loss function as the \"GAN loss function\" for the remainder of this paper.Note that, in this setting, we use independent mappings for source and target and learn only M t adversarially. This mimics the GAN setting, where the real image distribution remains fixed, and the generating distribution is learned to match it.The GAN loss function is the standard choice in the setting where the generator is attempting to mimic another unchanging distribution. However, in the setting where both distributions are changing, this objective will lead to oscillation-when the mapping converges to its optimum, the discriminator can simply flip the sign of its prediction in response. Tzeng et al. instead proposed the domain confusion objective, under which the mapping is trained using a cross-entropy loss function against a uniform distribution  #b11 :L adv M (X s , X t , D) = - d\u2208{s,t} E x d \u223cX d 1 2 log D(M d (x d )) + 1 2 log(1 -D(M d (x d ))) .(8)This loss ensures that the adversarial discriminator views the two domains identically.",
        "Adversarial discriminative domain adaptation": "The benefit of our generalized framework for domain adversarial methods is that it directly enables the development of novel adaptive methods. In fact, designing a new method has now been simplified to the space of making three design choices: whether to use a generative or discriminative base model, whether to tie or untie the weights, and which adversarial learning objective to use. In light of this view we can summarize our method, adversarial discriminative domain adaptation (ADDA), as well as its connection to prior work, according to our choices (see Table 1 \"ADDA\"). Specifically, we use a discriminative base model, unshared weights, and the standard GAN loss. We illustrate our overall sequential training procedure in Figure 3.First, we choose a discriminative base model, as we hypothesize that much of the parameters required to generate convincing in-domain samples are irrelevant for discriminative adaptation tasks. Most prior adversarial adaptive methods optimize directly in a discriminative space for this reason. One counter-example is CoGANs. However, this method has only shown dominance in settings where the source and target domain are very similar such as MNIST and USPS, and in our experiments we have had difficulty getting it to converge for larger distribution shifts.Next, we choose to allow independent source and target mappings by untying the weights. This is a more flexible learing paradigm as it allows more domain specific feature extraction to be learned. However, note that the target domain has no label access, and thus without weight sharing a target model may quickly learn a degenerate solution if we do not take care with proper initialization and training procedures. Therefore, we use the pre-trained source model as an intitialization for the target representation space and fix the source model during adversarial training.In doing so, we are effectively learning an asymmetric mapping, in which we modify the target model so as to match the source distribution. This is most similar to the original generative adversarial learning setting, where a generated space is updated until it is indistinguishable with a fixed real space. Therefore, we choose the inverted label GAN loss described in the previous section.Our proposed method, ADDA, thus corresponds to the following unconstrained optimization:min Ms,C L cls (X s , Y s ) = -E (xs,ys)\u223c(Xs,Ys) K k=1 1 [k=ys] log C(M s (x s )) min D L adv D (X s , X t , M s , M t ) = -E xs\u223cXs [log D(M s (x s ))] -E xt\u223cXt [log(1 -D(M t (x t )))] min Ms,Mt L adv M (X s , X t , D) = -E xt\u223cXt [log D(M t (x t ))].(9)We choose to optimize this objective in stages. We begin by optimizing L cls over M s and C by training using the labeled source data, X s and Y s . Because we have opted to leave M s fixed while learning M t , we can thus optimize L adv D and L adv M without revisiting the first objective term. A summary of this entire training process is provided in Figure 3. We note that the unified framework presented in the previous section has enabled us to compare prior domain adversarial methods and make informed decisions about the different factors of variation. Through this framework we are able to motivate a novel domain adaptation method, ADDA, and offer insight into our design decisions. In the next section we demonstrate promising results on unsupervised adaptation benchmark tasks, studying adaptation across digits and across modalities.",
        "Experiments": "We now evaluate ADDA for unsupervised classification adaptation across four different domain shifts. We explore three digits datasets of varying difficulty: MNIST  #b17 , USPS, and SVHN  #b18 . We additionally evaluate on the NYUD  #b19  dataset to study adaptation across modalities. Example images from all experimental datasets are provided in Figure 4.For the case of digit adaptation, we compare against multiple state-of-the-art unsupervised adaptation methods, all based upon domain adversarial learning objectives. In 3 of 4 of our experimental setups, our method outperforms all competing approaches, and in the last domain shift studied, our approach outperforms all but one competing approach.We also validate our model on a real-world modality adaptation task using the NYU depth dataset. Despite a large domain shift between the RGB and depth modalities, ADDA learns a useful depth representation without any labeled depth data and improves over the nonadaptive baseline by over 50% (relative).",
        "MNIST, USPS, and SVHN digits datasets": "We experimentally validate our proposed method in an unsupervised adaptation task between the MNIST  #b17 , USPS, and SVHN  #b18  digits datasets, which consist 10 classes of digits. Example images from each dataset are visualized in Figure 4 and Table 2. For adaptation between MNIST and USPS, we follow the training protocol established in  #b20 , sampling 2000 images from MNIST and 1800 from USPS. For adaptation between SVHN and MNIST, we use the full training sets for comparison against  #b15 . All experiments are performed in the unsupervised settings, where labels in the target domain are withheld, and we consider adaptation in three directions: MNIST\u2192USPS, USPS\u2192MNIST, and SVHN\u2192MNIST.For these experiments, we use the simple modified LeNet architecture provided in the Caffe source code  #b17  #b21 . When training with ADDA, our adversarial discriminator consists of 3 fully connected layers: two layers with 500 hidden units followed by the final discriminator output. Each of the 500-unit layers uses a ReLU activation function.Results of our experiment are provided in Table 2. On the easier MNIST and USPS shifts ADDA achieves comparable performance to the current state-of-the-art, CoGANs  #b12 , despite being a considerably simpler model. This provides compelling evidence that the machinery required to generate images is largely irrelevant to enabling effective adaptation. Additionally, we show convincing results on the challenging SVHN and MNIST task in comparison to other methods, indicating that our method has the potential to generalize to a variety of settings. In contrast, we were unable to get CoGANs to converge on SVHN and MNIST-because the domains are so disparate, we were unable to train coupled generators for them.",
        "Modality adaptation": "We use the NYU depth dataset  #b19 , which contains bounding box annotations for 19 object classes in 1449 images from indoor scenes. The dataset is split into a train (381  In order to ensure that the same instance is not seen in both domains, we use the RGB images from the train split as the source domain and the depth images from the val split as the target domain. This corresponds to 2,186 labeled source images and 2,401 unlabeled target images. Figure 4 visualizes samples from each of the two domains.We consider the task of adaptation between these RGB and HHA encoded depth images  #b22 , using them as source and target domains respectively. Because the bounding boxes are tight and relatively low resolution, accurate classification is quite difficult, even when evaluating in-domain. In addition, the dataset has very few examples for certain classes, such as toilet and bathtub, which directly translates to reduced classification performance.For this experiment, our base architecture is the VGG-16 architecture, initializing from weights pretrained on Ima-geNet  #b23 . This network is then fully fine-tuned on the source domain for 20,000 iterations using a batch size of 128. When training with ADDA, the adversarial discrim-inator consists of three additional fully connected layers: 1024 hidden units, 2048 hidden units, then the adversarial discriminator output. With the exception of the output, these additionally fully connected layers use a ReLU activation function. ADDA training then proceeds for another 20,000 iterations, again with a batch size of 128.We find that our method, ADDA, greatly improves classification accuracy for this task. For certain categories, like counter, classification accuracy goes from 2.9% under the source only baseline up to 44.7% after adaptation. In general, average accuracy across all classes improves significantly from 13.9% to 21.1%. However, not all classes improve. Three classes have no correctly labeled target images before adaptation, and adaptation is unable to recover performance on these classes. Additionally, the classes of pillow and nightstand suffer performance loss after adaptation.For additional insight on what effect ADDA has on classification, Figure 5 plots confusion matrices before adaptation, after adaptation, and in the hypothetical best-case scenario where the target labels are present. Examining the confusion matrix for the source only baseline reveals that the domain shift is quite large-as a result, the network is poorly conditioned and incorrectly predicts pillow for the majority of Source only 0.000 0.010 0.011 0.124 0.188 0.029 0.041 0.047 0.000 0.000 0.069 0.000 0.039 0.587 0.000 0.008 0.010 0.000 0.000 0.139 ADDA (Ours) 0.000 0.146 0.046 0.229 0.344 0.447 0.025 0.023 0.000 0.018 0.292 0.081 0.020 0.297 0.021 0.116 0.143 0.091 0.000 0.211 Train on target 0.105 0.531 0.494 0.295 0.619 0.573 0.057 0.636 0.120 0.291 0.576 0.189 0.235 0.630 0.362 0.248 0.357 0.303 0.647 0.468 Table 3: Adaptation results on the NYUD  #b19  dataset, using RGB images from the train set as source and depth images from the val set as target domains. We report here per class accuracy due to the large class imbalance in our target set (indicated in # instances). Overall our method improves average per category accuracy from 13.9% to 21.1%.  the dataset. This tendency to output pillow also explains why the source only model achieves such abnormally high accuracy on the pillow class, despite poor performance on the rest of the classes. In contrast, the classifier trained using ADDA predicts a much wider variety of classes. This leads to decreased accuracy for the pillow class, but significantly higher accuracies for many of the other classes. Additionally, comparison with the \"train on target\" model reveals that many of the mistakes the ADDA model makes are reasonable, such as confusion between the chair and table classes, indicating that the ADDA model is learning a useful representation on depth images.",
        "Conclusion": "We have proposed a unified framework for unsupervised domain adaptation techniques based on adversarial learning objectives. Our framework provides a simplified and cohesive view by which we may understand and connect the similarities and differences between recently proposed adaptation methods. Through this comparison, we are able to understand the benefits and key ideas from each approach and to combine these strategies into a new adaptation method, ADDA.We present evaluation across four domain shifts for our unsupervised adaptation approach. Our method generalizes well across a variety of tasks, achieving strong results on benchmark adaptation datasets as well as a challenging crossmodality adaptation task. Additional analysis indicates that the representations learned via ADDA resemble features learned with supervisory data in the target domain much more closely than unadapted features, providing further evidence that ADDA is effective at partially undoing the effects of domain shift."
    },
    {},
    {
        "b0": [
            "Decaf: A deep convolutional activation feature for generic visual recognition",
            "",
            "",
            "",
            "Donahue",
            "Jia",
            "Vinyals",
            "Hoffman",
            "Zhang",
            "Tzeng",
            "Darrell"
        ],
        "b1": [
            "How transferable are features in deep neural networks?",
            "",
            "",
            "",
            "Yosinski",
            "Clune",
            "Bengio",
            "Lipson"
        ],
        "b2": [
            "",
            "",
            "Covariate shift and local learning by distribution matching",
            ""
        ],
        "b3": [
            "Unbiased look at dataset bias",
            "",
            "",
            "",
            "Torralba",
            "Efros"
        ],
        "b4": [
            "",
            "",
            "Deep domain confusion: Maximizing for domain invariance",
            ""
        ],
        "b5": [
            "Learning transferable features with deep adaptation networks",
            "",
            "",
            "",
            "Long",
            "Wang"
        ],
        "b6": [
            "Return of frustratingly easy domain adaptation",
            "",
            "",
            "",
            "Sun",
            "Feng",
            "Saenko"
        ],
        "b7": [
            "Deep CORAL: correlation alignment for deep domain adaptation",
            "",
            "",
            "",
            "Sun",
            "Saenko"
        ],
        "b8": [
            "Deep reconstruction-classification networks for unsupervised domain adaptation",
            "",
            "",
            "",
            "Ghifary",
            "Bastiaan Kleijn",
            "Zhang",
            "Balduzzi",
            "Li"
        ],
        "b9": [
            "Generative adversarial nets",
            "",
            "",
            "",
            "Goodfellow",
            "Pouget-Abadie",
            "Mirza",
            "Xu",
            "Warde-Farley",
            "Ozair",
            "Courville",
            "Bengio"
        ],
        "b10": [
            "Unsupervised domain adaptation by backpropagation",
            "",
            "",
            "",
            "Ganin",
            "Lempitsky"
        ],
        "b11": [
            "Simultaneous deep transfer across domains and tasks",
            "",
            "",
            "",
            "Tzeng",
            "Hoffman",
            "Darrell",
            "Saenko"
        ],
        "b12": [
            "",
            "",
            "Coupled generative adversarial networks",
            ""
        ],
        "b13": [
            "",
            "",
            "Adversarial feature learning",
            ""
        ],
        "b14": [
            "",
            "",
            "Conditional generative adversarial nets",
            ""
        ],
        "b15": [
            "Domainadversarial training of neural networks",
            "",
            "",
            "",
            "Ganin",
            "Ustinova",
            "Ajakan",
            "Germain",
            "Larochelle",
            "Franc \u00b8ois Laviolette",
            "Marchand",
            "Lempitsky"
        ],
        "b16": [
            "",
            "",
            "Beyond sharing weights for deep domain adaptation",
            ""
        ],
        "b17": [
            "Gradient-based learning applied to document recognition",
            "",
            "",
            "",
            "Lecun",
            "Bottou",
            "Bengio",
            "Haffner"
        ],
        "b18": [
            "Reading digits in natural images with unsupervised feature learning",
            "",
            "",
            "",
            "Netzer",
            "Wang",
            "Coates",
            "Bissacco",
            "Wu",
            "Ng"
        ],
        "b19": [
            "Indoor segmentation and support inference from rgbd images",
            "",
            "",
            "",
            "Silberman",
            "Hoiem",
            "Kohli",
            "Fergus"
        ],
        "b20": [
            "Transfer feature learning with joint distribution adaptation",
            "",
            "",
            "",
            "Long",
            "Wang",
            "Ding",
            "Sun",
            "Yu"
        ],
        "b21": [
            "",
            "",
            "Convolutional architecture for fast feature embedding",
            ""
        ],
        "b22": [
            "Learning rich features from rgb-d images for object detection and segmentation",
            "",
            "",
            "",
            "Gupta",
            "Girshick",
            "Arbel\u00e1ez",
            "Malik"
        ],
        "b23": [
            "",
            "",
            "Very deep convolutional networks for large-scale image recognition",
            ""
        ]
    },
    {
        "tab_1": "Table 2 :2Experimental results on unsupervised adaptation among MNIST, USPS, and SVHN.MNIST \u2192 USPSUSPS \u2192 MNISTSVHN \u2192 MNISTMethod\u2192\u2192\u2192Source only0.752 \u00b1 0.0160.571 \u00b1 0.0170.601 \u00b1 0.011Gradient reversal0.771 \u00b1 0.0180.730 \u00b1 0.0200.739 [16]Domain confusion0.791 \u00b1 0.0050.665 \u00b1 0.0330.681 \u00b1 0.003CoGAN0.912 \u00b1 0.0080.891 \u00b1 0.008did not convergeADDA (Ours)0.894 \u00b1 0.0020.901 \u00b1 0.0080.760 \u00b1 0.018images), val (414 images) and test (654) sets. To performour cross-modality adaptation, we first crop out tight bound-ing boxes around instances of these 19 classes present inthe dataset and evaluate on a 19-way classification task overobject crops."
    }
]