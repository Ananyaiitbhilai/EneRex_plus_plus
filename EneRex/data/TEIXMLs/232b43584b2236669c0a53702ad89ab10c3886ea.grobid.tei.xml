<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Implicit Quantile Networks for Distributional Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Will</forename><surname>Dabney</surname></persName>
							<email>&lt;wdabney@google.com&gt;</email>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
								<address>
									<addrLine>1 DeepMind</addrLine>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
								<address>
									<addrLine>1 DeepMind</addrLine>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
							<email>trovski@google.com&gt;</email>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
								<address>
									<addrLine>1 DeepMind</addrLine>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Silver</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
								<address>
									<addrLine>1 DeepMind</addrLine>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rémi</forename><surname>Munos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
								<address>
									<addrLine>1 DeepMind</addrLine>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Implicit Quantile Networks for Distributional Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">08E610398A8E48B0CB9015A486985DD4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-11-21T16:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we build on recent advances in distributional reinforcement learning to give a generally applicable, flexible, and state-of-the-art distributional variant of DQN. We achieve this by using quantile regression to approximate the full quantile function for the state-action return distribution. By reparameterizing a distribution over the sample space, this yields an implicitly defined return distribution and gives rise to a large class of risk-sensitive policies. We demonstrate improved performance on the 57 Atari 2600 games in the ALE, and use our algorithm's implicitly defined distributions to study the effects of risk-sensitive policies in Atari games.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Distributional reinforcement learning <ref type="bibr" target="#b18">(Jaquette, 1973;</ref><ref type="bibr" target="#b35">Sobel, 1982;</ref><ref type="bibr" target="#b45">White, 1988;</ref><ref type="bibr">Morimura et al., 2010b;</ref><ref type="bibr" target="#b5">Bellemare et al., 2017)</ref> focuses on the intrinsic randomness of returns within the reinforcement learning (RL) framework. As the agent interacts with the environment, irreducible randomness seeps in through the stochasticity of these interactions, the approximations in the agent's representation, and even the inherently chaotic nature of physical interaction <ref type="bibr" target="#b48">(Yu et al., 2016)</ref>. Distributional RL aims to model the distribution over returns, whose mean is the traditional value function, and to use these distributions to evaluate and optimize a policy.</p><p>Any distributional RL algorithm is characterized by two aspects: the parameterization of the return distribution, and the distance metric or loss function being optimized. Together, these choices control assumptions about the random returns and how approximations will be traded off. Categorical DQN <ref type="bibr">(Bellemare et al., 2017, C51)</ref> combines a categorical distribution and the cross-entropy loss with the Cramér-minimizing projection <ref type="bibr" target="#b32">(Rowland et al., 2018)</ref>. For Proceedings of the 35 th International Conference on Machine <ref type="bibr">Learning, Stockholm, Sweden, PMLR 80, 2018.</ref> Copyright 2018 by the author(s). this, it assumes returns are bounded in a known range and trades off mean-preservation at the cost of overestimating variance.</p><p>C51 outperformed all previous improvements to DQN on a set of 57 Atari 2600 games in the Arcade Learning Environment <ref type="bibr" target="#b4">(Bellemare et al., 2013)</ref>, which we refer to as the Atari-57 benchmark. Subsequently, several papers have built upon this successful combination to achieve significant improvements to the state-of-the-art in Atari-57 <ref type="bibr" target="#b15">(Hessel et al., 2018;</ref><ref type="bibr" target="#b14">Gruslys et al., 2018)</ref>, and challenging continuous control tasks <ref type="bibr" target="#b3">(Barth-Maron et al., 2018)</ref>.</p><p>These algorithms are restricted to assigning probabilities to an a priori fixed, discrete set of possible returns. <ref type="bibr" target="#b9">Dabney et al. (2018)</ref> propose an alternate pair of choices, parameterizing the distribution by a uniform mixture of Diracs whose locations are adjusted using quantile regression. Their algorithm, QR-DQN, while restricted to a discrete set of quantiles, automatically adapts return quantiles to minimize the Wasserstein distance between the Bellman updated and current return distributions. This flexibility allows QR-DQN to significantly improve on C51's Atari-57 performance.</p><p>In this paper, we extend the approach of <ref type="bibr" target="#b9">Dabney et al. (2018)</ref>, from learning a discrete set of quantiles to learning the full quantile function, a continuous map from probabilities to returns. When combined with a base distribution, such as U ([0, 1]), this forms an implicit distribution capable of approximating any distribution over returns given sufficient network capacity. Our approach, implicit quantile networks (IQN), is best viewed as a simple distributional generalization of the DQN algorithm <ref type="bibr" target="#b24">(Mnih et al., 2015)</ref>, and provides several benefits over QR-DQN.</p><p>First, the approximation error for the distribution is no longer controlled by the number of quantiles output by the network, but by the size of the network itself, and the amount of training. Second, IQN can be used with as few, or as many, samples per update as desired, providing improved data efficiency with increasing number of samples per training update. Third, the implicit representation of the return distribution allows us to expand the class of policies to more fully take advantage of the learned distribution. Specifically, by taking the base distribution to be non-uniform, we expand the class of policies to -greedy policies on arbitrary distortion risk measures <ref type="bibr" target="#b47">(Yaari, 1987;</ref><ref type="bibr" target="#b41">Wang, 1996)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:1806.06923v1 [cs.LG] 14 Jun 2018</head><p>We begin by reviewing distributional reinforcement learning, related work, and introducing the concepts surrounding risk-sensitive RL. In subsequent sections, we introduce our proposed algorithm, IQN, and present a series of experiments using the Atari-57 benchmark, investigating the robustness and performance of IQN. Despite being a simple distributional extension to DQN, and forgoing any other improvements, IQN significantly outperforms QR-DQN and nearly matches the performance of Rainbow, which combines many orthogonal advances. In fact, in human-starts as well as in the hardest Atari games (where current RL agents still underperform human players) IQN improves over Rainbow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background / Related Work</head><p>We consider the standard RL setting, in which the interaction of an agent and an environment is modeled as a Markov Decision Process (X , A, R, P, γ) <ref type="bibr" target="#b31">(Puterman, 1994)</ref>, where X and A denote the state and action spaces, R the (state-and action-dependent) reward function, P (•|x, a) the transition kernel, and γ ∈ (0, 1) a discount factor. A policy π(•|x) maps a state to a distribution over actions.</p><p>For an agent following policy π, the discounted sum of future rewards is denoted by the random variable</p><formula xml:id="formula_0">Z π (x, a) = ∞ t=0 γ t R(x t , a t ), where x 0 = x, a 0 = a, x t ∼ P (•|x t-1 , a t-1 ), and a t ∼ π(•|x t ). The action-value function is defined as Q π (x, a) = E [Z π (x, a)],</formula><p>and can be characterized by the Bellman equation</p><formula xml:id="formula_1">Q π (x, a) = E [R(x, a)] + γE P,π [Q π (x , a )] .</formula><p>The objective in RL is to find an optimal policy π * , which maximizes E[Z π ], i.e. Q π * (x, a) ≥ Q π (x, a) for all π and all x, a. One approach is to find the unique fixed point</p><formula xml:id="formula_2">Q * = Q π *</formula><p>of the Bellman optimality operator <ref type="bibr" target="#b6">(Bellman, 1957)</ref>:</p><formula xml:id="formula_3">Q(x, a) = T Q(x, a) := E [R(x, a)] + γE P max a Q(x , a ).</formula><p>To this end, Q-learning <ref type="bibr" target="#b44">(Watkins, 1989)</ref> iteratively improves an estimate, Q θ , of the optimal action-value function, Q * , by repeatedly applying the Bellman update:</p><formula xml:id="formula_4">Q θ (x, a) ← E [R(x, a)] + γE P max a Q θ (x , a ) .</formula><p>The action-value function can be approximated by a parameterized function Q θ (e.g. a neural network), and trained by minimizing the squared temporal difference (TD) error,</p><formula xml:id="formula_5">δ 2 t = r t + γ max a ∈A Q θ (x t+1 , a ) -Q θ (x t , a t ) 2 ,</formula><p>over samples (x t , a t , r t , x t+1 ) observed while following an -greedy policy over Q θ . This policy acts greedily with respect to Q θ with probability 1and uniformly at random otherwise. DQN <ref type="bibr" target="#b24">(Mnih et al., 2015)</ref> uses a convolutional neural network to parameterize Q θ and the Q-learning algorithm to achieve human-level play on the Atari-57 benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Distributional RL</head><p>In distributional RL, the distribution over returns (the law of Z π ) is considered instead of the scalar value function Q π that is its expectation. This change in perspective has yielded new insights into the dynamics of RL <ref type="bibr" target="#b2">(Azar et al., 2012)</ref>, and been a useful tool for analysis <ref type="bibr" target="#b20">(Lattimore &amp; Hutter, 2012)</ref>. Empirically, distributional RL algorithms show improved sample complexity and final performance, as well as increased robustness to hyperparameter variation <ref type="bibr" target="#b3">(Barth-Maron et al., 2018)</ref>.</p><p>An analogous distributional Bellman equation of the form</p><formula xml:id="formula_6">Z π (x, a) D = R(x, a) + γZ π (X , A )</formula><p>can be derived, where A D = B denotes that two random variables A and B have equal probability laws, and the random variables X and A are distributed according to P (•|x, a) and π(•|x ), respectively. <ref type="bibr">Morimura et al. (2010a)</ref> defined the distributional Bellman operator explicitly in terms of conditional probabilities, parameterized by the mean and scale of a Gaussian or Laplace distribution, and minimized the Kullback-Leibler (KL) divergence between the Bellman target and the current estimated return distribution. However, the distributional Bellman operator is not a contraction in the KL.</p><p>As with the scalar setting, a distributional Bellman optimality operator can be defined by</p><formula xml:id="formula_7">T Z(x, a) D := R(x, a) + γZ(X , arg max a ∈A E Z(X , a )),</formula><p>with X distributed according to P (•|x, a). While the distributional Bellman operator for policy evaluation is a contraction in the p-Wasserstein distance <ref type="bibr" target="#b5">(Bellemare et al., 2017)</ref>, this no longer holds for the control case. Convergence to the optimal policy can still be established, but requires a more involved argument. <ref type="bibr" target="#b5">Bellemare et al. (2017)</ref> parameterize the return distribution as a categorical distribution over a fixed set of equidistant points and minimize the KL divergence to the projected distributional Bellman target. Their algorithm, C51, outperformed previous DQN variants on the Atari-57 benchmark. Subsequently, <ref type="bibr" target="#b15">Hessel et al. (2018)</ref> combined C51 with enhancements such as prioritized experience replay <ref type="bibr" target="#b34">(Schaul et al., 2016)</ref>, n-step updates <ref type="bibr" target="#b36">(Sutton, 1988)</ref>, and the dueling architecture <ref type="bibr" target="#b43">(Wang et al., 2016)</ref>, leading to the Rainbow agent, current state-of-the-art in Atari-57.</p><p>The categorical parameterization, using the projected KL loss, has also been used in recent work to improve the critic of a policy gradient algorithm, D4PG, achieving significantly improved robustness and state-of-the-art performance across a variety of continuous control tasks <ref type="bibr" target="#b3">(Barth-Maron et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">p-Wasserstein Metric</head><p>The p-Wasserstein metric, for p ∈ [1, ∞], plays a key role in recent results in distributional RL <ref type="bibr" target="#b5">(Bellemare et al., 2017;</ref><ref type="bibr" target="#b9">Dabney et al., 2018)</ref>. It has also been a topic of increasing interest in generative modeling <ref type="bibr" target="#b1">(Arjovsky et al., 2017;</ref><ref type="bibr" target="#b7">Bousquet et al., 2017;</ref><ref type="bibr" target="#b37">Tolstikhin et al., 2017)</ref>, because unlike the KL divergence, the Wasserstein metric inherently trades off approximate solutions with likelihoods.</p><p>The p-Wasserstein distance is the L p metric on inverse cumulative distribution functions (c.d.f.), also known as quantile functions <ref type="bibr" target="#b28">(Müller, 1997)</ref>. For random variables U and V with quantile functions F -1 U and F -1 V , respectively, the p-Wasserstein distance is given by</p><formula xml:id="formula_8">W p (U, V ) = 1 0 |F -1 U (ω) -F -1 V (ω)| p dω 1/p .</formula><p>The class of optimal transport metrics express distances between distributions in terms of the minimal cost for transporting mass to make the two distributions identical. This cost is given in terms of some metric, c : X × X → R ≥0 , on the underlying space X . The p-Wasserstein metric corresponds to c = L p . We are particularly interested in the Wasserstein metrics due to the predominant use of L p spaces in mean-value reinforcement learning.</p><p>2.3. Quantile Regression for Distributional RL <ref type="bibr" target="#b5">Bellemare et al. (2017)</ref> showed that the distributional Bellman operator is a contraction in the p-Wasserstein metric, but as the proposed algorithm did not itself minimize the Wasserstein metric, this left a theory-practice gap for distributional RL. Recently, this gap was closed, in both directions. First and most relevant to this work, <ref type="bibr" target="#b9">Dabney et al. (2018)</ref> proposed the use of quantile regression for distributional RL and showed that by choosing the quantile targets suitably the resulting projected distributional Bellman operator is a contraction in the ∞-Wasserstein metric. Concurrently, <ref type="bibr" target="#b32">Rowland et al. (2018)</ref> showed the original class of categorical algorithms are a contraction in the Cramér distance, the L 2 metric on cumulative distribution functions.</p><p>By estimating the quantile function at precisely chosen points, QR-DQN minimizes the Wasserstein distance to the distributional Bellman target <ref type="bibr" target="#b9">(Dabney et al., 2018)</ref>. This estimation uses quantile regression, which has been shown to converge to the true quantile function value when mini-mized using stochastic approximation <ref type="bibr" target="#b19">(Koenker, 2005)</ref>.</p><p>In QR-DQN, the random return is approximated by a uniform mixture of N Diracs,</p><formula xml:id="formula_9">Z θ (x, a) := 1 N N i=1 δ θi(x,a) ,</formula><p>with each θ i assigned a fixed quantile target, τi = τi-1+τi 2 for 1 ≤ i ≤ N , where τ i = i/N . These quantile estimates are trained using the Huber (1964) quantile regression loss, with threshold κ,</p><formula xml:id="formula_10">ρ κ τ (δ ij ) = |τ -I{δ ij &lt; 0}| L κ (δ ij ) κ , with L κ (δ ij ) = 1 2 δ 2 ij , if |δ ij | ≤ κ κ(|δ ij | -1 2 κ), otherwise ,</formula><p>on the pairwise TD-errors</p><formula xml:id="formula_11">δ ij = r + γθ j (x , π(x )) -θ i (x, a).</formula><p>At the time of this writing, QR-DQN achieves the best performance on Atari-57, human-normalized mean and median, of all agents that do not combine distributional RL, prioritized replay, and n-step updates <ref type="bibr" target="#b9">(Dabney et al., 2018;</ref><ref type="bibr" target="#b15">Hessel et al., 2018;</ref><ref type="bibr" target="#b14">Gruslys et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Risk in Reinforcement Learning</head><p>Distributional RL algorithms have been theoretically justified for the Wasserstein and Cramér metrics <ref type="bibr" target="#b5">(Bellemare et al., 2017;</ref><ref type="bibr" target="#b32">Rowland et al., 2018)</ref>, and learning the distribution over returns, in and of itself, empirically results in significant improvements to data efficiency, final performance, and stability <ref type="bibr" target="#b5">(Bellemare et al., 2017;</ref><ref type="bibr" target="#b9">Dabney et al., 2018;</ref><ref type="bibr" target="#b14">Gruslys et al., 2018;</ref><ref type="bibr" target="#b3">Barth-Maron et al., 2018)</ref>. However, in each of these recent works the policy used was based entirely on the mean of the return distribution, just as in standard reinforcement learning. A natural question arises: can we expand the class of policies using information provided by the distribution over returns (i.e. to the class of risk-sensitive policies)? Furthermore, when would this larger policy class be beneficial?</p><p>Here, 'risk' refers to the uncertainty over possible outcomes, and risk-sensitive policies are those which depend upon more than the mean of the outcomes. At this point, it is important to highlight the difference between intrinsic uncertainty, captured by the distribution over returns, and parametric uncertainty, the uncertainty over the value estimate typically associated with Bayesian approaches such as PSRL <ref type="bibr" target="#b30">(Osband et al., 2013)</ref> and Kalman TD <ref type="bibr" target="#b12">(Geist &amp; Pietquin, 2010)</ref>. Distributional RL seeks to capture the former, which classic approaches to risk are built upon 1 .</p><p>Expected utility theory states that if a decision policy is consistent with a particular set of four axioms regarding its choices then the decision policy behaves as though it is maximizing the expected value of some utility function U <ref type="bibr" target="#b40">(von Neumann &amp; Morgenstern, 1947)</ref>,</p><formula xml:id="formula_12">π(x) = arg max a E Z(x,a) [U (z)].</formula><p>This is perhaps the most pervasive notion of risk-sensitivity.</p><p>A policy maximizing a linear utility function is called riskneutral, whereas concave or convex utility functions give rise to risk-averse or risk-seeking policies, respectively.</p><p>Many previous studies on risk-sensitive RL adopt the utility function approach <ref type="bibr" target="#b16">(Howard &amp; Matheson, 1972;</ref><ref type="bibr" target="#b23">Marcus et al., 1997;</ref><ref type="bibr" target="#b21">Maddison et al., 2017)</ref>.</p><p>A crucial axiom of expected utility is independence: given random variables X, Y and Z, such that X Y (X preferred over Y ), any mixture between X and Z is preferred to the same mixture between Y and Z (von Neumann &amp; <ref type="bibr" target="#b40">Morgenstern, 1947)</ref>. Stated in terms of the cumulative probability functions,</p><formula xml:id="formula_13">αF X +(1-α)F Z ≥ αF Y +(1-α)F Z , ∀α ∈ [0, 1].</formula><p>This axiom in particular has troubled many researchers because it is consistently violated by human behavior <ref type="bibr" target="#b38">(Tversky &amp; Kahneman, 1992)</ref>. The Allais paradox is a frequently used example of a decision problem where people violate the independence axiom of expected utility theory <ref type="bibr" target="#b0">(Allais, 1990)</ref>.</p><p>However, as <ref type="bibr" target="#b47">Yaari (1987)</ref> showed, this axiom can be replaced by one in terms of convex combinations of outcome values, instead of mixtures of distributions. Specifically, if as before X Y , then for any α ∈ [0, 1] and random variable Z, αF -1</p><formula xml:id="formula_14">X + (1 -α)F -1 Z ≥ αF -1 Y + (1 -α)F -1</formula><p>Z . This leads to an alternate, dual, theory of choice than that of expected utility. Under these axioms the decision policy behaves as though it is maximizing a distorted expectation, for some continuous monotonic function h:</p><formula xml:id="formula_15">π(x) = arg max a ∞ -∞ z ∂ ∂z (h • F Z(x,a) )(z) dz.</formula><p>Such a function h is known as a distortion risk measure, as it distorts the cumulative probabilities of the random variable <ref type="bibr" target="#b41">(Wang, 1996)</ref>. That is, we have two fundamentally equivalent approaches to risk-sensitivity. Either, we choose a utility function and follow the expectation of this utility. Or, we choose a reweighting of the distribution and compute expectation under this distortion measure. Indeed, <ref type="bibr" target="#b47">Yaari (1987)</ref> further showed that these two functions are inverses of each other. The choice between them amounts to a choice 1 One exception is the recent work <ref type="bibr" target="#b25">(Moerland et al., 2017)</ref> towards combining both forms of uncertainty to improve exploration. over whether the behavior should be invariant to mixing with random events or to convex combinations of outcomes.</p><p>Distortion risk measures include, as special cases, cumulative probability weighting used in cumulative prospect theory <ref type="bibr" target="#b38">(Tversky &amp; Kahneman, 1992)</ref>, conditional value at risk <ref type="bibr" target="#b8">(Chow &amp; Ghavamzadeh, 2014)</ref>, and many other methods <ref type="bibr">(Morimura et al., 2010b)</ref>. Recently <ref type="bibr" target="#b22">Majumdar &amp; Pavone (2017)</ref> argued for the use of distortion risk measures in robotics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Implicit Quantile Networks</head><p>We now introduce the implicit quantile network (IQN), a deterministic parametric function trained to reparameterize samples from a base distribution, e.g. τ ∼ U ([0, 1]), to the respective quantile values of a target distribution. IQN provides an effective way to learn an implicit representation of the return distribution, yielding a powerful function approximator for a new DQN-like agent.</p><p>Let F -1 Z (τ ) be the quantile function at τ ∈ [0, 1] for the random variable Z. For notational simplicity we write</p><formula xml:id="formula_16">Z τ := F -1 Z (τ ), thus for τ ∼ U ([0, 1]) the resulting state- action return distribution sample is Z τ (x, a) ∼ Z(x, a).</formula><p>We propose to model the state-action quantile function as a mapping from state-actions and samples from some base distribution, typically τ ∼ U ([0, 1]), to Z τ (x, a), viewed as samples from the implicitly defined return distribution.</p><p>Let β : [0, 1] → [0, 1] be a distortion risk measure, with identity corresponding to risk-neutrality. Then, the distorted expectation of Z(x, a) under β is given by</p><formula xml:id="formula_17">Q β (x, a) := E τ ∼U ([0,1]) Z β(τ ) (x, a) .</formula><p>Notice that the distorted expectation is equal to the expected value of F -1 Z(x,a) weighted by β, that is,</p><formula xml:id="formula_18">Q β = 1 0 F -1 Z (τ )dβ(τ ).</formula><p>The immediate implication of this is that for any β, there exists a sampling distribution for τ such that the mean of Z τ is equal to the distorted expectation of Z under β, that is, any distorted expectation can be represented as a weighted sum over the quantiles <ref type="bibr" target="#b10">(Dhaene et al., 2012)</ref>. Denote by π β the risk-sensitive greedy policy</p><formula xml:id="formula_19">π β (x) = arg max a∈A Q β (x, a).</formula><p>(1)</p><p>For two samples τ, τ ∼ U ([0, 1]), and policy π β , the sampled temporal difference (TD) error at step t is</p><formula xml:id="formula_20">δ τ,τ t = r t + γZ τ (x t+1 , π β (x t+1 )) -Z τ (x t , a t ). (2)</formula><p>Then, the IQN loss function is given by</p><formula xml:id="formula_21">L(x t , a t , r t , x t+1 ) = 1 N N i=1 N j=1 ρ κ τi δ τi,τ j t ,<label>(3)</label></formula><p>where N and N denote the respective number of iid samples τ i , τ j ∼ U ([0, 1]) used to estimate the loss. A corresponding sample-based risk-sensitive policy is obtained by approximating</p><formula xml:id="formula_22">Q β in Equation 1 by K samples of τ ∼ U ([0, 1]): πβ (x) = arg max a∈A 1 K K k=1 Z β(τ k ) (x, a).</formula><p>Implicit quantile networks differ from the approach of Dabney et al. ( <ref type="formula">2018</ref>) in two ways. First, instead of approximating the quantile function at n fixed values of τ we approximate it with Z τ (x, a) ≈ f (ψ(x), φ(τ )) a for some differentiable functions f , ψ, and φ. If we ignore the distributional interpretation for a moment and view each Z τ (x, a) as a separate action-value function, this highlights that implicit quantile networks are a type of universal value function approximator (UVFA) <ref type="bibr" target="#b33">(Schaul et al., 2015)</ref>. There may be additional benefits to implicit quantile networks beyond the obvious increase in representational fidelity. As with UVFAs, we might hope that training over many different τ 's (goals in the case of the UVFA) leads to better generalization between values and improved sample complexity than attempting to train each separately.</p><p>Second, τ , τ , and τ are sampled from continuous, independent, distributions. Besides U ([0, 1]), we also explore risk-sentive policies π β , with non-linear β. The independent sampling of each τ , τ results in the sample TD errors being decorrelated, and the estimated action-values go from being the true mean of a mixture of n Diracs to a sample mean of the implicit distribution defined by reparameterizing the sampling distribution via the learned quantile function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Implementation</head><p>Consider the neural network structure used by the DQN agent <ref type="bibr" target="#b24">(Mnih et al., 2015)</ref>. Let ψ : X → R d be the function computed by the convolutional layers and f : R d → R |A| the subsequent fully-connected layers mapping ψ(x) to the estimated action-values, such that Q(x, a) ≈ f (ψ(x)) a . For our network we use the same functions ψ and f as in DQN, but include an additional function φ : [0, 1] → R d computing an embedding for the sample point τ . We combine these to form the approximation Z τ (x, a) ≈ f (ψ(x) φ(τ )) a , where denotes the element-wise (Hadamard) product.</p><p>As the network for f is not particularly deep, we use the multiplicative form, ψ φ, to force interaction between the convolutional features and the sample embedding. Alternative functional forms, e.g. concatenation or a 'residual' function ψ (1 + φ), are conceivable, and φ(τ ) can be parameterized in different ways. To investigate these, we compared performance across a number of architectural variants on six Atari 2600 games (ASTERIX, ASSAULT, BREAKOUT, MS.PACMAN, QBERT, SPACE INVADERS).</p><p>Full results are given in the Appendix. Despite minor variation in performance, we found the general approach to be robust to the various choices. Based upon the results we used the following function in our later experiments, for embedding dimension n = 64:</p><formula xml:id="formula_23">φ j (τ ) := ReLU( n-1 i=0 cos(πiτ )w ij + b j ).<label>(4)</label></formula><p>After settling on a network architecture, we study the effect of the number of samples, N and N , used in the estimate terms of Equation <ref type="formula" target="#formula_21">3</ref>.</p><p>We hypothesized that N , the number of samples of τ ∼ U ([0, 1]), would affect the sample complexity of IQN, with larger values leading to faster learning, and that with N = 1 one would potentially approach the performance of DQN. This would support the hypothesis that the improved performance of many distributional RL algorithms rests on their effect as auxiliary loss functions, which would vanish in the case of N = 1. Furthermore, we believed that N , the number of samples of τ ∼ U ([0, 1]), would affect the variance of the gradient estimates much like a mini-batch size hyperparameter. Our prediction was that N would have the greatest effect on variance of the long-term performance of the agent.</p><p>We used the same set of six games as before, with our chosen architecture, and varied N, N ∈ {1, 8, 32, 64}. In Figure <ref type="figure">2</ref> we report the average human-normalized scores on the six games for each configuration. affected performance very differently than expected: it had a strong effect on early performance, but minimal impact on long-term performance past N = 8.</p><p>Overall, while using more samples for both distributions is generally favorable, N = N = 8 appears to be sufficient to achieve the majority of improvements offered by IQN for long-term performance, with variation past this point largely insignificant. To our surprise we found that even for N = N = 1, which is comparable to DQN in the number of loss components, the longer term performance is still quite strong (≈ 3× DQN).</p><p>In an informal evaluation, we did not find IQN to be sensitive to K, the number of samples used for the policy, and have fixed it at K = 32 for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Risk-Sensitive Reinforcement Learning</head><p>In this section, we explore the effects of varying the distortion risk measure, β, away from identity. This only affects the policy, π β , used both in Equation 2 and for acting in the environment. As we have argued, evaluating under different distortion risk measures is equivalent to changing the sampling distribution for τ , allowing us to achieve various forms of risk-sensitive policies. We focus on a handful of sampling distributions and their corresponding distortion measures. The first one is the cumulative probability weighting parameterization proposed in cumulative prospect theory <ref type="bibr" target="#b38">(Tversky &amp; Kahneman, 1992;</ref><ref type="bibr" target="#b13">Gonzalez &amp; Wu, 1999)</ref>:</p><formula xml:id="formula_24">CPW(η, τ ) = τ η (τ η + (1 -τ ) η ) 1 η</formula><p>.</p><p>In particular, we use the parameter value η = 0.71 found by <ref type="bibr" target="#b46">Wu &amp; Gonzalez (1996)</ref> to most closely match human subjects. This choice is interesting as, unlike the others we consider, it is neither globally convex nor concave. For small values of τ it is locally concave and for larger values of τ it becomes locally convex. Recall that concavity corresponds to risk-averse and convexity to risk-seeking policies.</p><p>Second, we consider the distortion risk measure proposed by <ref type="bibr" target="#b42">Wang (2000)</ref>, where Φ and Φ -1 are taken to be the standard Normal cumulative distribution function and its inverse:</p><formula xml:id="formula_25">Wang(η, τ ) = Φ(Φ -1 (τ ) + η).</formula><p>For η &lt; 0, this produces risk-averse policies and we include it due to its simple interpretation and ability to switch between risk-averse and risk-seeking distortions.</p><p>Third, we consider a simple power formula for risk-averse (η &lt; 0) or risk-seeking (η &gt; 0) policies:</p><formula xml:id="formula_26">Pow(η, τ ) = τ 1 1+|η| , if η ≥ 0 1 -(1 -τ ) 1 1+|η| ,</formula><p>otherwise .</p><p>Finally, we consider conditional value-at-risk (CVaR):</p><formula xml:id="formula_27">CVaR(η, τ ) = ητ.</formula><p>CVaR has been widely studied in and out of reinforcement learning <ref type="bibr" target="#b8">(Chow &amp; Ghavamzadeh, 2014)</ref>. Its implementation as a modification to the sampling distribution of τ is particularly simple, as it changes</p><formula xml:id="formula_28">τ ∼ U ([0, 1]) to τ ∼ U ([0, η]).</formula><p>Another interesting sampling distribution, not included in our experiments, is denoted Norm(η) and corresponds to τ sampled by averaging η samples from U ([0, 1]).</p><p>In Figure <ref type="figure" target="#fig_2">3</ref> (right) we give an example of a distribution (Neutral) and how each of these distortion measures affects the implied distribution due to changing the sampling distribution of τ . Norm(3) and CPW(.71) reduce the impact of the tails of the distribution, while Wang and CVaR heavily shift the distribution mass towards the tails, creating a riskaverse or risk-seeking preference. Additionally, while CVaR entirely ignores all values corresponding to τ &gt; η, Wang gives these non-zero, but vanishingly small, probability.</p><p>By using these sampling distributions we can induce various risk-sensitive policies in IQN. We evaluate these on the same set of six Atari 2600 games previously used. Our algorithm simply changes the policy to maximize the distorted expectations instead of the usual sample mean. Figure <ref type="figure" target="#fig_2">3</ref> (left) shows our results in this experiment, with average scores reported under the usual, risk-neutral, evaluation criterion.</p><p>Intuitively, we expected to see a qualitative effect from risk-sensitive training, e.g. strengthened exploration from a risk-seeking objective. Although we did see qualitative differences, these did not always match our expectations. For two of the games, ASTERIX and ASSAULT, there is a very significant advantage to the risk-averse policies. Although CPW tends to perform almost identically to the standard risk-neutral policy, and the risk-seeking Wang(1.5) performs as well or worse than risk-neutral, we find that both risk-averse policies improve performance over standard IQN. However, we also observe that the more risk-averse of the two, CVaR(0.1), suffers some loss in performance on two other games (QBERT and SPACE INVADERS).</p><p>Additionally, we note that the risk-seeking policy significantly underperforms the risk-neutral policy on three of the six games.</p><p>It remains an open question as to exactly why we see improved performance for risk-averse policies.</p><p>There are many possible explanations for this phenomenon, e.g. that risk-aversion encodes a heuristic to stay alive longer, which in many games is correlated with increased rewards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Full Atari-57 Results</head><p>Finally, we evaluate IQN on the full Atari-57 benchmark, comparing with the state-of-the-art performance of Rainbow, a distributional RL agent that combines several advances in deep RL <ref type="bibr" target="#b15">(Hessel et al., 2018)</ref>, the closely related algorithm QR-DQN <ref type="bibr" target="#b9">(Dabney et al., 2018)</ref>, prioritized experience replay DQN <ref type="bibr" target="#b34">(Schaul et al., 2016)</ref>, and the original DQN agent <ref type="bibr" target="#b24">(Mnih et al., 2015)</ref>. Note that in this section we use the risk-neutral variant of the IQN, that is, the policy of the IQN agent is the regular -greedy policy with respect to the mean of the state-action return distribution.</p><p>It is important to remember that Rainbow builds upon the distributional RL algorithm C51 <ref type="bibr" target="#b5">(Bellemare et al., 2017)</ref>, but also includes prioritized experience replay <ref type="bibr" target="#b34">(Schaul et al., 2016)</ref>, Double <ref type="bibr">DQN (van Hasselt et al., 2016)</ref>, Dueling Network architecture <ref type="bibr" target="#b43">(Wang et al., 2016)</ref>, Noisy Networks <ref type="bibr" target="#b11">(Fortunato et al., 2017)</ref>, and multi-step updates <ref type="bibr" target="#b36">(Sutton, 1988)</ref>. In particular, besides the distributional update, nstep updates and prioritized experience replay were found to have significant impact on the performance of Rainbow. Our other competitive baseline is QR-DQN, which is currently state-of-the-art for agents that do not combine distributional updates, n-step updates, and prioritized replay.</p><p>Thus, between QR-DQN and the much more complex Rain-bow we compare to the two most closely related, and best performing, agents in published work. In particular, we would expect that IQN would benefit from the additional enhancements in Rainbow, just as Rainbow improved significantly over C51.</p><p>Figure <ref type="figure" target="#fig_3">4</ref> shows the mean (left) and median (right) humannormalized scores during training over the Atari-57 benchmark. IQN dramatically improves over QR-DQN, which itself improves on many previously published results. At 100 million frames IQN has reached the same level of performance as QR-DQN at 200 million frames. Table <ref type="table" target="#tab_1">1</ref> gives a comparison between the same methods in terms of their best, human-normalized, scores per game under the 30 random no-op start condition. These are averages over the given number of seeds. Additionally, using human-starts, IQN achieves 162% median human-normalized score, whereas Rainbow reaches 153% <ref type="bibr" target="#b15">(Hessel et al., 2018)</ref>, see   Finally, we took a closer look at the games in which each algorithm continues to underperform humans, and computed, on average, how far below human-level they perform 2 . We refer to this value as the human-gap 3 metric and give results in Table <ref type="table" target="#tab_1">1</ref>. Interestingly, C51 outperforms QR-DQN in this metric, and IQN outperforms all others. This shows that the remaining gap between Rainbow and IQN is entirely from games on which both algorithms are already super-human.</p><p>The games where the most progress in RL is needed happen to be the games where IQN shows the greatest improvement over QR-DQN and Rainbow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and Conclusions</head><p>We have proposed a generalization of recent work based around using quantile regression to learn the distribution over returns of the current policy. Our generalization leads to a simple change to the DQN agent to enable distributional RL, the natural integration of risk-sensitive policies, and significantly improved performance over existing methods. The IQN algorithm provides, for the first time, a fully integrated distributional RL agent without prior assumptions on the parameterization of the return distribution.</p><p>IQN can be trained with as little as a single sample from each state-action value distribution, or as many as computational limits allow to improve the algorithm's data efficiency. Furthermore, IQN allows us to expand the class of control policies to a large class of risk-sensitive policies connected to distortion risk measures. Finally, we show substantial gains on the Atari-57 benchmark over QR-DQN, and even halving the distance between QR-DQN and Rainbow.</p><p>Despite the significant empirical successes in this paper 2 Details of how this is computed can be found in the Appendix.</p><p>3 Thanks to Joseph Modayil for proposing this metric.</p><p>there are many areas in need of additional theoretical analysis. We highlight a few particularly relevant open questions we were unable to address in the present work. First, samplebased convergence results have been recently shown for a class of categorical distributional RL algorithms <ref type="bibr" target="#b32">(Rowland et al., 2018)</ref>. Could existing sample-based RL convergence results be extended to the QR-based algorithms?</p><p>Second, can the contraction mapping results for a fixed grid of quantiles given by <ref type="bibr" target="#b9">Dabney et al. (2018)</ref> be extended to the more general class of approximate quantile functions studied in this work? Finally, and particularly salient to our experiments with distortion risk measures, theoretical guarantees for risk-sensitive RL have been building over recent years, but have been largely limited to special cases and restricted classes of risk-sensitive policies. Can the convergence of the distribution of returns under the Bellman operator be leveraged to show convergence to a fixed-point in distorted expectations? In particular, can the control results of <ref type="bibr" target="#b5">Bellemare et al. (2017)</ref> be expanded to cover some class of risk-sensitive policies?</p><p>There remain many intriguing directions for future research into distributional RL, even on purely empirical fronts. Hessel et al. ( <ref type="formula">2018</ref>) recently showed that distributional RL agents can be significantly improved, when combined with other techniques. Creating a Rainbow-IQN agent could yield even greater improvements on Atari-57. We also recall the surprisingly rich return distributions found by <ref type="bibr" target="#b3">Barth-Maron et al. (2018)</ref>, and hypothesize that the continuous control setting may be a particularly fruitful area for the application of distributional RL in general, and IQN in particular.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix Architecture and Hyperparameters</head><p>We considered multiple architectural variants for parameterizing an IQN. All of these build on the Q-network of a regular DQN <ref type="bibr" target="#b24">(Mnih et al., 2015)</ref>, which can be seen as the composition of a convolutional stack ψ : X → R For the embedding φ, we considered a number of variants: a learned linear embedding, a learned MLP embedding with a single hidden layer of size n, and a learned linear function of n cosine basis functions of the form cos(πiτ ), i = 1, . . . , n. Each of those was followed by either a ReLU or sigmoid nonlinearity.</p><p>For the merging function m, the simplest choice would be a simple vector concatenation of ψ(x) and φ(τ ). Note however, that the MLP f which takes in the output of m and outputs the action-value quantiles, only has a single hidden layer in the DQN network. Therefore, to force a sufficiently early interaction between the two representations, we also considered a multiplicative function m(ψ, φ) = ψ φ, where denotes the element-wise (Hadamard) product of two vectors, as well as a 'residual' function m(ψ, φ) = ψ (1 + φ).</p><p>Early experiments showed that a simple linear embedding of τ was insufficient to achieve good performance, and the residual version of m didn't show any marked difference to the multiplicative variant, so we do not include results for these here. For the other configurations, Figure <ref type="figure" target="#fig_5">5</ref> shows pairwise comparisons between 1) a cosine basis function embedding and a completely learned MLP embedding, 2) an embedding size (hidden layer size or number of cosine basis elements) 32 and 64, 3) ReLU and sigmoid nonlinearity following the embedding, and 4) concatenation and a multiplicative interaction between ψ(x) and φ(τ ).</p><p>Each comparison 'violin plot' can be understood as a marginalization over the other variants of the architecture, with the human-normalized performance at the end of training, averaged across six Atari 2600 games, on the y-axis. Each white dot corresponds to a configuration (each represented by two seeds), the black dots show the position of our preferred configuration. The width of the colored regions corresponds to a kernel density estimate of the number of configurations at each performance level.</p><p>Our final choice is a multiplicative interaction with a linear function of a cosine embedding, with n = 64 and a ReLU nonlinearity (see Equation <ref type="formula" target="#formula_23">4</ref>), as this configuration yielded the highest performance consistently over multiple seeds. Also noteworthy is the overall robustness of the approach to these variations: most of the configurations consistently outperform the QR-DQN baseline shown as a grey horizontal line for comparison.</p><p>We give pseudo-code for the IQN loss in Algorithm 1. All other hyperparameters for this agent correspond to the ones used by <ref type="bibr" target="#b9">Dabney et al. (2018)</ref>. In particular, the Bellman target is computed using a target network. Notice that IQN will generally be more computationally expensive per-sample than QR-DQN. However, in practice IQN requires many fewer samples per update than QR-DQN so that the actual running times are comparable.</p><p>Algorithm 1 Implicit Quantile Network Loss Require: N, N , K, κ and functions β, Z input x, a, r, x , γ ∈ [0, 1) # Compute greedy next action a * ← arg max a</p><formula xml:id="formula_29">1 K K k Z τk (x , a ), τk ∼ β(•) # Sample quantile thresholds τ i , τ j ∼ U ([0, 1]), 1 ≤ i ≤ N, 1 ≤ j ≤ N # Compute distributional temporal differences δ ij ← r + γZ τ j (x , a * ) -Z τi (x, a), ∀i, j # Compute Huber quantile loss output N i=1 E τ ρ κ τi (δ ij )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation</head><p>The human-normalized scores reported in this paper are given by the formula <ref type="bibr" target="#b39">(van Hasselt et al., 2016;</ref><ref type="bibr" target="#b9">Dabney et al., 2018)</ref> score = agentrandom humanrandom ,</p><p>where agent, human and random are the per-game raw scores (undiscounted returns) for the given agent, a reference human player, and random agent baseline <ref type="bibr" target="#b24">(Mnih et al., 2015)</ref>.</p><p>The 'human-gap' metric referred to at the end of Section 5 builds on the human-normalized score, but emphasizes the remaining improvement for the agent to reach super-human performance. It is given by gap = max(1score, 0), with a value of 1 corresponding to random play, and a value of 0 corresponding to super-human level of performance. To avoid degeneracies in the case of human &lt; random, the quantity is being clipped above at 1. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Network architectures for DQN and recent distributional RL algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>0 NFigure 2 .</head><label>02</label><figDesc>Figure 2. Effect of varying N and N , the number of samples used in the loss function in Equation 3. Figures show human-normalized agent performance, averaged over six Atari games, averaged over first 10M frames of training (left) and last 10M frames of training (right). Corresponding values for baselines: DQN (32, 253) and QR-DQN (144, 1243).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure3. Effects of various changes to the sampling distribution, that is various cumulative probability weightings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Human-normalized mean (left) and median (right) scores on Atari-57 for IQN and various other algorithms. Random seeds shown as traces, with IQN averaged over 5, QR-DQN over 3, and Rainbow over 2 random seeds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>d and an MLP f : R d → R |A| , and extend it by an embedding of the sample point, φ : [0, 1] → R d , and a merging function m : R d × R d → R d , resulting in the function IQN(x, τ ) = f (m(ψ(x), φ(τ ))).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Comparison of architectural variants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">Mean Median Human Gap Seeds</cell></row><row><cell>DQN</cell><cell>228%</cell><cell>79%</cell><cell>0.334</cell><cell>1</cell></row><row><cell>PRIOR.</cell><cell>434%</cell><cell>124%</cell><cell>0.178</cell><cell>1</cell></row><row><cell>C51</cell><cell>701%</cell><cell>178%</cell><cell>0.152</cell><cell>1</cell></row><row><cell cols="2">RAINBOW 1189%</cell><cell>230%</cell><cell>0.144</cell><cell>2</cell></row><row><cell>QR-DQN</cell><cell>864%</cell><cell>193%</cell><cell>0.165</cell><cell>3</cell></row><row><cell>IQN</cell><cell>1019%</cell><cell>218%</cell><cell>0.141</cell><cell>5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Mean and median of scores across 57 Atari 2600 games, measured as percentages of human baseline<ref type="bibr" target="#b29">(Nair et al., 2015)</ref>. Scores are averages over number of seeds.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Human-starts (median)</cell></row><row><cell cols="3">DQN PRIOR. A3C</cell><cell>C51</cell><cell>RAINBOW</cell><cell>IQN</cell></row><row><cell>68%</cell><cell>128%</cell><cell cols="2">116% 125%</cell><cell>153%</cell><cell>162%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Median human-normalized scores for human-starts.</figDesc><table><row><cell cols="2">Implicit Quantile Networks for Distributional Reinforcement Learning</cell></row><row><cell>Mean</cell><cell>Median</cell></row><row><cell>Human-Normalized Score</cell><cell>DQN IQN Prioritized DQN QR-DQN Rainbow</cell></row><row><cell>Training Frames (Million)</cell><cell>Training Frames (Million)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Complete Atari-57 training curves.Figure 7. Raw scores for a single seed across all games, starting with 30 no-op actions. Reference values from<ref type="bibr" target="#b43">(Wang et al., 2016)</ref>.</figDesc><table><row><cell>RANDOM 227.8 5.8 222.4 210.0 719.1 47,388.7 HUMAN 7,127.7 1,719.5 742.0 8,503.3 12,850.0 29,028.1 279,987.0 DQN 1,620.0 978.0 4,280.4 4,359.0 1,364.5 14.2 753.1 455.0 2,360.0 37,187.5 29,900.0 363.9 16,926.5 8,627.5 123.7 2,630.4 585.6 23.1 160.7 50.4 0.1 12.1 88.0 1.7 30.5 385.5 2,090.9 12,017.0 4,657.7 811.0 7,387.8 6,126.0 10,780.5 35,829.4 110,763.0 2,874.5 18,688.9 23,633.0 152.1 1,971.0 12,149.4 -18.6 -16.4 -6.6 0.0 860.5 729.0 -91.7 -38.7 -4.9 0.0 29.6 30.8 65.2 4,334.7 797.4 257.6 2,412.5 8,777.4 173.0 3,351.4 473.0 1,027.0 30,826.4 20,437.8 -11.2 0.9 -1.9 29.0 302.8 768.5 52.0 3,035.0 7,259.0 1,598.0 2,665.5 8,422.3 258.5 22,736.3 26,059.0 0.0 4,753.3 0.0 307.3 6,951.6 3,085.6 2,292.3 8,049.0 8,207.8 761.4 7,242.6 8,485.2 -229.4 6,463.7 -286.1 -20.7 14.6 19.5 24.9 69,571.3 146.7 163.9 13,455.0 13,117.3 1,338.5 17,118.0 7,377.6 11.5 7,845.0 39,544.0 2.2 11.9 63.9 68.4 42,054.7 5,860.6 -17,098.1 -4,336.9 -13,062.3 1,236.3 12,326.7 3,482.8 148.0 1,668.7 1,692.3 664.0 10,250.0 54,282.0 -10.0 6.5 -5.6 -23.8 -8.3 12.2 3,568.0 5,229.2 4,870.0 11.4 167.6 68.1 533.4 11,693.2 9,989.9 0.0 1,187.5 163.0 16,256.9 17,667.9 196,760.4 563.5 4,756.5 2,704.0 3,092.9 54,576.9 18,098.9 Figure 6. GAMES Alien Amidar Assault Asterix Asteroids Atlantis Bank Heist Battle Zone Beam Rider Berzerk Bowling Boxing Breakout Centipede Chopper Command Crazy Climber Defender Demon Attack Double Dunk Enduro Fishing Derby Freeway Frostbite Gopher Gravitar H.E.R.O. Ice Hockey James Bond Kangaroo Krull Kung-Fu Master Montezumas Revenge Ms. Pac-Man Name This Game Phoenix Pitfall! Pong Private Eye Q*Bert River Raid Road Runner Robotank Seaquest Skiing Solaris Space Invaders Star Gunner Surround Tennis Time Pilot Tutankham Up and Down Venture Video Pinball Wizard Of Wor Yars Revenge Zaxxon 32.5 9,173.3 5,363.0</cell><cell>IQN Rainbow PRIOR. DUEL. 3,941.0 2,296.8 11,477.0 375,080.0 1,192.7 395,762.0 1,503.1 35,520.0 30,276.5 3,409.0 46.7 98.9 366.0 7,687.5 13,185.0 162,224.0 41,324.5 72,878.6 -12.5 2,306.4 41.3 33.0 7,413.0 104,368.2 238.0 21,036.5 -0.4 812.0 1,792.0 10,374.4 48,375.0 0.0 3,327.3 15,572.5 70,324.3 0.0 20.9 206.0 18,760.3 20,607.6 62,151.0 27.5 931.6 -19,949.9 133.4 15,311.5 125,117.0 1.2 0.0 7,553.0 245.9 33,879.1 48.0 479,197.0 12,352.0 69,618.1 13,886.0</cell><cell>DQN QR-DQN-1 QR-DQN 4,871 1,641 22,012 29,091 IQN 7,022 2,946 261,025 342,016 4,226 2,898 971,850 978,200 1,249 1,416 39,268 42,244 34,821 42,776 3,117 1,053 77.2 86.5 99.9 99.8 742 734 12,447 11,561 14,667 16,836 161,196 179,082 47,887 53,537 121,551 128,580 21.9 5.6 2,355 2,359 39.0 33.8 34.0 34.0 4,384 4,324 113,585 118,365 995 911 21,395 28,386 -1.7 0.2 4,703 35,108 15,356 15,487 11,447 10,707 76,642 73,512 0.0 0.0 5,821 6,349 21,890 22,682 16,585 56,599 0.0 0.0 21.0 21.0 350 200 572,510 25,750 17,571 17,765 64,262 57,900 59.4 62.5 8,268 30,140 -9,324 -9,289 6,740 8,007 20,972 28,888 77,495 74,677 8.2 9.4 23.6 23.6 10,345 12,236 297 293 71,260 88,148 43.9 1,318 705,662 698,045 25,061 31,190 26,447 28,379 13,112 21,772</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Allais paradox</title>
		<author>
			<persName><forename type="first">M</forename><surname>Allais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Utility and Probability</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="3" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wasserstein Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (ICML)</title>
		<meeting>the 34th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the sample complexity of reinforcement learning with a generative model</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distributional policy gradients</title>
		<author>
			<persName><forename type="first">G</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Muldal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Arcade Learning Environment: an evaluation platform for general agents</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="253" to="279" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A distributional perspective on reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning (ICML)</title>
		<meeting>the 34th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Dynamic Programming</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Bellman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957">1957</date>
			<publisher>Princeton University Press</publisher>
			<pubPlace>Princeton, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Simon-Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schoelkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07642</idno>
		<title level="m">From optimal transport to generative modeling: the vegan cookbook</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Algorithms for CVaR optimization in MDPs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3509" to="3517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributional reinforcement learning with quantile regression</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Remarks on quantiles and distortion risk measures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dhaene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kukush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Linders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Actuarial Journal</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="319" to="328" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Noisy networks for exploration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.10295</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Kalman temporal differences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="483" to="532" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the shape of the probability weighting function</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="129" to="166" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Reactor: a fast and sampleefficient actor-critic agent for reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gruslys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rainbow: combining improvements in deep reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Risk-sensitive markov decision processes</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Matheson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="356" to="369" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust estimation of a location parameter</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="101" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Markov decision processes with a new optimality criterion: discrete time</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Jaquette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="496" to="505" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Quantile Regression</title>
		<author>
			<persName><forename type="first">R</forename><surname>Koenker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PAC bounds for discounted MDPs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lattimore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Algorithmic Learning Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="320" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lawson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05820</idno>
		<title level="m">Particle value functions</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">How should a robot assess risk?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pavone</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.11040</idno>
	</analytic>
	<monogr>
		<title level="m">Towards an axiomatic theory of risk in robotics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Risk sensitive markov decision processes</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fernández-Gaucherand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hernández-Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Coraluppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Systems and Control in the Twenty-First Century</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="263" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Efficient exploration with double uncertain value networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Moerland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Broekens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Jonker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10789</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Parametric return density estimation for reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Morimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hachiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<meeting>the Conference on Uncertainty in Artificial Intelligence (UAI)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>a</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Nonparametric return distribution approximation for reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Morimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hachiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="799" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Integral probability metrics and their generating classes of functions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Probability</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="429" to="443" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Massively parallel methods for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alcicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fearon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Maria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName><surname>Petersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Deep Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">(more) efficient reinforcement learning via posterior sampling</title>
		<author>
			<persName><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3003" to="3011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Puterman</surname></persName>
		</author>
		<author>
			<persName><surname>Markov</surname></persName>
		</author>
		<title level="m">Decision Processes: Discrete Stochastic Dynamic Programming</title>
		<imprint>
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An analysis of categorical distributional reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AISTATS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Universal value function approximators</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1312" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Prioritized experience replay</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The variance of discounted markov decision processes</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Sobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Probability</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="794" to="802" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to predict by the methods of temporal differences</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="44" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schoelkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01558</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein auto-encoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Advances in prospect theory: cumulative representation of uncertainty</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Risk and Uncertainty</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="297" to="323" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with double Q-learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Theory of Games and Economic Behavior</title>
		<author>
			<persName><forename type="first">J</forename><surname>Von Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Morgenstern</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1947">1947</date>
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Premium calculation by transforming the layer premium density</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ASTIN Bulletin: The Journal of the IAA</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="92" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A class of distortion operators for pricing financial and insurance risks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Risk and Insurance</title>
		<imprint>
			<biblScope unit="page" from="15" to="36" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning from delayed rewards</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J C H</forename><surname>Watkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>King&apos;s College</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mean, variance, and probabilistic criteria in finite markov decision processes: a review</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Optimization Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Curvature of the probability weighting function</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1676" to="1690" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The dual theory of choice under risk. Econometrica</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Yaari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Econometric Society</title>
		<imprint>
			<biblScope unit="page" from="95" to="115" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">More than a million ways to be pushed. a high-fidelity experimental dataset of planar pushing</title>
		<author>
			<persName><forename type="first">K.-T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bauza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fazeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="30" to="37" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
