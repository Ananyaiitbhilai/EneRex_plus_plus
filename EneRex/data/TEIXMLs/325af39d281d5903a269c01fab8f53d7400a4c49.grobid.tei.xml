<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ArtTrack: Articulated Multi-person Tracking in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-05-09">9 May 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics Saarland Informatics Campus</orgName>
								<address>
									<settlement>Saabrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics Saarland Informatics Campus</orgName>
								<address>
									<settlement>Saabrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics Saarland Informatics Campus</orgName>
								<address>
									<settlement>Saabrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siyu</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics Saarland Informatics Campus</orgName>
								<address>
									<settlement>Saabrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Evgeny</forename><surname>Levinkov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics Saarland Informatics Campus</orgName>
								<address>
									<settlement>Saabrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics Saarland Informatics Campus</orgName>
								<address>
									<settlement>Saabrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics Saarland Informatics Campus</orgName>
								<address>
									<settlement>Saabrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ArtTrack: Articulated Multi-person Tracking in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-05-09">9 May 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">0AAFFFF20D3CCC7455C90E123B31B4AB</idno>
					<idno type="arXiv">arXiv:1612.01465v3[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-11-21T16:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we propose an approach for articulated tracking of multiple people in unconstrained videos. Our starting point is a model that resembles existing architectures for single-frame pose estimation but is substantially faster. We achieve this in two ways: (1) by simplifying and sparsifying the body-part relationship graph and leveraging recent methods for faster inference, and (2) by offloading a substantial share of computation onto a feed-forward convolutional architecture that is able to detect and associate body joints of the same person even in clutter. We use this model to generate proposals for body joint locations and formulate articulated tracking as spatio-temporal grouping of such proposals. This allows to jointly solve the association problem for all people in the scene by propagating evidence from strong detections through time and enforcing constraints that each proposal can be assigned to one person only. We report results on a public "MPII Human Pose" benchmark and on a new "MPII Video Pose" dataset of image sequences with multiple people. We demonstrate that our model achieves state-of-the-art results while using only a fraction of time and is able to leverage temporal information to improve state-of-the-art for crowded scenes 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper addresses the task of articulated human pose tracking in monocular video. We focus on scenes of realistic complexity that often include fast motions, large variability in appearance and clothing, and person-person occlusions. A successful approach must thus identify the number of people in each video frame, determine locations of the joints of each person and associate the joints over time.</p><p>One of the key challenges in such scenes is that people might overlap and only a subset of joints of the person might be visible in each frame either due to person-person occlusion or truncation by image boundaries (c.f . Fig. <ref type="figure" target="#fig_0">1</ref>). Arguably, resolving such cases correctly requires reasoning beyond purely geometric information on the arrangement of body joints in the image, and requires incorporation of a variety of image cues and joint modeling of several persons.</p><p>The design of our model is motivated by two factors. We would like to leverage bottom-up end-to-end learning to directly capture image information. At the same time we aim to address a complex multi-person articulated tracking problem that does not naturally lend itself to an end-to-end prediction task and for which training data is not available in the amounts usually required for end-to-end learning.</p><p>To leverage the available image information we learn a model for associating a body joint to a specific person in an end-to-end fashion relying on a convolutional network. We then incorporate these part-to-person association responses into a framework for jointly reasoning about assignment of body joints within the image and over time. To that end we use the graph partitioning formulation that has been used for people tracking and pose estimation in the past <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b22">23]</ref>, but has not been shown to enable articulated people tracking.</p><p>To facilitate efficient inference in video we resort to fast inference methods based on local combinatorial optimization <ref type="bibr" target="#b19">[20]</ref> and aim for a sparse model that keeps the number of connections between variables to a minimum. As we demonstrate, in combination with feed-forward reasoning for joint-to-person association this allows us to achieve substantial speed-ups compared to state-of-the-art <ref type="bibr" target="#b13">[14]</ref> while maintaining the same level of accuracy.</p><p>The main contribution of this work is a new articulated tracking model that operates by bottom-up assembly of part detections within each frame and over time. In contrast to <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22]</ref> this model is suitable for scenes with an unknown number of subjects and reasons jointly across multiple people incorporating inter-person exclusion constraints and propagating strong observations to neighboring frames.</p><p>Our second contribution is a formulation for singleframe pose estimation that relies on a sparse graph between body parts and a mechanism for generating body-part proposals conditioned on a person's location. This is in contrast to state-of-the-art approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b13">14]</ref> that perform expensive inference in a full graph and rely on generic bottom-up proposals. We demonstrate that a sparse model with a few spatial edges performs competitively with a fully-connected model while being much more efficient. Notably, a simple model that operates in top-down/bottom-up fashion exceeds the performance of a fully-connected model while being 24x faster at inference time (cf. Tab. 3). This is due to offloading of a large share of the reasoning about body-part association onto a feed-forward convolutional architecture.</p><p>Finally, we contribute a new challenging dataset for evaluation of articulated body joint tracking in crowded realistic environments with multiple overlapping people. Related work. Convolutional networks have emerged as an effective approach to localizing body joints of people in images <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b13">14]</ref> and have also been extended for joint estimation of body configurations over time <ref type="bibr" target="#b11">[12]</ref>, and 3D pose estimation in outdoor environments in multi-camera setting <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">11]</ref>.</p><p>Current approaches are increasingly effective for estimating body configurations of single people <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12]</ref> achieving high accuracies on this task, but are still failing on fast moving and articulated limbs. More complex recent models jointly reason about entire scenes <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16]</ref>, but are too complex and inefficient to directly generalize to image sequences. Recent feed-forward models are able to jointly infer body joints of the same person and even operate over time <ref type="bibr" target="#b11">[12]</ref> but consider isolated persons only and do not generalize to the case of multiple overlapping people. Similarly, <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22]</ref> consider a simplified task of tracking upper body poses of isolated upright individuals.</p><p>We build on recent CNN detectors <ref type="bibr" target="#b13">[14]</ref> that are effective in localizing body joints in cluttered scenes and explore different mechanisms for assembling the joints into multiple person configurations. To that end we rely on a graph partitioning approach closely related to <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b13">14]</ref>. In contrast to <ref type="bibr" target="#b24">[25]</ref> who focus on pedestrian tracking, and <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b13">14]</ref> who perform single frame multi-person pose estimation, we solve a more complex problem of articulated multi-person pose tracking.</p><p>Earlier approaches to articulated pose tracking in monocular videos rely on hand-crafted image representations and focus on simplified tasks, such as tracking upper body poses of frontal isolated people <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b7">8]</ref>, or tracking walking pedestrians with little degree of articulation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. In contrast, we address a harder problem of multi-person articulated pose tracking and do not make assumptions about the type of body motions or activities of people. Our approach is closely related to <ref type="bibr" target="#b16">[17]</ref> who propose a similar formulation based on graph partitioning. Our approach differs from <ref type="bibr" target="#b16">[17]</ref> primarily in the type of body-part proposals and the structure of the spatio-temporal graph. In our approach we introduce a person-conditioned model that is trained to associate body parts of a specific person already at the detection stage. This is in contrast to the approach of <ref type="bibr" target="#b16">[17]</ref> that relies on the generic body-part detectors <ref type="bibr" target="#b13">[14]</ref>. Overview. Our model consists of the two components: (1) a convolutional network for generating body part proposals and (2) an approach to group the proposals into spatiotemporal clusters. In Sec. 2 we introduce a general formulation for multi-target tracking that follows <ref type="bibr" target="#b24">[25]</ref> and allows us to define pose estimation and articulated tracking in a unified framework. We then describe the details of our articulated tracking approach in Sec. 3, and introduce two variants of our formulation: bottom-up (BU) and top-down/bottomup (TD/BU). We present experimental results in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Tracking by Spatio-temporal Grouping</head><p>Our body part detector generates a set of proposals D = {d i } for each frame of the video. Each proposal is given by d i = (t i , d pos i , π i , τ i ), where t i denotes the index of the video frame, d pos i is the spatial location of the proposal in image coordinates, π i is the probability of correct detection, and τ i is the type of the body joint (e.g. ankle or shoulder).</p><p>Let G = (D, E) be a graph whose nodes D are the joint detections in a video and whose edges E connect pairs of detections that hypothetically correspond to the same target.</p><p>The output of the tracking algorithm is a subgraph G = (D , E ) of G, where D is a subset of nodes after filtering redundant and erroneous detections and E are edges linking nodes corresponding to the same target. We specify G via binary variables x ∈ {0, 1} D and y ∈ {0, 1} E that define subsets of edges and nodes included in G . In particular each track will correspond to a connected component in G .</p><p>As a general way to introduce constraints on edge configurations that correspond to a valid tracking solution we introduce a set Z ⊆ {0, 1} D∪E and define a combination of edge and node indicator variables to be feasible if and only if (x, y) ∈ Z. An example of a constraint encoded through Z is that endpoint nodes of an edge included by y must also be included by x. Note that the variables x and y are coupled though Z. Moreover, assuming that (x, y) ∈ Z we are free to set components of x and y independently to maximize the tracking objective.</p><p>Given image observations we compute a set of features for each node and edge in the graph. We denote such node and edge features as f and g respectively. Assuming independence of the feature vectors the conditional probability of indicator functions x of nodes and y of edges given features f and g and given a feasible set Z is given by p(x, y|f, g, Z) ∝ p(Z|x, y)</p><formula xml:id="formula_0">d∈D p(x d |f d ) e∈E p(y e |g e ),<label>(1)</label></formula><p>where p(Z|x, y) assigns a constant non-zero probability to every feasible solution and is equal to zero otherwise. Minimizing the negative log-likelihood of Eq. 1 is equivalent to solving the following integer-linear program:</p><formula xml:id="formula_1">min (x,y)∈Z d∈D c d x d + e∈E d e y e ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">c d = log p(x d =1|f d ) p(x d =0|f d )</formula><p>is the cost of retaining d as part of the solution, and d e = log p(ye=1|g e ) p(ye=0|g e ) is the cost of assigning the detections linked by an edge e to the same track.</p><p>We define the set of constraints Z as in <ref type="bibr" target="#b24">[25]</ref>:</p><formula xml:id="formula_3">∀e = vw ∈ E : y vw ≤ x v (3) ∀e = vw ∈ E : y vw ≤ x w (4) ∀C ∈ cycles(G) ∀e ∈ C : (1 -y e ) ≤ e ∈C\{e} (1 -y e )<label>(5)</label></formula><p>Jointly with the objective in Eq. 2 the constraints (3)-( <ref type="formula" target="#formula_3">5</ref>) define an instance of the minimum cost subgraph multicut problem <ref type="bibr" target="#b24">[25]</ref>. The constraints (3) and (4) ensure that assignment of node and edge variables is consistent. The constraint <ref type="bibr" target="#b4">(5)</ref> ensures that for every two nodes either all or none of the paths between these nodes in graph G are contained in one of the connected components of subgraph G . This constraint is necessary to unambigously assign person identity to a body part proposal based on its membership in a specific connnected component of G .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Articulated Multi-person Tracking</head><p>In Sec. 2 we introduced a general framework for multiobject tracking by solving an instance of the subgraph multicut problem. The subgraph multicut problem is NP-hard, but recent work <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b19">20]</ref> has shown that efficient approximate inference is possible with local search methods. The framework allows for a variety of graphs and connectivity patterns. Simpler connectivity allows for faster and more efficient processing at the cost of ignoring some of the potentially informative dependencies between model variables.</p><p>Our goal is to design a model that is efficient, with as few edges as possible, yet effective in crowded scenes, and that allows us to model temporal continuity and inter-person exclusion. Our articulated tracking approach proceeds by constructing a graph G that couples body part proposals within the same frame and across neighboring frames. In general the graph G will have three types of edges: (1) cross-type edges shown in Fig. <ref type="figure" target="#fig_1">2</ref>  We now define two variants of our model that we denote as Bottom-Up (BU) and Top-Down/Bottom-Up (TD/BU). In the BU model the body part proposals are generated with our publicly available convolutional part detector <ref type="bibr" target="#b13">[14]</ref> <ref type="foot" target="#foot_1">2</ref> . In the TD/BU model we substitute these generic part detectors with a new convolutional body-part detector that is trained to output consistent body configurations conditioned on the person location. This alows to further reduce the complexity of the model graph since the task of associating body parts is addressed within the proposal mechanism. As we show in Sec. 4 this leads to considerable gains in performance and allows for faster inference. Note that the BU and TD/BU models have identical same-type and temporal pairwise terms, but differ in the form of cross-type pairwise terms, and the connectivity of the nodes in G. For both models we rely on the solver from <ref type="bibr" target="#b19">[20]</ref> for inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Bottom-Up Model (BU).</head><p>For each body part proposal d i the detector outputs image location, probability of detection π i , and a label τ i that indicates the type of the detected part (e.g. shoulder or ankle). We directly use the probability of detection to derive the unary costs in Eq. 2 as features f d in this case correspond to the image representation generated by the convolutional network.</p><formula xml:id="formula_4">c di = log(π i /(1 -π i )). Image</formula><p>We consider two connectivity patterns for nodes in the graph G. We either define edges for every pair of proposals which results in a fully connected graph in each image. Alternatively we obtain a sparse version of the model by defining edges for a subset of part types only as is shown in Fig. <ref type="figure" target="#fig_1">2 (a)</ref>. The rationale behind the sparse version is to obtain a simpler and faster version of the model by omitting edges between parts that carry little information about each other's image location (e.g. left ankle and right arm). Edge costs. In our Bottom-Up model the cost of the edges d e connecting two body part detections d i and d j is defined as a function of the detection types τ i and τ j . Following <ref type="bibr" target="#b13">[14]</ref> we thus train for each pair of part types a regression function that predicts relative image location of the parts in the pair. The cost d e is given by the output of the logistic regression given the features computed from offset and angle of the predicted and actual location of the other joint in the pair. We refer to <ref type="bibr" target="#b13">[14]</ref> for more details on these pairwise terms.</p><p>Note that our model generalizes <ref type="bibr" target="#b24">[25]</ref> in that the edge cost depends on the type of nodes linked by the edge. It also generalizes <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b13">14]</ref> by allowing G to be sparse. This is achieved by reformulating the model with a more general type of cycle constraint <ref type="bibr" target="#b4">(5)</ref>, in contrast to simple triangle inequalities used in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b13">14]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Top-Down/Bottom-up Model (TD/BU)</head><p>We now introduce a version of our model that operates by first generating body part proposals conditioned on the locations of people in the image and then performing joint 3 See Sec. 2.1 in <ref type="bibr" target="#b22">[23]</ref> reasoning to group these proposals into spatio-temporal clusters corresponding to different people. We follow the intuition that it is considerably easier to identify and detect individual people (e.g. by detecting their heads) compared to correctly associating body parts such as ankles and wrists to each person. We select person's head as a root part that is responsible for representing the person location, and delegate the task of identifying body parts of the person corresponding to a head location to a convolutional network.</p><p>The structure of TD/BU model is illustrated in Fig. <ref type="figure" target="#fig_3">3</ref> (b) for the simplified case of two distinct head detections. Let us denote the set of all root part detections as D root = {d root i }. For each pair of the root nodes we explicitly set the corresponding edge indicator variables y d root j ,d root k = 0. This implements a "must-not-link" constraint between these nodes, and in combination with the cycle inequality (5) implies that each proposal can be connected to one of the "person nodes" only. The cost for an edge connecting detection proposal d k and a "person node"</p><formula xml:id="formula_5">d root i is based on the con- ditional distribution p d c k (d pos k |d root i</formula><p>) generated by the convolutional network. The output of such network is a set of conditional distributions, one for each node type. We augment the graph G with attractive/repulsive and temporal terms as described in Sec. 3.3 and Sec. 3.4 and set the unary costs for all indicator variables x d to a constant. Any proposal not connected to any of the root nodes is excluded from the final solution. We use the solver from <ref type="bibr" target="#b19">[20]</ref> for consistency, but a simpler KL-based solver as in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b18">19]</ref> could be used as well since the TD/BU model effectively ignores the unary variables x d . The processing stages of TD/BU model are shown in Fig. <ref type="figure" target="#fig_3">3</ref>. Note that the body-part heatmaps change depending on the person-identity signal  provided by the person's neck, and that the bottom-up step was able to correct the predictions on the forearms of the front person. Implementation details. For head detection, we use a version of our model that contains the two head parts (neck and head top). This makes our TD/BU model related to the hierarchical model defined in <ref type="bibr" target="#b13">[14]</ref> that also uses easier-to-detect parts to guide the rest of the inference process. However here we replace all the stages in the hierarchical inference except the first one with a convolutional network.</p><p>The structure of the convolutional network used to generate person-conditioned proposals is shown on Fig. <ref type="figure" target="#fig_6">4</ref>. The network uses the ResNet-101 from <ref type="bibr" target="#b12">[13]</ref> that we modify to bring the stride of the network down to 8 pixels <ref type="bibr" target="#b13">[14]</ref>. The network generates predictions for all body parts after the conv4 4 block. We use the cross-entropy binary classification loss at this stage to predict the part heatmaps. At each training iteration we forward pass an image with multiple people potentially in close proximity to each other. We select a single person from the image and condition the network on the person's neck location by zeroing out the heatmap of the neck joint outside the ground-truth region. We then pass the neck heatmap through a convolutional layer to match the dimensionality of the feature channels and add them to the main stream of the ResNet. We finally add a joint prediction layer at the end of the network with a loss that considers predictions to be correct only if they correspond to the body joints of the selected person. Spatial propagation (SP). In our network the person identity signal is provided by the location of the head. In principle the receptive field size of the network is large enough to propagate this signal to all body parts. However we found that it is useful to introduce an additional mechanism to propagate the person identity signal. To that end we inject intermediate supervision layers for individual body parts arranged in the order of kinematic proximity to the root joint (Fig. <ref type="figure" target="#fig_6">4</ref>). We place prediction layers for shoulders at conv4 8, for elbows and hips at conv4 14 and for knees at conv4 18. We empirically found that such an explicit form of spatial propagation significantly improves performance on joints such as ankles, that are typically far from the head in the image space (see Tab. 2 for details).</p><p>Training. We use Caffe's <ref type="bibr" target="#b17">[18]</ref> ResNet implementation and initialize from the ImageNet-pre-trained models. Networks are trained on the MPII Human Pose dataset <ref type="bibr" target="#b0">[1]</ref> with SGD for 1M iterations with stepwise learning rate (lr=0.002 for 400k, lr=0.0002 for 300k and lr=0.0001 for 300k).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Attractive/Repulsive Edges</head><p>Attractive/repulsive edges are defined between two proposals of the same type within the same image. The costs of these edges is inversely-proportional to distance <ref type="bibr" target="#b13">[14]</ref>. The decision to group two nodes is made based on the evidence from the entire image, which is in contrast to typical nonmaximum suppression based on the state of just two detections. Inversely, these edges prevent grouping of multiple distant hypothesis of the same type, e.g. prevent merging two heads of different people.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Temporal Model</head><p>Regardless of the type of within frame model (BU or TD/BU) we rely on the same type of temporal edges that connect nodes of the same type in adjacent frames. We derive the costs for such temporal edges via logistic regression. Given the feature vector g ij the probability that the two proposals d i and d j in adjacent frames correspond to the same body part is given by:</p><formula xml:id="formula_6">p(y ij = 1|g ij ) = 1/(1 + exp(-ω t , g ij )), where g ij = (∆ L2 ij , ∆ Sif t ij , ∆ DM ij , ∆DM ij ),<label>and</label></formula><formula xml:id="formula_7">∆ L2 ij = d pos i -d pos j 2 , ∆ Sif t ij</formula><p>is Euclidean distance between the SIFT descriptors computed at d pos i and d pos j , and ∆ DM ij and ∆DM ij measure the agreement with the dense motion field computed with the DeepMatching approach of <ref type="bibr" target="#b29">[30]</ref>.</p><p>For SIFT features we specify the location of the detection proposal, but rely on SIFT to identify the local orientation. In cases with multiple local maxima in orientation estimation we compute SIFT descriptor for each orientation and set ∆ Sif t ij to the minimal distance among all pairs of descriptors. We found that this makes the SIFT distance more robust in the presence of rotations of the body limbs.</p><p>We define the features ∆ DM ij and ∆DM ij as in <ref type="bibr" target="#b25">[26]</ref>. Let R i = R(d i ) be an squared image region centered on the part proposal d i . We define ∆ DM ij as a ratio of the number of point correspondences between the regions R i and R j and the total number of point correspondences in either of them. Specifically, let C = {c k |k = 1, . . . , K} be a set of point correspondences between the two images computed with DeepMatching, where c k = (c k 1 , c k 2 ) and c k 1 and c k 2 denote the corresponding points in the first and second image respectively. Using this notation we define:</p><formula xml:id="formula_8">∆ DM ij = |{c k |c k 1 ∈ R i ∧ c k 2 ∈ R j }| |{c k |c k 1 ∈ R i }| + |{c k |c k 2 ∈ R j }| . (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>The rationale behind computing ∆ DM ij by aggregating across multiple correspondences is to make the feature robust to outliers and to inaccuracies in body part detection. ∆DM ij is defined analogously, but using the DeepMatching correspondences obtained by inverting the order of images. Discussion. As we demonstrate in Sec. 4, we found the set of features described above to be complementary to each other. Euclidean distance between proposals is informative for finding correspondences for slow motions, but fails for faster motions and in the presence of multiple people. DeepMatching is usually effective in finding corresponding regions between the two images, but occasionally fails in the case of sudden background changes due to fast motion or large changes in body limb orientation. In these cases SIFT is often still able to provide a meaningful measure of similarity due to its rotation invariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and evaluation measure</head><p>Single frame. We evaluate our single frame models on the MPII Multi-Person dataset <ref type="bibr" target="#b0">[1]</ref>. We report all intermediate results on a validation set of 200 images sampled uniformly at random (MPII Multi-Person Val), while major results and comparison to the state of the art are reported on the test set. Video. In order to evaluate video-based models we introduce a novel "MPII Video Pose" dataset<ref type="foot" target="#foot_2">4</ref> . To this end we manually selected challenging keyframes from MPII Multi-Person dataset. Selected keyframes represent crowded scenes with highly articulated people engaging in various dynamic activities. In addition to each keyframe, we include +/-10 neighboring frames from the corresponding publicly available video sequences, and annotate every second frame <ref type="foot" target="#foot_3">5</ref> . Each body pose was annotated following the standard annotation procedure <ref type="bibr" target="#b0">[1]</ref>, while maintaining person identity throughout the sequence. In contrast to MPII Multi-Person where some frames may contain nonannotated people, we annotate all people participating in the activity captured in the video, and add ignore regions for areas that contain dense crowds (e.g. static spectators in the dancing sequences). In total, our dataset consists of 28 sequences with over 2, 000 annotated poses. Evaluation details. The average precision (AP) measure <ref type="bibr" target="#b22">[23]</ref> is used for evaluation of pose estimation accuracy. For each algorithm we also report run time τ CNN of the proposal generation and τ graph of the graph partitioning stages. All time measurements were conducted on a single core Intel Xeon 2.70GHz. Finally we also evaluate tracking perfomance using standard MOTA metric <ref type="bibr" target="#b3">[4]</ref>. Evaluation on our "MPII Video Pose" dataset is performed on the full frames using the publicly available evaluation kit of <ref type="bibr" target="#b0">[1]</ref>. On MPII Multi-Person we follow the official evaluation protocol <ref type="foot" target="#foot_4">6</ref> and evaluate on groups using the provided rough group location and scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Single-frame models</head><p>We compare the performance of different variants of our Bottom-Up (BU) and Top-Down/Bottom-Up (TD/BU) models introduced in Sec. 3.1 and Sec. 3.2. For BU we consider a model that (1) uses a fully-connected graph with up to 1, 000 detection proposals and jointly performs partitioning and body-part labeling similar to <ref type="bibr" target="#b13">[14]</ref> (BU-full, label); (2) is same as <ref type="bibr" target="#b0">(1)</ref>, but labeling of detection proposals is done based on detection score (BU-full); (3) is same as (2), but uses a sparsely-connected graph (BU-sparse). The results are shown in Tab. 1 <ref type="foot" target="#foot_5">7</ref> . BU-full, label achieves 70.5% AP with a median inference run-time τ graph of 3.06 s/f. BU-full achieves 8× run-time reduction (0.38 vs. 3.06 s/f): prelabeling detection candidates based on detection score significantly reduces the number of variables in the problem graph. Interestingly, pre-labeling also improves the performance (71.9 vs. 70.5% AP): some of the low-scoring detections may complicate the search for an optimal labeling. BU-sparse further reduces run-time (0.22 vs. 0.38 s/f), as  it reduces the complexity of the initial problem by sparsifying the graph, at a price of a drop in performance (70.6 vs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>71.9% AP).</head><p>In Tab. 2 we compare the variants of the TD/BU model. Our TD approach achieves 71.7% AP, performing on par with a more complex BU-full. Explicit spatial propagation (TD+SP) further improves the results (72.5 vs. 71.7% AP). The largest improvement is observed for ankles: progressive prediction that conditions on the close-by parts in the tree hierarchy reduces the distance between the conditioning signal and the location of the predicted body part and simplifies the prediction task. Performing inference (TD/BU+SP) improves the performance to 73.3% AP, due to more optimal assignment of part detection candidates to corresponding persons. Graph simplification in TD/BU allows to further reduce the inference time for graph partitioning (0.08 vs. 0.22 for BU-sparse).</p><p>Comparison to the State of the Art. We compare the proposed single-frame approaches to the state of the art on MPII Multi-Person Test and WAF <ref type="bibr" target="#b8">[9]</ref> datasets. Comparison on MPII is shown in Tab. 3. Both BU-full and TD/BU improve over the best published result of DeeperCut <ref type="bibr" target="#b14">[15]</ref>, achieving 72.9 and 74.3% AP respectively vs. 70.0% AP by DeeperCut. For the TD/BU the improvements on articulated parts (elbows, wrists, ankles, knees) are particularly pronounced. We argue that this is due to using the network that is directly trained to disambiguate body parts of different people, instead of using explicit geometric pairwise terms that only serve as a proxy to person's identity. Overall, the performance of our best TD/BU method is noticeably higher (74.3 vs. 70.0% AP). Remarkably, its run-time τ graph of graph partitioning stage is 5 orders of magnitude faster compared to DeeperCut. This speed-up is due to two factors. First, TD/BU relies on a faster solver <ref type="bibr" target="#b19">[20]</ref> that tackles the graph-partitioning problem via local search, in contrast to the exact solver used in <ref type="bibr" target="#b13">[14]</ref>. Second, in the case of TD/BU model the graph is sparse and a large portion of the computation is performed by the feed-forward CNN introduced in Sec. 3.2. On WAF <ref type="bibr" target="#b8">[9]</ref>  improves over the best published result (87.7 vs. 82.0% AP by <ref type="bibr" target="#b14">[15]</ref>). We refer to supplemental material for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Multi-frame models</head><p>Comparison of video-based models. Performance of the proposed video-based models is compared in Tab. 4. Videobased models outperform single-frame models in each case. BU-full+temporal slightly outperforms BU-full, where improvements are noticeable for ankle, knee and head. BU-sparse+temporal noticeably improves over BU-sparse (73.1 vs. 71.6% AP). We observe significant improvements on the most difficult parts such as ankles (+3.9% AP) and wrists (+2.6% AP). Interestingly, BU-sparse+temporal outperforms BU-full + temporal: longer-range connections such as, e.g., head to ankle, may introduce additional confusion when information is propagated over time. Finally, TD/BU+temporal improves over TD/BU (+0.7% AP). Similarly to BU-sparse+temporal, improvement is most prominent on ankles (+1.8% AP) and wrists (+0.9% AP). Note that even the single-frame TD/BU outperforms the best temporal BU model. We show examples of articulated tracking on "MPII Video Pose" in Fig. <ref type="figure" target="#fig_7">5</ref>. Temporal reasoning helps in cases when image information is ambiguous due to close proximity of multiple people. For example the video-based approach succeeds in correctly localizing legs of the person in Fig. <ref type="figure" target="#fig_7">5</ref> (d) and (h). Temporal features. We perform an ablative experiment on the "MPII Video Pose" dataset to evaluate the individual contribution of the temporal features introduced in Sec. 3.4. The Euclidean distance alone achieves 72.1 AP, adding DeepMatching features improves the resuls to 72.5 AP, whereas the combination of all features achieves the best result of 73.1 AP (details in supplemental material).</p><p>Tracking evaluation. In Tab. 5 we present results of the evaluation of multi-person articulated body tracking. We treat each body joint of each person as a tracking target and measure tracking performance using a standard multiple object tracking accuracy (MOTA) metric <ref type="bibr" target="#b3">[4]</ref> that incorporates identity switches, false positives and false negatives <ref type="foot" target="#foot_6">8</ref> . We experimentally compare to a baseline model See http://youtube.com/watch?v=eYtn13fzGGo for the supplemental material showcasing our results.</p><p>that first tracks people across frames and then performs perframe pose estimation. To track a person we use a reduced version of our algorithm that operates on the two head joints only. This allows to achieve near perfect person tracking results in most cases. Our tracker still fails when the person head is occluded for multiple frames as it does not incorporate long-range connectivity between target hypothesis. We leave handling of long-term occlusions for the future work. For full-body tracking we use the same inital head tracks and add them to the set of body part proposals, while also adding must-link and must-cut constraints for the temporal edges corresponding to the head parts detections. The rest of the graph remains unchanged so that at inference time the body parts can be freely assigned to different person tracks. For the BU-sparse the full body tracking improves performance by +5.9 and +5.8 MOTA on wrists and ankles, and by +5.0 and +2.4 MOTA on elbows and knees respectively. TD/BU benefits from adding temporal connections between body parts as well, but to a lesser extent than BU-sparse. The most significant improvement is for ankles (+1.4 MOTA). BU-sparse also achieves the best overall score of 58.5 compared to 55.9 by TD/BU. This is surprising since TD/BU outperformed BU-sparse on the pose estimation task (see Tab. 1 and 3). We hypothesize that limited improvement of TD/BU could be due to balanc- ing issues between the temporal and spatial pairwise terms that are estimated independently of each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we introduced an efficient and effective approach to articulated body tracking in monocular video. Our approach defines a model that jointly groups body part proposals within each video frame and across time. Grouping is formulated as a graph partitioning problem that lends itself to efficient inference with recent local search techniques. Our approach improves over state-of-the-art while being substantially faster compared to other related work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices A. Additional Results on the MPII Multi-Person Dataset</head><p>We perform qualitative comparison of the proposed single-frame based TD/BU and BU-full methods on challenging scenes containing highly articulated and strongly overlapping individuals. Results are shown in Fig. <ref type="figure">6</ref> and Figure <ref type="figure">7</ref>. The BU-full works well when persons are sufficiently separated (images 11 and 12). However, it fails on images where people significantly overlap (images 1-3, 5-10) or exhibit high degree of articulation (image 4). This is due to the fact that geometric image-conditioned pairwise may get confused in the presence of multiple overlapping individuals and thus mislead post-CNN bottom-up assembling of body poses. In contrast, TD/BU performs explicit modeling of person identity via top-dop bottom-up reasoning while offloading the larger share of the reasoning about body-part association onto feed-forward convolutional architecture, and thus is able to resolve such challenging cases. Interestingly, TD/BU is able to correctly predict lower limbs of people in the back through partial occlusion (image 3, 5, 7, 10). TD/BU model occasionally incorrectly assembles body parts in kinematically implausible manner (image 12), as it does not explicitly model geometric body part relations. Finally, both models fail in presense of high variations in scale (image 13). We envision that reasoning over multiple scales is likely to improve the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results on the We Are Family dataset</head><p>We compare our proposed TD/BU model to the state-ofthe-art methods on the "We Are Family" (WAF) <ref type="bibr" target="#b8">[9]</ref> dataset and present results in Table <ref type="table" target="#tab_5">6</ref>. We use evaluation protocol from <ref type="bibr" target="#b13">[14]</ref> and report the AP evaluation measure. TD/BU model outperforms the best published results <ref type="bibr" target="#b13">[14]</ref> across all body parts (87.7 vs 82.0% AP) as well improves on articulated parts such as wrists (+6.4% AP) and elbows (+6.4% AP). We attribute that to the ability of top-down model to better learn part associations compared to explicit modeling geometric pairwise relations as in <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation of temporal features.</head><p>We evaluate the importance of combining temporal features introduced in Sec. 3.4 of the paper on our Multi-Person Video dataset.</p><p>To that end, we consider BU-sparse+temporal model and compare results to BU-sparse in Tab. 7. Single-frame BU-sparse achieves 71.6% AP. It can be seen that using geometry based det-distance features slightly improves the results to 72.1% AP, as it enables the propagation of information from neighboring frames. Using deepmatch features slightly improves the performance further as it helps to link the same body part of the same person over time based on the body part appearance. It is especially helpful in the case of fast motion where det-distance may fail. The combination of both geometry and appearance based features further improves the performance to 72.5%, which shows their complementarity. Finally, adding the sift-distance feature improves the results to 73.1%, since it copes better with the sudden changes in background and body part orientations. Overall, using a combination of temporal features in BU-sparse+temporal results in a 1.5% AP improvement over the single-frame BU-sparse. This demonstrates the advantages of the proposed approach to improve pose estimation performance using temporal information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Example articulated tracking results of our approach.</figDesc><graphic coords="1,312.09,278.46,113.39,79.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Visualization of (a) sparse connectivity, (b) attractiverepulsive edges and (c) temporal edges in our model. We show only a subset of attractive/repulsive and temporal edges for clarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) and Fig. 3 (b) that connect two parts of different types, (2) same-type edges shown in Fig. 2 (b) that connect two nodes of the same type in the same image, and (3) temporal edges shown in Fig. 2 (c) that connect nodes in the neighboring frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. (a) Processing stages of the Top-Down model shown for an example with significantly overlapping people. Left: Heatmaps for the chin (=root part) used to condition the CNN on the location of the person in the back (top) and in the front (bottom). Middle: Output heatmaps for all body parts, notice the ambiguity in estimates of the arms of the front person. Right: TD predictions for each person. (b) Example of the Top-Down/Bottom-Up graph. Red dotted line represents the must-cut constraint. Note that body part proposals of different type are connected to person nodes but not between each other. (c) Top-Down/Bottom-Up predictions. Notice that the TD/BU inference correctly assigns the forearm joints of the frontal person.</figDesc><graphic coords="4,51.90,147.89,75.10,67.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>3 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. CNN architecture based on ResNet-101 for computing person conditioned proposals and pairwise terms. SP block for shoulders at conv 4 8 is omitted for clarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Qualitative comparison of results using single frame based model (BU-sparse) vs. articulated tracking (BU-sparse+temporal).See http://youtube.com/watch?v=eYtn13fzGGo for the supplemental material showcasing our results.</figDesc><graphic coords="8,72.02,289.11,113.30,63.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Setting Head Sho Elb Wri Hip Knee Ank AP τCNN τ graph BU-full, label 90.0 84.9 71.1 58.4 69.7 64.7 54.7 70.5 0.18 3.06 BU-full 91.2 86.0 72.9 61.5 70.4 65.4 55.5 71.9 0.18 0.38 BU-sparse 91.1 86.5 70.7 58.1 69.7 64.7 53.8 70.6 0.18 0.22 TD/BU + SP 92.2 86.1 72.8 63.0 74.0 66.2 58.4 73.3 0.94 7 0.08 Effects of various variants of BU model on pose estimation performance (AP) on MPII Multi-Person Val and comparison to the best variant of TD/BUmodel.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>TD/BU + SP 92.2 86.1 72.8 63.0 74.0 66.2 58.4 73.3 Effects of various versions of TD/BU model on pose estimation performance (AP) on MPII Multi-Person Val.</figDesc><table><row><cell>Setting</cell><cell>Head Sho Elb Wri Hip Knee Ank AP</cell></row><row><cell>TD</cell><cell>91.6 84.7 72.9 63.2 72.3 64.7 52.8 71.7</cell></row><row><cell>TD + SP</cell><cell>90.7 85.0 72.0 63.1 73.1 65.0 58.3 72.5</cell></row><row><cell>Setting</cell><cell>Head Sho Elb Wri Hip Knee Ank AP τ graph</cell></row><row><cell>BU-full</cell><cell>91.5 87.8 74.6 62.5 72.2 65.3 56.7 72.9 0.12</cell></row><row><cell>TD/BU+ SP</cell><cell>88.8 87.0 75.9 64.9 74.2 68.8 60.5 74.3 0.005</cell></row><row><cell cols="2">DeeperCut [14] 79.1 72.2 59.7 50.0 56.0 51.0 44.6 59.4 485</cell></row><row><cell cols="2">DeeperCut [15] 89.4 84.5 70.4 59.3 68.9 62.7 54.6 70.0 485</cell></row><row><cell cols="2">Iqbal&amp;Gall [16] 58.4 53.9 44.5 35.0 42.2 36.7 31.1 43.1 10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Pose estimation results (AP) on MPII Multi-Person Test.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>dataset TD/BU substantially Pose estimation results (AP) on "MPII Video Pose".</figDesc><table><row><cell>Setting</cell><cell>Head Sho Elb Wri Hip Knee Ank AP</cell></row><row><cell>BU-full</cell><cell>84.0 83.8 73.0 61.3 74.3 67.5 58.8 71.8</cell></row><row><cell cols="2">+ temporal 84.9 83.7 72.6 61.6 74.3 68.3 59.8 72.2</cell></row><row><cell>BU-sparse</cell><cell>84.5 84.0 71.8 59.5 74.4 68.1 59.2 71.6</cell></row><row><cell cols="2">+ temporal 85.6 84.5 73.4 62.1 73.9 68.9 63.1 73.1</cell></row><row><cell cols="2">TD/BU+ SP 82.2 85.0 75.7 64.6 74.0 69.8 62.9 73.5</cell></row><row><cell cols="2">+ temporal 82.6 85.1 76.3 65.5 74.1 70.7 64.7 74.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Tracking results (MOTA) on the "MPII Video Pose".</figDesc><table /><note><p><p>Setting</p>Head Sho Elb Wri Hip Knee Ank Average Head track + BU-sparse 70.5 71.7 53.0 41.7 57.0 52.4 41.9 55.5 + temporal 70.6 72.7 58.0 47.6 57.6 54.8 47.7 58.5 Head track + TD/BU 64.8 69.4 55.4 43.4 56.4 52.2 44.8 55.2 + temporal 65.0 69.9 56.3 44.2 56.7 53.2 46.1 55.9</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Pose estimation results (AP) on WAF dataset.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Head Sho Elb Wri Total</cell></row><row><cell>TD/BU</cell><cell></cell><cell>97.5 86.2 82.1 85.2 87.7</cell></row><row><cell cols="3">DeeperCut [14] 92.6 81.1 75.7 78.8 82.0</cell></row><row><cell cols="2">DeepCut [23]</cell><cell>76.6 80.8 73.7 73.6 76.2</cell></row><row><cell cols="3">Chen&amp;Yuille [7] 83.3 56.1 46.3 35.5 55.3</cell></row><row><cell>Setting</cell><cell cols="2">Head Sho Elb Wri Hip Knee Ank AP</cell></row><row><cell>BU-sparse</cell><cell cols="2">84.5 84.0 71.8 59.5 74.4 68.1 59.2 71.6</cell></row><row><cell>+ det-distance</cell><cell cols="2">84.8 84.3 72.9 61.8 74.1 67.4 59.1 72.1</cell></row><row><cell>+ deepmatch</cell><cell cols="2">85.5 83.9 73.0 62.0 74.0 68.0 59.5 72.3</cell></row><row><cell>+ det-distance</cell><cell cols="2">85.1 83.6 72.2 61.5 74.4 68.8 62.2 72.5</cell></row><row><cell cols="3">+ sift-distance 85.6 84.5 73.4 62.1 73.9 68.9 63.1 73.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Effects of different temporal features on pose estimation performance (AP) (BU-sparse+temporal model) on our "MPII Video Pose".</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The models and the "MPII Video Pose" dataset are available at pose. mpi-inf.mpg.de/art-track.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://pose.mpi-inf.mpg.de/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>Dataset is available at pose.mpi-inf.mpg.de/art-track.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>The annotations in the original key-frame are kept unchanged.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>http://human-pose.mpi-inf.mpg.de/#evaluation</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>Our current implementation of TD/BU operates on the whole image when computing person-conditioned proposals and computes the proposals sequentially for each person. More efficient implementation would only compute the proposals for a region surrounding the person and run multiple people in a single batch. Clearly in cases when two people are close in the image this would still process the same image region multiple times. However the image regions far from any person would be excluded from processing entirely. On average we expect similar image area to be processed during proposal generation stage in both TD/BU and BU-sparse, and expect the runtimes τCNN to be comparable for both models.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>Note that MOTA metric does not take the confidence scores of detection or track hypotheses into account. To compensate for that in the experiment in Tab. 5 we remove all body part detections with a score ≤ 0.65 for BU-sparse and ≤ 0.7 for TD/BU prior to evaluation.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work has been supported by the Max Planck Center for Visual Computing and Communication. The authors thank Varvara Obolonchykova and Bahar Tarakameh for their help in creating the video dataset.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;14</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">People-tracking-bydetection and people-detection-by-tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno>CVPR&apos;08. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Monocular 3d pose estimation and tracking by detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<title level="m">Evaluating multiple object tracking performance: The CLEAR MOT metrics. Image and Video Processing</title>
		<imprint>
			<date type="published" when="2008-05">2008. May 2008</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV&apos;16</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Personalizing human video pose estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Magee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hogg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CVPR&apos;16. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Parsing occluded people by flexible compositions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Mixing body-part sequences for human pose estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno>CVPR&apos;14. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">We are family: Joint pose estimation of multiple persons</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV&apos;10</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Efficient convnet-based marker-less motion capture in general scenes with a low number of cameras</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elhayek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno>CVPR&apos;15. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">11) and failure (12-13) pose estimation results by single-frame based TD/BU and comparison to BU-full on MPII Multi-Person dataset</title>
		<author>
			<persName><surname>Successfull</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<title level="m">Chained predictions using convolutional neural networks</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deepercut: A deeper, stronger, and faster multiperson pose estimation model</title>
		<author>
			<persName><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>In ECCV&apos;16. 2, 3, 4, 5, 6</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deepercut: A deeper, stronger, and faster multiperson pose estimation model</title>
		<author>
			<persName><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno>arXiv&apos;16. 7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-person pose estimation with local joint-to-person associations</title>
		<author>
			<persName><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVw&apos;16</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Posetrack: Joint multi-person pose estimation and tracking</title>
		<author>
			<persName><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<idno>CVPR&apos;17. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Efficient decomposition of image and mesh graphs by lifted multicuts</title>
		<author>
			<persName><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bonneel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lavoué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<idno>ICCV&apos;15. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Joint graph decomposition &amp; node labeling: Problem, algorithms, applications</title>
		<author>
			<persName><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In CVPR&apos;17. 2, 3, 4, 7</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV&apos;16</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Flowing convnets for human pose estimation in videos</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>ICCV&apos;15. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;16</title>
		<imprint>
			<date type="published" when="2009">1, 2, 4, 6, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Parsing human motion with stretchable models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<idno>CVPR&apos;11. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Subgraph decomposition for multi-target tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004">2015. 1, 2, 3, 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multiperson tracking by multicuts and deep matching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMTT</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Breaking the chain: liberation from the temporal markov assumption for tracking human poses</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tokola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno>ICCV&apos;13. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<idno>NIPS&apos;14. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;16</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deepflow: Large displacement optical flow with deep matching</title>
		<author>
			<persName><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno>ICCV&apos;13. 5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning adaptive value of information for structured prediction</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<idno>NIPS&apos;13. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
