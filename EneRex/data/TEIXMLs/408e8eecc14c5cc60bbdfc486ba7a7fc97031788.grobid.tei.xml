<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discriminative Unsupervised Feature Learning with Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014-06-26">26 Jun 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Jost</roleName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
							<email>dosovits@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Freiburg im Breisgau</orgName>
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<postCode>79110</postCode>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tobias</forename><surname>Springenberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Freiburg im Breisgau</orgName>
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<postCode>79110</postCode>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
							<email>riedmiller@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Freiburg im Breisgau</orgName>
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<postCode>79110</postCode>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
							<email>brox@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Freiburg im Breisgau</orgName>
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<postCode>79110</postCode>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Discriminative Unsupervised Feature Learning with Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2014-06-26">26 Jun 2014</date>
						</imprint>
					</monogr>
					<idno type="MD5">3DFC6499C923E46FDED6100D7A5681A4</idno>
					<idno type="arXiv">arXiv:1406.6909v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-11-21T16:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current methods for training convolutional neural networks depend on large amounts of labeled samples for supervised training. In this paper we present an approach for training a convolutional neural network using only unlabeled data. We train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled 'seed' image patch. We find that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition. The feature representation learned by our algorithm achieves classification results matching or outperforming the current state-of-the-art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional neural networks (CNNs) trained via backpropagation were recently shown to perform well on image classification tasks with millions of training images and thousands of categories <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. While CNNs have been known to yield good results on supervised image classification tasks such as MNIST for a long time <ref type="bibr" target="#b2">[3]</ref>, the feature representation learned by the recent networks achieves stateof-the-art performance not only on the classification task for which the network was trained, but also on various other visual recognition tasks, for example: classification on Caltech-101 <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>, Caltech-256 <ref type="bibr" target="#b1">[2]</ref>, Caltech-UCSD birds dataset <ref type="bibr" target="#b3">[4]</ref>, SUN-397 scene recognition database <ref type="bibr" target="#b3">[4]</ref>; detection on PASCAL VOC dataset <ref type="bibr" target="#b4">[5]</ref>. The capability to generalize to new datasets makes supervised CNN training an attractive approach for generic visual feature learning.</p><p>The downside of this approach is the need for expensive labeling, as the amount of required labeled samples grows quickly the larger the model gets. The large performance increase achieved by methods based on the work of Krizhevsky et al. <ref type="bibr" target="#b0">[1]</ref> was only possible due to massive efforts on manually annotating millions of images. For this reason, unsupervised learning -although currently underperforming -remains an appealing paradigm, since it can make use of raw unlabeled images and videos. Furthermore, on vision tasks outside classification it is not even sure, if training based on object class labels is advantageous. For example, unsupervised feature learning is known to be beneficial for image restoration <ref type="bibr" target="#b5">[6]</ref> and recent results show that it outperforms supervised feature learning also on descriptor matching <ref type="bibr" target="#b6">[7]</ref>.</p><p>In this work we combine the power of a discriminative objective with the major advantage of unsupervised feature learning: cheap data acquisition. We introduce a novel training procedure for convolutional neural networks that does not require any labeled data. It rather relies on an automatically generated surrogate task. The task is created by taking the idea of data augmentationwhich is commonly used in supervised learning -to the extreme. Starting with trivial surrogate classes consisting of one random image patch each, we augment the data by applying a random set of transformations to each patch. Then we train a CNN to classify these surrogate classes.</p><p>The feature representation learned by the network is, by construction, discriminative and invariant to typical transformations. We confirm this both theoretically and empirically, showing that this approach matches or outperforms all previous unsupervised feature learning methods on the standard image classification benchmarks STL-10, CIFAR-10, and Caltech-101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>Our approach is related to a large body of work on unsupervised learning of invariant features and training of convolutional neural networks.</p><p>Convolutional training is commonly used in both supervised and unsupervised methods to utilize the invariance of image statistics to translations (e.g. LeCun et al. <ref type="bibr" target="#b2">[3]</ref>, Kavukcuoglu et al. <ref type="bibr" target="#b7">[8]</ref>, Krizhevsky et al. <ref type="bibr" target="#b0">[1]</ref>). Similar to our approach the current surge of successful methods employing convolutional neural networks for object recognition often rely on data augmentation to generate additional training samples for their classification objective (e.g. Krizhevsky et al. <ref type="bibr" target="#b0">[1]</ref>, Zeiler and Fergus <ref type="bibr" target="#b1">[2]</ref>). While we share the architecture (a convolutional neural network) with these approaches, our method does not rely on any labeled training data.</p><p>In unsupervised learning, several studies on learning invariant representations exist. Denoising autoencoders <ref type="bibr" target="#b8">[9]</ref>, for example, learn features that are robust to noise by trying to reconstruct data from randomly perturbed input samples. Similarly, contractive autoencoders <ref type="bibr" target="#b9">[10]</ref> penalize the sensitivity of the feature representation to small changes in the input. Zou et al. <ref type="bibr" target="#b10">[11]</ref> learn invariant features from video by enforcing a temporal slowness constraint on the feature representation learned by a linear autoencoder. Sohn and Lee <ref type="bibr" target="#b11">[12]</ref> and Hui <ref type="bibr" target="#b12">[13]</ref> learn features invariant to local image transformations. In contrast to our discriminative approach, all these methods rely on directly modeling the input distribution and are typically hard to use for jointly training multiple layers of a CNN.</p><p>The idea of learning features that are invariant to transformations has also been explored for supervised training of neural networks. The research most similar to ours is early work on tangent propagation <ref type="bibr" target="#b13">[14]</ref> (and the related double backpropagation <ref type="bibr" target="#b14">[15]</ref>) which aims to learn invariance to small predefined transformations in a neural network by directly penalizing the derivative of the output with respect to the magnitude of the transformations. In contrast, our algorithm does not regularize the derivative explicitly. Thus it is less sensitive to the magnitude of the applied transformation. This work is also loosely related to the use of unlabeled data for regularizing supervised algorithms, for example self-training <ref type="bibr" target="#b15">[16]</ref> or entropy regularization <ref type="bibr" target="#b16">[17]</ref>. In contrast to these semi-supervised methods, our network training does not require any labeled data.</p><p>Finally, the idea of creating an auxiliary task in order to learn a good data representation was used by Ahmed et al. <ref type="bibr" target="#b17">[18]</ref>, Collobert et al. <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Creating Surrogate Training Data</head><p>The input to the training procedure is a set of unlabeled images, which come from roughly the same distribution as the images to which we later aim to apply the learned features. We randomly sample N ∈ [50, 32000] patches of size 32×32 pixels from different images at varying positions and scales forming the initial training set X = {x 1 , . . . x N }. We are interested in patches containing objects or parts of objects, hence we sample only from regions containing considerable gradients.</p><p>We define a family of transformations {T α | α ∈ A} parameterized by vectors α ∈ A, where A is the set of all possible parameter vectors. Each transformation T α is a composition of elementary transformations from the following list:</p><p>• translation: vertical or horizontal translation by a distance within 0.2 of the patch size;</p><p>• scaling: multiplication of the patch scale by a factor between 0.7 and 1.4;</p><p>• rotation: rotation of the image by an angle up to 20 degrees; • contrast 1: multiply the projection of each patch pixel onto the principal components of the set of all pixels by a factor between 0.5 and 2 (factors are independent for each principal component and the same for all pixels within a patch);  • contrast 2: raise saturation and value (S and V components of the HSV color representation) of all pixels to a power between 0.25 and 4 (same for all pixels within a patch), multiply these values by a factor between 0.7 and 1.4, add to them a value between -0.1 and 0.1; • color: add a value between -0.1 and 0.1 to the hue (H component of the HSV color representation) of all pixels in the patch (the same value is used for all pixels within a patch).</p><p>All numerical parameters of elementary transformations, when concatenated together, form a single parameter vector α. For each initial patch x i ∈ X we sample K ∈ [1, 300] random parameter vectors {α 1 i , . . . , α K i } and apply the corresponding transformations T i = {T α 1 i , . . . , T α K i } to the patch x i . This yields the set of its transformed versions S xi = T i x i = {T x i | T ∈ T i }. Afterwards we subtract the mean of each pixel over the whole resulting dataset. We do not apply any other preprocessing. Exemplary patches sampled from the STL-10 unlabeled dataset are shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Examples of transformed versions of one patch are shown in Fig. <ref type="figure" target="#fig_1">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Algorithm</head><p>Given the sets of transformed image patches, we declare each of these sets to be a class by assigning label i to the class S xi . We next train a CNN to discriminate between these surrogate classes. Formally, we minimize the following loss function:</p><formula xml:id="formula_0">L(X) = xi∈X T ∈Ti l(i, T x i ),<label>(1)</label></formula><p>where l(i, T x i ) is the loss on the transformed sample T x i with (surrogate) true label i. We use a CNN with a softmax output layer and optimize the multinomial negative log likelihood of the network output, hence in our case</p><formula xml:id="formula_1">l(i, T x i ) = M (e i , f (T x i )), M (y, f ) = -y, log f = - k y k log f k ,<label>(2)</label></formula><p>where f (•) denotes the function computing the values of the output layer of the CNN given the input data, and e i is the ith standard basis vector. We note that in the limit of an infinite number of transformations per surrogate class, the objective function (1) takes the form</p><formula xml:id="formula_2">L(X) = xi∈X E α [l(i, T α x i )],<label>(3)</label></formula><p>which we shall analyze in the next section.</p><p>Intuitively, the classification problem described above serves to ensure that different input samples can be distinguished. At the same time, it enforces invariance to the specified transformations. In the following sections we provide a foundation for this intuition. We first present a formal analysis of the objective, separating it into a well defined classification problem and a regularizer that enforces invariance (resembling the analysis in Wager et al. <ref type="bibr" target="#b19">[20]</ref>). We then discuss the derived properties of this classification problem and compare it to common practices for unsupervised feature learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formal Analysis</head><p>We denote by α ∈ A the random vector of transformation parameters, by g(x) the vector of activations of the second-to-last layer of the network when presented the input patch x, by W the matrix of the weights of the last network layer, by h(x) = Wg(x) the last layer activations before applying the softmax, and by f (x) = softmax (h(x)) the output of the network. By plugging in the definition of the softmax activation function</p><formula xml:id="formula_3">softmax (z) = exp(z)/ exp(z) 1<label>(4)</label></formula><p>the objective function (3) with loss (2) takes the form</p><formula xml:id="formula_4">xi∈X E α -e i , h(T α x i ) + log exp(h(T α x i )) 1 .<label>(5)</label></formula><p>With</p><formula xml:id="formula_5">g i = E α [g(T α x i )]</formula><p>being the average feature representation of transformed versions of the image patch x i we can rewrite Eq. ( <ref type="formula" target="#formula_4">5</ref>) as</p><formula xml:id="formula_6">xi∈X -e i , W g i + log exp(W g i ) 1 + xi∈X E α [log exp(h(T α x i )) 1 ] -log exp(W g i ) 1 .<label>(6)</label></formula><p>The first sum is the objective function of a multinomial logistic regression problem with input-target pairs ( g i , e i ). This objective falls back to the transformation-free instance classification problem</p><formula xml:id="formula_7">L(X) = xi∈X l(i, x i ) if g(x i ) = E α [g(T α x)].</formula><p>In general, this equality does not hold and thus the first sum enforces correct classification of the average representation E α [g(T α x i )] for a given input sample. For a truly invariant representation, however, the equality is achieved. Similarly, if we suppose that T α x = x for α = 0, that for small values of α the feature representation g(T α x i ) is approximately linear with respect to α and that the random variable α is centered, i.e. E α [α] = 0,</p><formula xml:id="formula_8">then g i = E α [g(T α x i )] ≈ E α [g(x i ) + ∇ α (g(T α x i ))| α=0 α] = g(x i ).</formula><p>The second sum in Eq. ( <ref type="formula" target="#formula_6">6</ref>) can be seen as a regularizer enforcing all h(T α x i ) to be close to their average value, i.e., the feature representation is sought to be approximately invariant to the transformations T α . To show this we use the convexity of the function log exp(•) 1 and Jensen's inequality, which yields (proof in supplementary material)</p><formula xml:id="formula_9">E α [log exp(h(T α x i )) 1 ] -log exp(W g i ) 1 ≥ 0.<label>(7)</label></formula><p>If the feature representation is perfectly invariant, then h(T α x i ) = W g i and inequality <ref type="bibr" target="#b11">(12)</ref> turns to equality, meaning that the regularizer reaches its global minimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Conceptual Comparison to Previous Unsupervised Learning Methods</head><p>Suppose we want to unsupervisedly learn a feature representation useful for a recognition task, for example classification. The mapping from input images x to a feature representation g(x) should then satisfy two requirements: (1) there must be at least one feature that is similar for images of the same category y (invariance); (2) there must be at least one feature that is sufficiently different for images of different categories (ability to discriminate).</p><p>Most unsupervised feature learning methods aim to learn such a representation by modeling the input distribution p(x). This is based on the assumption that a good model of p(x) contains information about the category distribution p(y|x). That is, if a representation is learned, from which a given sample can be reconstructed perfectly, then the representation is expected to also encode information about the category of the sample (ability to discriminate). Additionally, the learned representation should be invariant to variations in the samples that are irrelevant for the classification task, i.e., it should adhere to the manifold hypothesis (see e.g. Rifai et al. <ref type="bibr" target="#b20">[21]</ref> for a recent discussion). Invariance is classically achieved by regularization of the latent representation, e.g., by enforcing sparsity <ref type="bibr" target="#b7">[8]</ref> or robustness to noise <ref type="bibr" target="#b8">[9]</ref>.</p><p>In contrast, the discriminative objective in Eq. ( <ref type="formula" target="#formula_0">1</ref>) does not directly model the input distribution p(x) but learns a representation that discriminates between input samples. The representation is not required to reconstruct the input, which is unnecessary in a recognition or matching task. This leaves more degrees of freedom to model the desired variability of a sample. As shown in our analysis (see Eq. ( <ref type="formula">12</ref>)), we achieve partial invariance to transformations applied during surrogate data creation by forcing the representation g(T α x i ) of the transformed image patch to be predictive of the surrogate label assigned to the original image patch x i .</p><p>An assumption of our approach is that the transformations T α do not change the identity of the image content. Imagine a task that mainly relies on color information, as differentiating black panthers from pumas. If we use a color transformation, we will force the network to be invariant to this change and cannot expect the extracted features to perform well in this task <ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To compare our discriminative approach to previous unsupervised feature learning methods, we report classification results on the STL-10 <ref type="bibr" target="#b21">[22]</ref>, CIFAR-10 <ref type="bibr" target="#b22">[23]</ref> and Caltech-101 <ref type="bibr" target="#b23">[24]</ref> datasets. Moreover, we assess the influence of the augmentation parameters on the classification performance and study the invariance properties of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>The datasets we test on differ in the number of classes (10 for CIFAR and STL, 101 for Caltech) and the number of samples per class. STL is especially well suited for unsupervised learning as it contains a large set of 100,000 unlabeled samples. In all experiments (except for the dataset transfer experiment in the supplementary material) we extracted surrogate training data from the unlabeled subset of STL-10. When testing on CIFAR-10, we resized the images from 32 × 32 pixels to 64 × 64 pixels so that the scale of depicted objects roughly matches the two other datasets.</p><p>We worked with two network architectures. A "small" network was used to evaluate the influence of different components of the augmentation procedure on classification performance. It consists of two convolutional layers with 64 filters each followed by a fully connected layer with 128 neurons. This last layer is succeeded by a softmax layer, which serves as the network output. A "large" network, consisting of three convolutional layers with 64, 128 and 256 filters respectively followed by a fully connected layer with 512 neurons, was trained to compare our method to the state-ofthe-art. In both models all convolutional filters are connected to a 5 × 5 region of their input.</p><p>2 × 2 max-pooling was performed after the first and second convolutional layers. Dropout <ref type="bibr" target="#b24">[25]</ref> was applied to the fully connected layers. We trained the networks using an implementation based on Caffe <ref type="bibr" target="#b25">[26]</ref>. Details on the training procedure, the hyperparameter settings, and an analysis of the performance depending on the network architecture is provided in the supplementary material.</p><p>We applied the feature representation to images of arbitrary size by convolutionally computing the responses of all the network layers except the top softmax. To each feature map, we applied the pooling method that is commonly used on the respective dataset: 1) 4-quadrant max-pooling, resulting in 4 values per feature map, which is the standard procedure on STL-10 and CIFAR-10 [27, 11, 28, 13];</p><p>2) 3-layer spatial pyramid, i.e. max-pooling over the whole image as well as within 4 quadrants and within the cells of a 4 × 4 grid, resulting in 1 + 4 + 16 = 21 values per feature map, which is the standard on Caltech-101 <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b29">30]</ref>. Finally, we trained a linear support vector machine (SVM) on the pooled features.</p><p>On all datasets we used the standard training and test protocols. On STL-10 the SVM was trained on 10 pre-defined folds of the training data. We report the mean and standard deviation achieved on the fixed test set. For CIFAR-10 we report two results:  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Classification Results</head><p>In Table <ref type="table" target="#tab_0">1</ref> we compare our method to several unsupervised feature learning methods, including the current state-of-the-art on each dataset. We do not compare to supervised methods which use class labels for learning feature representations. Additionally we show the dimensionality of the feature vectors produced by each method before final pooling. The small network was trained on 8000 surrogate classes containing 150 samples each and the large one on 16000 classes with 100 samples each.</p><p>The features extracted from the larger network match or outperform the best prior result on all datasets. This is despite the fact that the dimensionality of the feature vector is smaller than that of most other approaches and that the networks are trained on the STL-10 unlabeled dataset (i.e. they are used in a transfer learning manner when applied to CIFAR-10 and Caltech 101). The increase in performance is especially pronounced when only few labeled samples are available for training the SVM (as is the case for all the datasets except full CIFAR-10). This is in agreement with previous evidence that with increasing feature vector dimensionality and number of labeled samples, training an SVM typically becomes less dependent on the quality of the extracted features <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>Remarkably, on STL-10 we achieve an accuracy of 72.8% ± 0.4%, which is a large improvement over all previously reported results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Detailed Analysis</head><p>Apart from the overall classification results, we studied the effect of three design choices in our approach and validate the invariance properties of the learned features. Additional experiments on the influence of the dataset, from which the 'seed' patches are sampled, can be found in the supplementary material. We used the "small" network in all experiments reported below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Number of Surrogate Classes</head><p>We varied the number N of surrogate classes between 50 and 32000. As a sanity check, we also tried classification with random filters. The results are shown in Fig. <ref type="figure" target="#fig_3">3</ref>.</p><p>Clearly, the classification accuracy increases with the number of surrogate classes until it reaches an optimum for about 8000 surrogate classes after which it did not change or even decreased. This is to be expected: the larger the number of surrogate classes, the more likely it is to draw very similar or even identical samples, which are hard or impossible to discriminate. Few such cases are not detrimental to the classification performance, but as soon as such collisions dominate the set of surrogate labels, the discriminative loss is no longer reasonable. The classification problem becomes too difficult and adapting the network to the surrogate task no longer succeeds. To check the validity of this explanation we also plot in Fig. <ref type="figure" target="#fig_3">3</ref> the classification error on the validation set (taken from the surrogate data) computed after training the network. It rapidly grows as the number of surrogate classes increases. We also observed that the optimal number of surrogate classes increases with  the size of the network (not shown in the figure), but eventually saturates. This demonstrates the main limitation of our current approach of randomly sampling 'seed' patches: it does not scale to arbitrarily large amounts of unlabeled data. However, we do not see this as a fundamental restriction and discuss possible solutions in Section 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Number of Samples per Surrogate Class</head><p>Fig. <ref type="figure" target="#fig_4">4</ref> shows the classification accuracy when the number K of training samples per surrogate class varies between 1 and 300. The performance improves with more samples per surrogate class and saturates at around 100 samples. This indicates that this amount is sufficient to approximate the formal objective from Eq. ( <ref type="formula" target="#formula_2">3</ref>), hence further increasing the number of samples does not significantly change the optimization problem. On the other hand, if the number of samples is too small, there is insufficient data to learn the desired invariance properties. We varied the transformations used for creating the surrogate data to analyze their influence on the final classification performance. The set of 'seed' patches was fixed. The result is shown in Fig. <ref type="figure" target="#fig_5">5</ref>. The value '0' corresponds to applying random compositions of all elementary transformations: scaling, rotation, translation, color variation, and contrast variation. Different columns of the plot show the difference in classification accuracy as we discarded some types of elementary transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Types of Transformations</head><p>Several tendencies can be observed. First, rotation and scaling have only a minor impact on the performance, while translations, color variations and contrast variations are significantly more important. Secondly, the results on STL-10 and CIFAR-10 consistently show that spatial invariance and color-contrast invariance are approximately of equal importance for the classification performance. This indicates that variations in color and contrast, though often neglected, may also improve performance in a supervised learning scenario. Thirdly, on Caltech-101 color and contrast transformations are much more important compared to spatial transformations than on the two other datasets. This is not surprising, since Caltech-101 images are often well aligned, and this dataset bias makes spatial invariance less useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Invariance Properties of the Learned Representation</head><p>In a final experiment, we analyzed to which extent the representation learned by the network is invariant to the transformations applied during training. We randomly sampled 500 images from the STL-10 test set and applied a range of transformations (translation, rotation, contrast, color) to each image. To avoid empty regions beyond the image boundaries when applying spatial transformations, we cropped the central 64 × 64 pixel sub-patch from each 96 × 96 pixel image. We then applied two measures of invariance to these patches.</p><p>First, as an explicit measure of invariance, we calculated the normalized Euclidean distance between normalized feature vectors of the original image patch and the transformed one <ref type="bibr" target="#b10">[11]</ref> (see the supplementary material for details). The downside of this approach is that the distance between extracted features does not take into account how informative and discriminative they are.</p><p>We therefore evaluated a second measure -classification performance depending on the magnitude of the transformation applied to the classified patches -which does not come with this problem. To compute the classification accuracy, we trained an SVM on the central 64 × 64 pixel patches from one fold of the STL-10 training set and measured classification performance on all transformed versions of 500 samples from the test set.</p><p>The results of both experiments are shown in Fig. <ref type="figure" target="#fig_6">6</ref> . Due to space restrictions we show only few representative plots. Overall the experiment empirically confirms that our objective function leads to learning invariant features. Features in the third layer and the final pooled feature representation compare favorably to a HOG baseline (Fig. <ref type="figure" target="#fig_6">6 (a)</ref>). Furthermore, adding stronger transformations in the surrogate training data leads to more invariant classification with respect to these transformations (Fig. <ref type="figure" target="#fig_6">6 (b)-(d)</ref>). However, adding too much contrast variation may deteriorate classification performance (Fig. <ref type="figure" target="#fig_6">6 (d)</ref>). One possible reason is that level of contrast can be a useful feature: for example, strong edges in an image are usually more important than weak ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>We have proposed a way to use a discriminative objective for unsupervised feature learning by training a CNN without class labels. The core idea is to generate a set of surrogate labels via data augmentation. The features learned by the network yield a large improvement in classification accuracy compared to features obtained with previous unsupervised methods. These results strongly indicate that a discriminative objective is superior to objectives previously used for unsupervised feature learning.</p><p>One potential shortcoming of the proposed method is that in its current state it does not scale to arbitrarily large datasets. Two probable reasons for this are that (1) as the number of surrogate classes grows larger, many of them become similar, which contradicts the discriminative objective, and (2) the surrogate task we use is relatively simple and does not allow the network to learn invariance to complex variations, such as 3D viewpoint changes or inter-instance variation. We hypothesize that the presented approach could learn more powerful higher-level features, if the surrogate data were more diverse. This could be achieved by using additional weak supervision, for example, by means of video or a small number of labeled samples. Another possible way of obtaining richer surrogate training data and at the same time avoiding similar surrogate classes would be (unsupervised) merging of similar surrogate classes. We see these as interesting directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Details on Training Procedure</head><p>We describe here in detail which network architectures we tried and explain the network training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Network Architecture</head><p>We tested various network architectures in combination with our training procedure. They are coded as follows: NcF stands for a convolutional layer with N filters of size F × F pixels, Nf stands for a fully connected layer with N neurons. For example, 64c5-64c5-128f denotes a network with two convolutional layers containing 64 filters spanning 5 × 5 pixels each followed by a fully connected layer with 128 neurons. The last specified layer is always succeeded by a softmax layer, which serves as the network output. We applied 2 × 2 max-pooling to the outputs of the first and second convolutional layers.</p><p>As stated in the paper we used a 64c5-64c5-128f architecture in our experiments to evaluate the influence of different components of the augmentation procedure (we refer to this architecture as the 'small' network). A large network, coded as 64c5-128c5-256c5-512f, was then used to achieve better classification performance.</p><p>All considered networks contained rectified linear units in each layer but the softmax layer. Dropout was applied to the fully connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training the Networks</head><p>We adopted the common practice of training the network with stochastic gradient descent with a fixed momentum of 0.9. We started with a learning rate of 0.01 and gradually decreased the learning rate during training. That is, we trained until there was no improvement in validation error, then decreased the learning rate by a factor of 3, and repeated this procedure until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We report here two additional experiments studying influence of different aspects of the algorithm on the quality of the learned features. We also give the details on how we measure invariance of feature representations in Section 4.3.4 of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Influence of the Network Architecture on Classification Performance</head><p>We perform an additional experiment to evaluate the influence of the network architecture on classification performance. The results of this experiment are shown in Table <ref type="table">2</ref>. All networks were trained using a surrogate training set containing either 8000 classes with 150 samples each or 16000 classes with 100 samples each (for larger networks). We vary the number of layers, layer sizes and filter sizes. Classification accuracy generally improves with the network size indicating that our classification problem scales well to relatively large networks without overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Influence of the Dataset</head><p>We applied our feature learning algorithm to images sampled from three datasets -STL-10 unlabeled dataset, CIFAR-10 and Caltech-101 -and evaluated the performance of the learned feature representations on classification tasks on these datasets. We used the "small" network (64c5-64c5-128f) for this experiment.</p><p>We show first layer filters learned from the three datasets in Fig. <ref type="figure" target="#fig_7">7</ref>. Note how filters qualitatively differ depending on the dataset they were trained on.</p><p>Classification results are shown in Table <ref type="table" target="#tab_1">3</ref>. The best classification results for each dataset are obtained when training on the patches extracted from the dataset itself. However, the difference is not drastic, indicating that the learned features generalize well to other datasets. Table <ref type="table">2</ref>: Classification accuracy depending on the network architecture. The name coding is as follows: NcF stands for a convolutional layer with N filters of size F × F pixels, Nf stands for a fully connected layer with N neurons. For example, 64c5-64c5-128f denotes a network with two convolutional layers containing 64 filters spanning 5 × 5 pixels each followed by a fully connected layer with 128 neurons. We also show the number of surrogate classes used for training each network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Details of computing the measure of invariance</head><p>We now explain in detail and motivate computation of the normalized Euclidean distance used as a measure of invariance in the paper.</p><p>First we compute feature vectors of all image patches and their transformed versions. We next normalize each feature vector to unit Euclidean norm and compute Euclidean distances between each original patch and all of its transformed versions. For each transformation and magnitude we average these distances over all patches. Finally, we divide the resulting curves by their maximal values (typically it is the value for the maximum magnitude of the transformation).</p><p>The normalizations are performed to compensate for possibly different scales of different features.</p><p>Normalizing feature vectors to unit length ensures that the values are in the same range for different features. The final normalization of the curves by the maximal value allows to compensate for different variation of different features: as an extreme, a constant feature would be considered perfectly invariant without this normalization, which is certainly not desirable.</p><p>The resulting curves show how quickly the feature representation changes when an image is transformed more and more. A representation for which the curve steeply goes up and then remains constant cannot be considered invariant to the transformation: the feature vector of the transformed patch becomes completely uncorrelated with the original feature vector even for small magnitudes of the transformation. On the other hand, if the curve grows gradually, this indicates that the feature representation changes slowly when the transformation is applied, meaning invariance or, rather, covariance of the representation. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Exemplary patches sampled from the STL unlabeled dataset which are later augmented by various transformations to obtain surrogate data for the CNN training.</figDesc><graphic coords="3,112.95,81.86,188.10,70.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Several random transformations applied to one of the patches extracted from the STL unlabeled dataset. The original ('seed') patch is in the top left corner.</figDesc><graphic coords="3,310.95,81.86,188.10,70.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 1 )</head><label>1</label><figDesc>training the SVM on the whole CIFAR-10 training set ('CIFAR-10'); (2) the average over 10 random selections of 400 training samples per class ('CIFAR-10(400)'). For Caltech-101 we follow the usual protocol of selecting 30 random samples per class for training and not more than 50 samples per class for testing. This is repeated 10 times.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Influence of the number of surrogate training classes. The validation error on the surrogate data is shown in red. Note the different y-axes for the two curves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Classification performance on STL for different numbers of samples per class. Random filters can be seen as '0 samples per class'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Influence of removing groups of transformations during generation of the surrogate training data. Baseline ('0' value) is applying all transformations. Each group of three bars corresponds to removing some of the transformations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Invariance properties of the feature representation learned by our network. (a): Normalized Euclidean distance between feature vectors of the original and the translated image patches vs. the magnitude of the translation, (b)-(d): classification performance on transformed image patches vs. the magnitude of the transformation for various magnitudes of transformations applied for creating surrogate data. (b): rotation, (c): additive color change, (d): multiplicative contrast change.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Filters learned by first layers of 64c5-64c5-128f networks when training on surrogate data from various dataset. Top -from STL-10, middle -CIFAR-10, bottom -Caltech-101.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classification accuracies on several datasets (in percent). † Average per-class accuracy 2 78.0% ± 0.4%. ‡ Average per-class accuracy 84.4% ± 0.6%.</figDesc><table><row><cell>Algorithm</cell><cell cols="5">STL-10 CIFAR-10(400) CIFAR-10 Caltech-101 #features</cell></row><row><cell>Convolutional K-means Network [27]</cell><cell>60.1 ± 1</cell><cell>70.7 ± 0.7</cell><cell>82.0</cell><cell>-</cell><cell>8000</cell></row><row><cell>Multi-way local pooling [29]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">77.3 ± 0.6 1024 × 64</cell></row><row><cell>Slowness on videos [11]</cell><cell>61.0</cell><cell>-</cell><cell>-</cell><cell>74.6</cell><cell>556</cell></row><row><cell cols="2">Hierarchical Matching Pursuit (HMP) [28] 64.5 ± 1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1000</cell></row><row><cell>Multipath HMP [30]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>82.5 ± 0.5</cell><cell>5000</cell></row><row><cell>View-Invariant K-means [13]</cell><cell>63.7</cell><cell>72.6 ± 0.7</cell><cell>81.9</cell><cell>-</cell><cell>6400</cell></row><row><cell>Small net (64c5-64c5-128f)</cell><cell>67.1 ± 0.3</cell><cell>69.7 ± 0.3</cell><cell>75.7</cell><cell>79.8 ± 0.5 †</cell><cell>256</cell></row><row><cell>Large net (64c5-128c5-256c5-512f)</cell><cell cols="2">72.8 ± 0.4 75.3 ± 0.2</cell><cell>82.0</cell><cell>85.5 ± 0.4 ‡</cell><cell>960</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Dependence of classification performance (in %) on the training and testing datasets. Each column corresponds to different test data, each row to different training data (i.e. source of seed patches). We used the "small" network (64c5-64c5-128f) for this experiment.</figDesc><table><row><cell></cell><cell cols="5">#classes STL-10 CIFAR-10(400) CIFAR-10 Caltech-101</cell></row><row><cell>32c5-32c5-64f</cell><cell cols="2">8000 63.8 ± 0.4</cell><cell>66.1 ± 0.4</cell><cell>71.3</cell><cell>78.2 ± 0.6</cell></row><row><cell>64c5-64c5-128f</cell><cell cols="2">8000 67.1 ± 0.3</cell><cell>69.7 ± 0.3</cell><cell>75.7</cell><cell>79.8 ± 0.5</cell></row><row><cell>64c7-64c5-128f</cell><cell cols="2">8000 66.3 ± 0.4</cell><cell>69.5 ± 0.3</cell><cell>75.0</cell><cell>79.4 ± 0.7</cell></row><row><cell>64c5-64c5-64c5-128f</cell><cell cols="2">8000 68.5 ± 0.3</cell><cell>70.9 ± 0.3</cell><cell>77.0</cell><cell>82.2 ± 0.7</cell></row><row><cell cols="3">64c5-64c5-64c5-64c5-128f 8000 64.7 ± 0.5</cell><cell>67.5 ± 0.3</cell><cell>75.2</cell><cell>75.7 ± 0.4</cell></row><row><cell>128c5-64c5-128f</cell><cell cols="2">8000 67.2 ± 0.4</cell><cell>69.9 ± 0.2</cell><cell>76.1</cell><cell>80.1 ± 0.5</cell></row><row><cell>64c5-256c5-128f</cell><cell cols="2">8000 69.2 ± 0.3</cell><cell>71.7 ± 0.3</cell><cell>77.9</cell><cell>81.6 ± 0.5</cell></row><row><cell>64c5-64c5-512f</cell><cell cols="2">8000 69.0 ± 0.4</cell><cell>71.7 ± 0.2</cell><cell>79.3</cell><cell>82.9 ± 0.4</cell></row><row><cell>128c5-256c5-512f</cell><cell cols="2">8000 71.2 ± 0.3</cell><cell>73.9 ± 0.3</cell><cell>81.5</cell><cell>84.3 ± 0.6</cell></row><row><cell>128c5-256c5-512f</cell><cell cols="2">16000 71.9 ± 0.3</cell><cell>74.3 ± 0.3</cell><cell>81.4</cell><cell>84.6 ± 0.6</cell></row><row><cell>64c5-128c5-256c5-512f</cell><cell cols="2">16000 72.8 ± 0.4</cell><cell>75.3 ± 0.3</cell><cell>82.0</cell><cell>85.5 ± 0.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TESTING</cell><cell></cell></row><row><cell>TRAINING</cell><cell>STL-10</cell><cell cols="3">CIFAR-10(400) CALTECH-101</cell></row><row><cell>STL-10</cell><cell>67.5 ± 0.4</cell><cell cols="2">69.5 ± 0.3</cell><cell>76.8 ± 0.4</cell></row><row><cell>CIFAR-10</cell><cell>64.0 ± 0.5</cell><cell cols="2">70.2 ± 0.2</cell><cell>77.9 ± 0.7</cell></row><row><cell cols="2">CALTECH-101 64.0 ± 0.3</cell><cell cols="2">68.4 ± 0.3</cell><cell>79.2 ± 0.6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Such cases could be covered either by careful selection of applied transformations or by combining features from multiple networks trained with different sets of transformations and letting the final classifier choose which features to use.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>There are two ways to compute the test accuracy on Caltech-101: average accuracy over all samples (average overall accuracy) or calculate the accuracy for each class separately and then average these values (average per-class accuracy). These differ, as some classes contain fewer than 50 test samples. We found that most researchers in machine learning use average overall accuracy, hence this is what we report in the tables.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We acknowledge funding by the ERC Starting Grant VideoLearn (279401); the work was also partly supported by the BrainLinks-BrainTools Cluster of Excellence funded by the German Research Foundation (DFG, grant number EXC 1086).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material 1 Formal Analysis</head><p>In this section we present the proofs for the formal analysis from Section 3.1 of our paper.</p><p>we need to prove the convexity of the log-sum-exp function. The Hessian ∇ 2 of this function is given as</p><p>with u = exp(x) and 1 ∈ R n being a vector of ones. To show the convexity we must prove that z T ∇ 2 Z(x)z ≥ 0 for all x, z ∈ R n . From <ref type="bibr" target="#b9">(10)</ref> we get</p><p>Inequality <ref type="bibr" target="#b9">(10)</ref> </p><p>where the constant c does not depend on k. This immediately gives z = c1, which proves the second statement of the proposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 2 Let α ∈ A be a random vector with values in a bounded set</head><p>) holds and only turns to equality if for all α 1 , α 2 ∈ A: (x(α 1 ) -x(α 2 )) ∈ span (1) .</p><p>Proof Inequality <ref type="bibr" target="#b11">(12)</ref> immediately follows from convexity of the function log exp(•) 1 and Jensen's inequality.</p><p>Jensen's inequality only turns to equality if the function it is applied to is affine-linear on the convex hull of the integration region. In particular this implies (x(α 1 ) -x(α 2 )) T ∇ 2 Z(x(α 1 )) (x(α 1 ) -x(α 2 )) = 0 (13) for all α 1 , α 2 ∈ A. The second statement of Proposition 1 thus immediately gives x(α 1 ) -x(α 2 ) = c1, Q.E.D.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.2901v3[cs.CV</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">pre-print</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.1531v1[cs.CV</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">pre-print</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.2524v1[cs.CV</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">pre-print</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simple sparsification improves sparse denoising autoencoders in denoising highly corrupted images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. JMLR Workshop and Conference Proceedings</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Descriptor matching with convolutional neural networks: a comparison to SIFT</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.5769v1[cs.CV</idno>
		<imprint/>
	</monogr>
	<note type="report_type">2014. pre-print</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning convolutional feature hierachies for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A generative process for contractive auto-encoders</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep learning of invariant features via simulated fixations in video</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3212" to="3220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning invariant representations with local transformations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Direct modeling of complex invariances for visual object features</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tangent prop -a formalism for specifying selected invariances in an adaptive network</title>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Victorri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving generalization performance using double backpropagation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="991" to="997" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi supervised logistic regression</title>
		<author>
			<persName><forename type="first">M.-R</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="390" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Entropy regularization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semi-Supervised Learning</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="151" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Training hierarchical feed-forward visual recognition models using transfer learning from pseudo-tasks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV (3)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="69" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dropout training as adaptive regularization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The manifold tangent classifier</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images. Master&apos;s thesis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR WGMBV</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>arxiv:cs/1207.0580v3</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">pre-print</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Caffe: An open source convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="http://caffe.berkeleyvision.org/" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Selecting receptive fields in deep networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2528" to="2536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised Feature Learning for RGB-D Based Object Recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISER</title>
		<imprint>
			<date type="published" when="2012-06">June 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ask the locals: multi-way local pooling for image recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV&apos;11</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multipath sparse coding using hierarchical matching pursuit</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="660" to="667" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
