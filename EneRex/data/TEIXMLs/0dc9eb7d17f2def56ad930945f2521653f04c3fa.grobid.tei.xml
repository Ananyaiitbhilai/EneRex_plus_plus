<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014-12-03">3 Dec 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Joris Pelemans Ciprian Chelba Google</orgName>
								<address>
									<addrLine>Inc., 1600 Amphitheatre Parkway Mountain View</addrLine>
									<postCode>94043</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2014-12-03">3 Dec 2014</date>
						</imprint>
					</monogr>
					<idno type="MD5">9FDA385AA58F63940A1C9D41E3BCE20B</idno>
					<idno type="arXiv">arXiv:1412.1454v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-11-21T16:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel family of language model (LM) estimation techniques named Sparse Non-negative Matrix (SNM) estimation.</p><p>A first set of experiments empirically evaluating it on the One Billion Word Benchmark <ref type="bibr" target="#b11">[Chelba et al., 2013]</ref> shows that SNM n-gram LMs perform almost as well as the well-established Kneser-Ney (KN) models. When using skip-gram features the models are able to match the state-of-the-art recurrent neural network (RNN) LMs; combining the two modeling techniques yields the best known result on the benchmark.</p><p>The computational advantages of SNM over both maximum entropy and RNN LM estimation are probably its main strength, promising an approach that has the same flexibility in combining arbitrary features effectively and yet should scale to very large amounts of data as gracefully as n-gram LMs do.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A statistical language model estimates the prior probability values P (W ) for strings of words W in a vocabulary V whose size is in the tens, hundreds of thousands and sometimes even millions. Typically the string W is broken into sentences, or other segments such as utterances in automatic speech recognition, which are assumed to be conditionally independent; we will assume that W is such a segment, or sentence.</p><p>Estimating full sentence language models is computationally hard if one seeks a properly normalized probability model 1 over strings of words of finite length in V * . A simple and sufficient way to ensure proper normalization of the model is to decompose the sentence probability according to the chain rule and make sure that the end-of-sentence symbol &lt;/s&gt; is predicted with non-zero probability in any context. With W = w 1 , w 2 , . . . , w n we get:</p><formula xml:id="formula_0">P (W ) = n i=1 P (w i |w 1 , w 2 , . . . , w i-1 )<label>(1)</label></formula><p>Since the parameter space of P (w k |w 1 , w 2 , . . . , w k-1 ) is too large, the language model is forced to put the context W k-1 = w 1 , w 2 , . . . , w k-1 into an equivalence class determined by a function Φ(W k-1 ). As a result,</p><formula xml:id="formula_1">P (W ) ∼ = n k=1 P (w k |Φ(W k-1 ))<label>(2)</label></formula><p>The word strings encountered in a practical application are of finite length. The probability distribution P (W ) should assign probability 0.0 to strings of words of infinite length, and thus sum up to 1.0 over the set of strings of finite lengththe support of P (W ). From a modeling point of view in a practical situation, the text gets broken into sentences, and the language model needs to predict the distinguished end-of-sentence symbol &lt;/s&gt;. It can be easily shown that if the language model is smooth, i.e. P (w k |Φ(W k-1 )) &gt; ǫ &gt; 0, ∀w k , W k-1 , then we also have P (&lt;/s&gt;|Φ(W k-1 )) &gt; ǫ &gt; 0, ∀W k-1 which in turn ensures that the model assigns probability 1.0 to the set strings of words of finite length.</p><p>Research in language modeling consists of finding appropriate equivalence classifiers Φ and methods to estimate P (w k |Φ(W k-1 )). The most successful paradigm in language modeling uses the (n -1)-gram equivalence classification, that is, defines</p><formula xml:id="formula_2">Φ(W k-1 ) . = w k-n+1 , w k-n+2 , . . . , w k-1</formula><p>Once the form Φ(W k-1 ) is specified, only the problem of estimating P (w k |Φ(W k-1 )) from training data remains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Perplexity as a Measure of Language Model Quality</head><p>A statistical language model can be evaluated by how well it predicts a string of symbols W t -commonly referred to as test data-generated by the source to be modeled.</p><p>model is side-stepped at a gain in modeling power and simplicity.</p><p>A commonly used quality measure for a given model M is related to the entropy of the underlying source and was introduced under the name of perplexity (PPL) <ref type="bibr" target="#b13">[Jelinek, 1997]</ref>:</p><formula xml:id="formula_3">P P L(M ) = exp(- 1 N N k=1 ln [P M (w k |W k-1 )])<label>(3)</label></formula><p>To give intuitive meaning to perplexity, it represents the number of guesses the model needs to make in order to ascertain the identity of the next word, when running over the test word string from left to right. It can be easily shown that the perplexity of a language model that uses the uniform probability distribution over words in the vocabulary V equals the size of the vocabulary; a good language model should of course have lower perplexity, and thus the vocabulary size is an upper bound on the perplexity of any sensible language model. Very likely, not all words in the test string W t are part of the language model vocabulary. It is common practice to map all words that are out-of-vocabulary to a distinguished unknown word symbol, and report the out-of-vocabulary (OOV) rate on test data-the rate at which one encounters OOV words in the test string W tas yet another language model performance metric besides perplexity. Usually the unknown word is assumed to be part of the language model vocabulary-open vocabulary language models-and its occurrences are counted in the language model perplexity calculation, Eq. (3). A situation less common in practice is that of closed vocabulary language models where all words in the test data will always be part of the vocabulary V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Skip-gram Language Modeling</head><p>Recently, neural network (NN) smoothing <ref type="bibr" target="#b0">[Bengio et al., 2003]</ref>, <ref type="bibr" target="#b2">[Emami, 2006]</ref>, <ref type="bibr">[Schwenk, 2007]</ref>, and in particular recurrent neural networks <ref type="bibr" target="#b7">[Mikolov, 2012]</ref> (RNN) have shown excellent performance in language modeling <ref type="bibr" target="#b11">[Chelba et al., 2013]</ref>. Their excellent performance is attributed to a combination of leveraging long-distance context, and training a vector representation for words.</p><p>Another simple way of leveraging long distance context is to use skip-grams. In our approach, a skip-gram feature extracted from the context W k-1 is characterized by the tuple (r, s, a) where:</p><p>• r denotes number of remote context words • s denotes the number of skipped words</p><p>• a denotes the number of adjacent context words relative to the target word w k being predicted. For example, in the sentence, &lt;S&gt; The quick brown fox jumps over the lazy dog &lt;/S&gt; a (1, 2, 3) skip-gram feature for the target word dog is: [brown skip-2 over the lazy]</p><p>For performance reasons, it is recommended to limit s and to limit either (r+a) or limit both r and s; not setting any limits will result in events containing a set of skip-gram features whose total representation size is quintic in the length of the sentence.</p><p>We configure the skip-gram feature extractor to produce all features f , defined by the equivalence class Φ(W k-1 ), that meet constraints on the minimum and maximum values for:</p><p>• the number of context words used r + a;</p><p>• the number of remote words r;</p><p>• the number of adjacent words a;</p><p>• the skip length s.</p><p>We also allow the option of not including the exact value of s in the feature representation; this may help with smoothing by sharing counts for various skip features. Tied skip-gram features will look like: [curiousity skip-* the cat]</p><p>In order to build a good probability estimate for the target word w k in a context W k-1 we need a way of combining an arbitrary number of skip-gram features f k-1 , which do not fall into a simple hierarchy like regular n-gram features. The following section describes a simple, yet novel approach for combining such predictors in a way that is computationally easy, scales up gracefully to large amounts of data and as it turns out is also very effective from a modeling point of view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sparse Non-negative Matrix Modeling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model definition</head><p>In the Sparse Non-negative Matrix (SNM) paradigm, we represent the training data as a sequence of events E = e 1 , e 2 , ... where each event e ∈ E consists of a sparse non-negative feature vector f and a sparse non-negative target word vector t. A language model is represented by a non-negative matrix M that, when applied to a given feature vector f , produces a dense prediction vector y:</p><formula xml:id="formula_4">y = Mf ≈ t (4)</formula><p>Upon evaluation, we normalize y such that we end up with a conditional probability distribution P M (t|f ) for a model M. For each word w ∈ V that corresponds to index j in t, and its feature vector f that is defined by the equivalence class Φ applied to the history h(w) of that word in a text, the conditional probability P M (w|Φ(h(w))) then becomes:</p><formula xml:id="formula_5">P M (w|Φ(h(w))) = P M (t j |f ) = y j |V| u=1 y u = i∈P os(f ) M ij i∈P os(f ) |V| u=1 M iu<label>(5)</label></formula><p>For convenience, we will write P (t j |f ) instead of P M (t j |f ) in the rest of the paper.</p><p>As required by the denominator in Eq. ( <ref type="formula" target="#formula_5">5</ref>), this computation involves summing over all of the present features for the entire vocabulary. However, if we precompute the row sums |V| u=1 M iu and store them together with the model, the evaluation can be done very efficiently in only |P os(f )| time. Moreover, only the positive entries in M i need to be considered, making the range of the sum sparse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adjustment function and metafeatures</head><p>We let the entries of M be a slightly modified version of the relative frequencies:</p><formula xml:id="formula_6">M ij = e A(i,j) C ij C i * (6)</formula><p>where C is a feature-target count matrix, computed over the entire training corpus and A(i, j) is a real-valued function, dubbed adjustment function. For each featuretarget pair (f i , t j ), the adjustment function extracts k new features α k , called metafeatures, which are hashed as keys to store corresponding weights θ(hash(α k )) in a huge hash table. To limit memory usage, we use a flat hash table and allow collisions, although this has the potentially undesirable effect of tying together the weights of different metafeatures. Computing the adjustment function for any (f i , t j ) then amounts to summing the weights that correspond to its metafeatures:</p><formula xml:id="formula_7">A(i, j) = k θ(hash[α k (i, j)])<label>(7)</label></formula><p>From the given input features, such as regular n-grams and skip n-grams, we construct our metafeatures as conjunctions of any or all of the following elementary metafeatures:</p><p>• feature identity, e.g. [brown skip-2 over the lazy]</p><p>• feature type, e.g. (1, 2, 3) skip-grams</p><formula xml:id="formula_8">• feature count C i *</formula><p>• target identity, e.g. dog</p><p>• feature-target count C ij where we reused the example from Section 2. Note that the seemingly absent feature-target identity is represented by the conjunction of the feature identity and the target identity. Since the metafeatures may involve the feature count and feature-target count, in the rest of the paper we will write α k (i, j, C i * , C ij ). This will become important later when we discuss leave-one-out training. Each elementary metafeature is joined with the others to form more complex metafeatures which in turn are joined with all the other elementary and complex metafeatures, ultimately ending up with all 2 5 -1 possible combinations of metafeatures.</p><p>Before they are joined, count metafeatures are bucketed together according to their (floored) log 2 value. As this effectively puts the lowest count values, of which there are many, into a different bucket, we optionally introduce a second (ceiled) bucket to assure smoother transitions. Both buckets are then weighted according to the log 2 fraction lost by the corresponding rounding operation. Note that if we apply double bucketing to both the feature and feature-target count, the amount of metafeatures per input feature becomes 2 7 -1.</p><p>We will come back to these metafeatures in Section 4.4 where we examine their individual effect on the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss function</head><p>Estimating a model M corresponds to finding optimal weights θ k for all the metafeatures for all events in such a way that the average loss over all events between the target vector t and the prediction vector y is minimized, according to some loss function L. The most natural choice of loss function is one that is based on the multinomial distribution. That is, we consider t to be multinomially distributed with |V| possible outcomes. The loss function L multi then is:</p><formula xml:id="formula_9">L multi (y, t) = -log(P multi (t|f )) = -log( y j |V| u=1 y u ) = log( |V| u=1 y u ) -log(y j )<label>(8)</label></formula><p>Another possibility is the loss function based on the Poisson distribution<ref type="foot" target="#foot_1">2</ref> : we consider each t j in t to be Poisson distributed with parameter y j . The conditional probability of P P oisson (t|f ) then is:</p><formula xml:id="formula_10">P P oisson (t|f ) = j∈t y t j j e -y j t j !<label>(9)</label></formula><p>and the corresponding Poisson loss function is:</p><formula xml:id="formula_11">L P oisson (y, t) = -log(P P oisson (t|f )) = - j∈t [t j log(y j ) -y j -log(t j !)] = j∈t y j - j∈t t j log(y j )<label>(10)</label></formula><p>where we dropped the last term, since t j is binary-valued<ref type="foot" target="#foot_2">3</ref> . Although this choice is not obvious in the context of language modeling, it is well suited to gradient-based optimization and, as we will see, the experimental results are in fact excellent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Estimation</head><p>The adjustment function is learned by applying stochastic gradient descent on the loss function. That is, for each feature-target pair (f i , t j ) in each event we need to update the parameters of the metafeatures by calculating the gradient with respect to the adjustment function.</p><p>For the multinomial loss, this gradient is:</p><formula xml:id="formula_12">∂(L multi (Mf , t)) ∂(A(i, j)) = ∂(log( |V| u=1 (Mf ) u ) -log(Mf ) j ) ∂(M ij ) ∂(M ij ) ∂(A ij ) = [ ∂(log( |V| u=1 (Mf ) u )) ∂(M ij ) - ∂(log(Mf ) j ) ∂(M ij ) ]M ij = [ ∂( |V| u=1 (Mf ) u ) |V| u=1 (Mf ) u ∂(M ij ) - ∂(Mf ) j (Mf ) j ∂(M ij ) ]M ij = ( f i |V| u=1 (Mf ) u - f i y j )M ij = f i M ij ( 1 |V| u=1 y u - 1 y j )<label>(11)</label></formula><p>The problem with this update rule is that we need to sum over the entire vocabulary V in the denominator. For most features f i , this is not a big deal as C iu = 0, but some features occur with many if not all targets e.g. the empty feature for unigrams. Although we might be able to get away with this by re-using these sums and applying them to many/all events in a mini batch, we chose to work with the Poisson loss in our first implementation.</p><p>If we calculate the gradient of the Poisson loss, we get the following:</p><formula xml:id="formula_13">∂(L P oisson (Mf , t)) ∂(A(i, j)) = ∂( |V| u=1 (Mf ) u - |V| u=1 t u log(Mf ) u ) ∂(M ij ) ∂(M ij ) ∂(A(i, j)) = [ ∂( |V| u=1 (Mf ) u ) ∂(M ij ) - ∂( |V| u=1 t u log(Mf ) u ) ∂(M ij ) ]M ij = [f i - t j (Mf ) j ∂(Mf ) j ∂(M ij ) ]M ij = [f i - t j f i (Mf ) j ]M ij = f i M ij (1 - t j y j )<label>(12)</label></formula><p>If we were to apply this gradient to each (positive and negative) training example, it would be computationally too expensive, because even though the second term is zero for all the negative training examples, the first term needs to be computed for all |E||P os(f )||V| training examples.</p><p>However, since the first term does not depend on y j , we are able to distribute the updates for the negative examples over the positive ones by adding in gradients for a fraction of the events where f i = 1, but t j = 0. In particular, instead of adding the term f i M ij , we add</p><formula xml:id="formula_14">f i t j C i * C ij M ij : C i * C ij M ij e=(f i ,t j )∈E f i t j = C i * C ij M ij C ij = M ij e=(f i ,t j )∈E f i (13)</formula><p>which lets us update the gradient only on positive examples. We note that this update is only strictly correct for batch training, and not for online training since M ij changes after each update. Nonetheless, we found this to yield good results as well as seriously reducing the computational cost. The online gradient applied to each training example then becomes:</p><formula xml:id="formula_15">∂(L P oisson (Mf , t)) ∂(A(i, j)) = f i t j M ij ( C i * C ij - 1 y j ) (<label>14</label></formula><formula xml:id="formula_16">)</formula><p>which is non-zero only for positive training examples, hence speeding up computation by a factor of |V|. These aggregated gradients however do not allow us to use additional data to train the adjustment function, since they tie the update computation to the relative frequencies C i * C ij . Instead, we have to resort to leave-one-out training to prevent the model from overfitting the training data. We do this by excluding the event, generating the gradients, from the counts used to compute those gradients. So, for each positive example (f i , t j ) of each event e = (f , t), we compute the gradient, excluding f i from C i * and f i t j from C ij . For the gradients of the negative examples on the other hand we only exclude f i from C i * and we leave C ij untouched, since here we did not observe t j . In order to keep the aggregate computation of the gradients for the negative examples, we distribute them uniformly over all the positive examples with the same feature; each of the C ij positive examples will then compute the gradient of</p><formula xml:id="formula_17">C i * -C ij C ij</formula><p>negative examples. To summarize, when we do leave-one-out training we apply the following gradient update rule on all positive training examples:</p><formula xml:id="formula_18">∂(L P oisson (Mf , t)) ∂(A(i, j)) = f i t j C i * -C ij C ij C ij C i * -1 e k θ(hash[α k (i,j,C i * -1,C ij )]) + f i t j C ij -1 C i * -1 y ′ j -1 y ′ j e k θ(hash[α k (i,j,C i * -1,C ij -1)]) (15)</formula><p>where y ′ j is the product of leaving one out for all the relevant features i.e.</p><formula xml:id="formula_19">y ′ j = (M ′ f ) j and M ′ ij = e k θ(hash[α k (i,j,C i * -1,C ij -1)]) C ij -1 C i * -1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Corpus: One Billion Benchmark</head><p>Our experimental setup used the One Billion Word Benchmark corpus<ref type="foot" target="#foot_3">4</ref> made available by <ref type="bibr" target="#b11">[Chelba et al., 2013]</ref>.</p><p>For completeness, here is a short description of the corpus, containing only monolingual English data:</p><p>• Total number of training tokens is about 0.8 billion  <ref type="table">1</ref>: Perplexity results for Kneser-Ney, Katz and SNM, as well as for the linear interpolation of Kneser-Ney and SNM. Optimal interpolation weights are always around 0.6 -0.7 (KN) and 0.3 -0.4 (SNM).</p><p>• Words outside of the vocabulary were mapped to &lt;UNK&gt; token, also part of the vocabulary</p><p>• Sentence order was randomized</p><p>• The test data consisted of 159658 words (without counting the sentence beginning marker &lt;S&gt; which is never predicted by the language model)</p><p>• The out-of-vocabulary (OoV) rate on the test set was 0.28%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SNM for n-gram LMs</head><p>When trained using solely n-gram features, SNM comes very close to the stateof-the-art Kneser-Ney <ref type="bibr" target="#b6">[Kneser and Ney, 1995]</ref> (KN) models. Table <ref type="table">1</ref> shows that Katz <ref type="bibr" target="#b5">[Katz, 1995]</ref> performs considerably worse than both SNM and KN which only differ by about 5%. When we interpolate these two models linearly, the added gain is only about 1%, suggesting that they are approximately modeling the same things. The difference between KN and SNM becomes smaller when we increase the size of the context, going from 5% for 5-grams to 3% for 8-grams, which indicates that SNM is better suited to a large number of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sparse Non-negative Modeling for Skip n-grams</head><p>When we incorporate skip-gram features, we can either build a 'pure' skip-gram SNM that contains no regular n-gram features, except for unigrams, and interpolate this model with KN, or we can build a single SNM that has both the regular ngram features and the skip-gram features. We compared the two approaches by choosing skip-gram features that can be considered the skip-equivalent of 5-grams i.e. they contain at most 4 words. In particular, we used skip-gram features where the remote span is limited to at most 3 words for skips of length between 1 and 3 We then built a model that uses both these features and regular 5-grams (SNM5-skip), as well as one that only uses the skip-gram features (SNM5skip (no n-grams)).</p><formula xml:id="formula_20">(r = [1..3], s = [1..3], r + a = [1..4])</formula><p>As it turns out and as can be seen from Table <ref type="table">2</ref>, it is better to incorporate all the features into one single SNM model than to interpolate with a KN 5-gram model (KN5). Interpolating the all-in-one SNM5-skip with KN5 yields almost no additional gain.</p><p>The best SNM results so far (SNM10-skip) were achieved using 10-grams, together with untied skip features of at most 5 words with a skip of exactly 1 word (s = 1, r + a = [1..5]) as well as tied skip features of at most 4 words where only 1 word is remote, but up to 10 words can be skipped (r = 1, s = [1..10], r + a = [1..4]).</p><p>This mixture of rich short-distance and shallow long-distance features enables the model to achieve state-of-the-art results, as can be seen in Table <ref type="table" target="#tab_2">3</ref>. When we compare the perplexity of this model with the state-of-the art RNN results in <ref type="bibr" target="#b11">[Chelba et al., 2013]</ref>, the difference is only 3%. Moreover, although our model has more parameters than the RNN (33 vs 20 billion), training takes about a tenth of the time (24 hours vs 240 hours). Interestingly, when we interpolate the two models, we have an additional gain of 20%, and as far as we know, the perplexity of 41.3 is already the best ever reported on this database, beating the previous best by 6% <ref type="bibr" target="#b11">[Chelba et al., 2013]</ref>.</p><p>Finally, when we optimize interpolation weights over all models in <ref type="bibr" target="#b11">[Chelba et al., 2013]</ref>, including SNM5-skip and SNM10-skip, the contribution of the other models as well as the perplexity reduction is negligible, as can be seen in Table <ref type="table" target="#tab_2">3</ref>, which also summarizes the perplexity results for each of the individual models.  <ref type="bibr" target="#b11">[Chelba et al., 2013]</ref>, and SNM5-skip and SNM10-skip, as well as interpolation results and weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Experiments</head><p>To find out how much, if anything at all, each metafeature contributes to the adjustment function, we ran a series of ablation experiments in which we ablated one metafeature at a time. When we experimented on SNM5, we found, unsurprisingly, that the most important metafeature is the feature-target count. At first glance, it does not seem to matter much whether the counts are stored in 1 or 2 buckets, but the second bucket really starts to pay off for models with a large number of singleton features e.g. SNM10-skip<ref type="foot" target="#foot_4">5</ref> . This is not the case for the feature counts, where having a single bucket is always better, although in general the feature counts do not contribute much. In any case, feature counts are definitely the least important for the model. The remaining metafeatures all contribute more or less equally, all of which can be seen in Table <ref type="table" target="#tab_3">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>SNM estimation is closely related to all n-gram LM smoothing techniques that rely on mixing relative frequencies at various orders. Unlike most of those, it combines the predictors at various orders without relying on a hierarchical nesting of the contexts, setting it closer to the family of maximum entropy (ME) <ref type="bibr" target="#b8">[Rosenfeld, 1994]</ref> We are not the first ones to highlight the effectiveness of skip n-grams at capturing dependencies across longer contexts, similar to RNN LMs; previous such results were reported in <ref type="bibr" target="#b14">[Singh and Klakow, 2013]</ref>.</p><p>The speed-ups to ME, and RNN LM training provided by hierarchically predicting words at the output layer <ref type="bibr">[Goodman, 2001b]</ref>, and subsampling <ref type="bibr" target="#b14">[Xu et al., 2011]</ref> still require updates that are linear in the vocabulary size times the number of words in the training data, whereas the SNM updates in Eq. ( <ref type="formula">15</ref>) for the much smaller adjustment function eliminate the dependency on the vocabulary size.</p><p>The computational advantages of SNM over both Maximum Entropy and RNN LM estimation are probably its main strength, promising an approach that has the same flexibility in combining arbitrary features effectively and yet should scale to very large amounts of data as gracefully as n-gram LMs do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>We have presented SNM, a new family of LM estimation techniques. A first empirical evaluation on the One Billion Word Benchmark <ref type="bibr" target="#b11">[Chelba et al., 2013]</ref> shows that SNM n-gram LMs perform almost as well as the well-established KN models.</p><p>When using skip-gram features the models are able to match the stat-of-the-art RNN LMs; combining the two modeling techniques yields the best known result on the benchmark.</p><p>Future work items include model pruning, exploring richer features similar to <ref type="bibr">[Goodman, 2001a]</ref>, as well as richer metafeatures in the adjustment model, mixing SNM models trained on various data sources such that they perform best on a given development set, and estimation techniques that are more flexible in this respect.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Both vectors are binary-valued, indicating the presence or absence of a feature or target words, respectively. Hence, the training data consists of |E||P os(f )| positive and |E||P os(f )|(|V| -1) negative training examples, where P os(f ) denotes the number of positive elements in the vector f .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Number of parameters (in billions) and perplexity results for each of the models in</figDesc><table><row><cell></cell><cell cols="4">Num. Params PPL interpolation weights</cell></row><row><cell>KN5</cell><cell>1.76 B</cell><cell>67.6</cell><cell>0.06</cell><cell>0.00</cell></row><row><cell>HSME</cell><cell>6 B</cell><cell>101.3</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>SBO</cell><cell>1.13 B</cell><cell>87.9</cell><cell>0.20</cell><cell>0.04</cell></row><row><cell>SNM5-skip</cell><cell>62 B</cell><cell>54.2</cell><cell></cell><cell>0.10</cell></row><row><cell>SNM10-skip</cell><cell>33 B</cell><cell>52.9</cell><cell>0.4</cell><cell>0.27</cell></row><row><cell>RNN256</cell><cell>20 B</cell><cell>58.2</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>RNN512</cell><cell>20 B</cell><cell>54.6</cell><cell>0.13</cell><cell>0.07</cell></row><row><cell>RNN1024</cell><cell>20 B</cell><cell>51.3</cell><cell>0.6 0.61</cell><cell>0.53</cell></row><row><cell>SNM10-skip+RNN1024</cell><cell></cell><cell></cell><cell>41.3</cell><cell></cell></row><row><cell>Previous best</cell><cell></cell><cell></cell><cell>43.8</cell><cell></cell></row><row><cell>ALL</cell><cell></cell><cell></cell><cell></cell><cell>41.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>, Metafeature ablation experiments on SNM5 or exponential models.</figDesc><table><row><cell>Ablated feature</cell><cell>PPL</cell></row><row><cell>No ablation</cell><cell>70.8</cell></row><row><cell>Feature</cell><cell>71.9</cell></row><row><cell>Feature type</cell><cell>71.4</cell></row><row><cell>Feature count</cell><cell>70.6</cell></row><row><cell cols="2">Feature count: second bucket 70.3</cell></row><row><cell>Link count</cell><cell>73.2</cell></row><row><cell>Link count: second bucket</cell><cell>70.6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We note that in some practical systems the constraint on using a properly normalized language</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Although we do not use it at this point, the Poisson loss also lends itself nicely for multiple target prediction which might be useful in e.g. subword modeling.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>In fact, even in the general case where t k can take any non-negative value, this term will disappear in the gradient, as it is independent of M.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>http://www.statmt.org/lm-benchmark</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Ideally we want to have the SNM10-skip ablation results as well, but this takes up a lot of time, during which other development is hindered.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Class-Based N-gram Models of Natural Language. Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992">1992. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">A</forename><surname>Emami</surname></persName>
		</author>
		<author>
			<persName><surname>Emami</surname></persName>
		</author>
		<title level="m">A Neural Syntactic Language Model</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
		<respStmt>
			<orgName>Johns Hopkins University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A bit of progress in language modeling, extended version</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><surname>Goodman</surname></persName>
		</author>
		<idno>MSR-TR-2001-72</idno>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Classes for fast maximum entropy training</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Goodman ; B]</surname></persName>
		</author>
		<author>
			<persName><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Estimation of probabilities from sparse data for the language model component of a speech recognizer</title>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">S</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="1987">1995. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improved Backing-Off For M-Gram Language Modeling</title>
		<author>
			<persName><forename type="first">Ney</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical Probabilistic Neural Network Language Model</title>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2005">2012. 2012. 2005. 2005</date>
		</imprint>
		<respStmt>
			<orgName>Brno University of Technology. [Morin and Bengio</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
	<note>Statistical Language Models based on Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive Statistical Language Modeling: A Maximum Entropy Approach</title>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">R</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Continuous space language models</title>
		<imprint>
			<date type="published" when="1994">1994. 1994. 2007. 2007</date>
			<biblScope unit="volume">21</biblScope>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">LSTM Neural Networks for Language Modeling</title>
		<author>
			<persName><surname>Sundermeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A hierarchical Bayesian language model based on PitmanYor processes</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Coling/ACL</title>
		<meeting>Coling/ACL</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling</title>
		<author>
			<persName><surname>Chelba</surname></persName>
		</author>
		<idno>41880</idno>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Google Tech Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Interpolated estimation of Markov source parameters from sparse data</title>
		<author>
			<persName><forename type="first">Mercer</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition in Practice</title>
		<editor>
			<persName><forename type="first">Kanal</forename><surname>Gelsema</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1980">1980. 1980</date>
			<biblScope unit="page" from="381" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><surname>Jelinek</surname></persName>
		</author>
		<title level="m">Frederick Jelinek 1997. Information Extraction From Speech And Text</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Comparing RNNs and log-linear interpolation of improved skip-model on four Babel languages: Cantonese, Pashto, Tagalog, Turkish</title>
		<author>
			<persName><forename type="first">Klakow</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klakow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Gunawardana</surname></persName>
		</editor>
		<editor>
			<persName><surname>Khudanpur</surname></persName>
		</editor>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2011">2013. 2013. 2011. 2011</date>
		</imprint>
	</monogr>
	<note>Proceedings of EMNLP</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
