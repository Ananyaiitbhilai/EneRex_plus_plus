<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Super-resolution via Feature-augmented Random Forest</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hailiang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic and Information Engineering</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kin-Man</forename><surname>Lam</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic and Information Engineering</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Miaohui</forename><surname>Wang</surname></persName>
							<email>mhwang@szu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">College of Information Engineering</orgName>
								<orgName type="institution">Shenzhen University</orgName>
								<address>
									<settlement>Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Image Super-resolution via Feature-augmented Random Forest</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DED2841618870F706B3E979A3AB6EEB0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-11-21T16:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Random forest, gradient magnitude filter, clustering and regression</term>
					<term>image superresolution</term>
					<term>weighted ridge regression</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent random-forest (RF)-based image super-resolution approaches inherit some properties from dictionary-learning-based algorithms, but the effectiveness of the properties in RF is overlooked in the literature. In this paper, we present a novel feature-augmented random forest (FARF) for image super-resolution, where the conventional gradient-based features are augmented with gradient magnitudes and different feature recipes are formulated on different stages in an RF. The advantages of our method are that, firstly, the dictionary-learning-based features are enhanced by adding gradient magnitudes, based on the observation that the non-linear gradient magnitude are with highly discriminative property. Secondly, generalized locality-sensitive hashing (LSH) is used to replace principal component analysis (PCA) for feature dimensionality reduction and original high-dimensional features are employed, instead of the compressed ones, for the leaf-nodes' regressors, since regressors can benefit from higher dimensional features. This original-compressed coupled feature sets scheme unifies the unsupervised LSH evaluation on both image super-resolution and content-based image retrieval (CBIR). Finally, we present a generalized weighted ridge regression (GWRR) model for the leaf-nodes' regressors. Experiment results on several public benchmark datasets show that our FARF method can achieve an average gain of about 0.3 dB, compared to traditional RF-based methods.</p><p>Furthermore, a fine-tuned FARF model can compare to or (in many cases) outperform some recent stateof-the-art deep-learning-based algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In the past few years, random forest (RF) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14]</ref> as a machine-learning tool, working via an ensemble of multiple decision trees, has been employed for efficient classification or regression problems, and applied to a large variety of computer-vision applications, such as object recognition <ref type="bibr" target="#b26">[27]</ref>, face alignment <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21]</ref>, data clustering <ref type="bibr" target="#b16">[17]</ref>, single image super-resolution (SISR) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref>, and so on.</p><p>The RF method, which benefits from its simple implementation of binary trees, has been widely used, and exhibits a number of merits, including (1) it works with an ensemble of multiple decision trees to express the principle that "two heads are better than one", <ref type="bibr" target="#b1">(2)</ref> it is easy to be sped up with parallel processing technology, on both the training and inference stages, (3) it has sub-linear search complexity, because of the use of the binary tree structure, (4) the bagging strategy for feature candidates on splitnodes enable it to handle high-dimensional features and avoid over-fitting on regression, and (5) the clustering-regression scheme employs the "divide and conquer" strategy, which can tackle the classification and regression tasks with more stable performance.</p><p>The RF-based image super-resolution approach can be considered as a clustering/classificationbased method, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. But the clustering and regression problems in RF require with different discriminative features, which have not been systematically studied in existing literature. Feature engineering has been a research hotspot for decades. Several features have been proposed for learning the mapping functions from low-resolution (LR) patches to high-resolution (HR) patches on image restoration problems. Pioneer work in <ref type="bibr" target="#b44">[45]</ref> used a simple high-pass filter as simple as subtracting a low-pass filtered values from the input image raw values. Meanwhile, most algorithms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8]</ref> follow the approach in <ref type="bibr" target="#b27">[28]</ref>, which concatenates the first-and second-order gradients to form the features, as an inexpensive solution to approximating high-pass filtering. Since RF is used as a dictionarylearning-based tool, it inherits many properties from the conventional dictionary-learning-based algorithms on feature extraction. However, the discriminative ability of those gradient-based features for random forest has been overlooked in the literature. We found, from experiments, that augmented features based on two gradient-magnitude filters can achieve more than 0.1dB quality improvement in RF based SISR, with the same parameter setting.</p><p>In most dictionary-learning-based algorithms, principal component analysis (PCA) is used for dimensionality reduction before classification and regression processes. The impact of using PCA has also been paid less attention in the literature. PCA projection may damage the structure of features, which are originally discriminative for clustering at the split-nodes and regression at the leaf-nodes. Motivated from content-based image retrieval (CBIR) <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref>, where the coarse-level search uses compressed features, while the fine-level search uses augmented features. Therefore, in our method, we use the original features rather than the compressed features generated by PCA as worked in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b27">28]</ref>, so that more accurate regression and higher image quality improvement can be achieved. Moreover, the unsupervised locality-sensitive hashing (LSH) model, instead of PCA, is employed for feature dimensionality reduction, which can reduce the damage on the feature structure for the compressed features used on clustering at the split-nodes and thus improve the final image quality.</p><p>For regression problems at the leaf-nodes, we propose a generalized weighted ridge regression (GWRR) as an extension of the work in <ref type="bibr" target="#b0">[1]</ref>. GWRR models are generated based on the data distributions from the leaf-nodes.</p><p>The main contribution of our method is on feature augmentation, so we call our method featureaugmented random forest (FARF). The pipeline of our FARF method, which includes feature extraction, the training stage, and inference stages for SISR, is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. In the FARF-based image SR scheme, higher discriminative features are extracted by using the first-and second-order gradients and their magnitudes. Then, the conventional PCA is replaced by the generalized LSH for dimensionality reduction, and the compressed features are used for clustering in the split-nodes on an RF. Finally, the respective regressors at the leaf-nodes are learned by using the original high dimensional features with the GWRR models.</p><p>Having introduced the main idea of our paper, the remainder of this paper is organized as follows.</p><p>In Section 2, we review the related works on SISR, particularly the RF-based approaches and our insights.</p><p>In Section 3, we introduce the proposed method FARF, including the discriminative feature augmented by the gradient-magnitude filters, the generalized weighted ridge regression (GWRR) model, and the fine-tuned FARF version. In Section 4, we evaluate our FARF scheme on public datasets, and conclusions are given in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">IMAGE SUPER-RESOLUTION VIA RANDOM FOREST</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image Super-Resolution</head><p>Image SR attempts to achieve an impressive HR quality image from one or a set of LR images via artistic skills, which has been an active research topic for decades in the image restoration area. Generalized SR includes interpolation algorithms, such as the classic bicubic interpolation, and other edge-preserving algorithms <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>The traditional super-resolution algorithms are based on pixel operations. Intuitively, operating on a "big pixel", i.e. a patch <ref type="bibr" target="#b51">[52]</ref>, is more effective. Since patch-based algorithms can preserve the local texture structure of an image, various methods based on image patches, such as non-local means <ref type="bibr" target="#b50">[51]</ref>, self-similarity <ref type="bibr" target="#b30">[31]</ref>, manifold learning <ref type="bibr" target="#b28">[29]</ref>, block-matching and 3D filtering (BM3D) <ref type="bibr" target="#b52">[53]</ref>, sparse representation <ref type="bibr" target="#b27">[28]</ref>, etc. have been proposed.</p><p>The neighbor-embedding (NE) methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> are the milestone for patch-based dictionary learning methods. NE learns the mapping between low-and high-resolution patches, with the use of manifold learning. Based on the locally linear embedding (LLE) theory, an LR patch can be represented as a linear combination of its nearest neighbors in a learned dictionary, and its HR counterpart can be approximated as a linear combination of the corresponding HR patches of its LR neighbors, with the same coefficients. Although the NE method is simple and sounds practical, a problem with the method is how to build a feasible patch dictionary. For example, for a patch size of 5×5, with 256 gray levels, it is necessary to have a massive dataset, which has millions of patches, in order to achieve high-quality reconstructed HR patches, if the patches are collected directly from natural scene images. Because of the large dictionary size, it is time consuming to search for a neighbor in such a large dataset.</p><p>Other method to reduce the dictionary size is to learn a relatively smaller dictionary with discrete cosine transform (DCT) or wavelet fixed basis, which the adaptiveness is sacrificed. In 2010, Yang et al. <ref type="bibr" target="#b27">[28]</ref> proposed a sparse prior for dictionary learning. Using sparse coding, image representation can work with a relatively smaller dictionary while keep the adaptiveness by learning the basis from data directly, which opens the era for sparse coding in the image inverse problems.</p><p>With the sparse constraint used in the sparse-coding super-resolution (ScSR) framework, an LR patch and its corresponding HR patch can both be reconstructed through two learned coupled dictionaries, with the same coefficients as following:</p><p>𝑦 ≈ D 𝑙 𝛼, x ≈ D ℎ 𝛼, 𝛼 ∈ R 𝑘 with ‖𝛼‖ 0 ≪ 𝑘.</p><p>where 𝑥 and 𝑦 denote an LR patch and its HR counterpart, respectively, and D 𝑙 and D ℎ are the low and high-resolution coupled dictionaries trained jointly from LR and HR patch samples. The value of 𝜗 in ‖𝛼‖ ϑ is the sparsity factor of the coefficients 𝛼. ‖𝛼‖ 0 , called the 𝑙 0 -norm, is the non-zero count of the coefficients in 𝛼. The LR and HR coupled dictionaries are trained jointly with a sparsity constraint, as following:</p><formula xml:id="formula_1">D ℎ , D 𝑙 = argmin D ℎ ,D 𝑙 ‖𝑥 -D ℎ 𝛼‖ 2 2 + ‖𝑦 -D 𝑙 𝛼‖ 2 2 + 𝜆‖𝛼‖ 0,<label>(2)</label></formula><p>an LR patch 𝑦 of an input LR image Y can be formulated in terms of D 𝑙 as following:</p><formula xml:id="formula_2">min‖𝛼‖ 0 s.t. ‖D 𝑙 𝛼 -𝑦‖ 2 2 ≤ 𝜀,<label>(3)</label></formula><formula xml:id="formula_3">or min‖𝛼‖ 0 s.t. ‖𝐹D 𝑙 𝛼 -𝐹𝑦‖ 2 2 ≤ 𝜀,<label>(4)</label></formula><p>where 𝐹 is a feature-extraction operator on the LR patches, which aims to extract discriminative features from LR patches, rather than using the raw pixel intensity.</p><p>Although the 𝑙 0 -norm of α is an ideal regularization term for the sparse constraint, this strong constraint leads to an NP-hard problem in solving the coefficients α. Yang et al. <ref type="bibr" target="#b27">[28]</ref> relaxed the 𝑙 0 -norm to 𝑙 1 -norm, so as to achieve a feasible solution as following:</p><formula xml:id="formula_4">min‖𝛼‖ 1 s.t. ‖𝐹D 𝑙 𝛼 -𝐹𝑦‖ 2 2 ≤ 𝜀,<label>(5)</label></formula><p>and an equivalent formulation can be achieved by using the Lagrange multiplier,</p><formula xml:id="formula_5">min 𝛼 ‖𝐹D 𝑙 𝛼 -𝐹y‖ 2 2 + 𝜆‖𝛼‖ 1 ,<label>(6)</label></formula><p>where the parameter 𝜆 balances the sparsity of the solution and the fidelity of the approximation to 𝑦.</p><p>As the sparse constraint in <ref type="bibr" target="#b27">[28]</ref> is still a bottleneck on training dictionaries considering the computation, an intuitive way to solve it is to relax the constraint again to 𝑙 2 -norm. Meanwhile, the effectiveness of sparsity is challenged <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref> by researchers as to whether sparsity or collaborative representation really helps in image classification and restoration. As a natural solution to that, Timofte et al. proposed an anchored neighborhood regression (ANR) <ref type="bibr" target="#b1">[2]</ref> framework, where there is no sparse constraint in the model. ANR replaces the sparse-decomposition optimization (𝑙 1 -norm) with a ridge regression (i.e. 𝑙 2norm), where the coefficients can be computed offline and each coefficient can be stored as an atom (anchor) in the dictionary. This offline learning can greatly speed-up the prediction stage, and this approach has subsequently led to several variant algorithms.</p><p>Timofte et al. later extended the ANR approach to the A+ <ref type="bibr" target="#b4">[5]</ref>. In A+ <ref type="bibr" target="#b4">[5]</ref>, the coupled dictionaries are trained from a large pool of training samples (in the order of millions) rather than only from the anchoring atoms, which greatly improves the image quality. After that, more extensions based on ANR and A+ have emerged <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>However, in the above-mentioned dictionary-learning methods, the complexity of finding those similar patches by comparing an input patch with all the dictionary items has been overlooked. Recently, algorithms using random forest (RF) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref> have achieved state-of-the-art performances, in terms of both accuracy and efficiency for classification and regression tasks. This is mainly due to the use of ensemble learning and sublinear search based on binary trees. Schulter et al. <ref type="bibr" target="#b7">[8]</ref> adopted random forest and the clustering-regression scheme to learn regressors from the patches in leaf-nodes for SISR. With the same number of regressors, the RF-based algorithm can outperform or achieve comparable performance with A+ and its variants, in terms of accuracy but with less computational complexity.</p><p>In recent years, deep learning has achieved promising performances on image super-resolution <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>. In <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>, milestone works on image super-resolution based on deep learning were presented, where a convolutional neural network (SRCNN) was proposed to learn an end-to-end mapping between LR and HR images for image super-resolution. Later a scheme with very deep networks for SISR was proposed in <ref type="bibr" target="#b38">[39]</ref>, where the convergence rate of the deep network is improved by using residual learning and extremely high learning rates. In addition, Ledig et al. <ref type="bibr" target="#b39">[40]</ref> introduced a generative adversarial network (GAN) based image super-resolution model (SRGAN), where the image perceptual loss function is reformulated as the combination of content loss and adversarial loss. Although deeplearning-based approaches have achieved promising progress on SISR, the heavy computational requirement is still a large burden even though the implementation is accelerated by GPU. This may limit them from those applications without powerful GPU, such as smart mobile terminals. In the inference stage, each decision tree returns a class probability 𝑝 𝑡 (𝑦|𝒗) for a given test sample 𝒗 ∈ 𝑅 𝑚 , and the final class label 𝑦 * is then obtained via averaging, as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image Super-Resolution via Random Forest</head><formula xml:id="formula_6">𝑦 * = arg max 𝑦 1 𝑇 ∑ 𝑝 𝑡 (𝑦|𝒗), 𝑇 𝑡=1<label>(7)</label></formula><p>A splitting function 𝑠(𝒗; Θ) is typically parameterized by two values: (i) a feature dimensional index: Θ 𝑖 {1, . . . , 𝑚}, and (ii) a threshold Θ 𝑡 ℝ. The splitting function is defined as follows:</p><formula xml:id="formula_7">𝑠(𝒗; Θ) = { 0, if 𝒗(Θ 𝑖 ) &lt; Θ 𝑡 , 1, otherwise,<label>(8)</label></formula><p>where the outcome defines to which child node 𝒗 is routed, and 0 and 1 are the two labels belonging to the left and right child node, respectively. Each node chooses the best splitting function Θ * out of a randomly sampled set {Θ 𝑖 }, and the threshold Θ 𝑡 is determined by optimizing the following function:</p><formula xml:id="formula_8">𝐼 = |𝐿| |𝐿|+|𝑅| 𝐻(𝐿) + |𝑅| |𝐿|+|𝑅| 𝐻(𝑅),<label>(9)</label></formula><p>where 𝐿 and 𝑅 are the sets of samples that are routed to the left and right child nodes, respectively, and |𝑆| represents the number of samples in the set 𝑆. During the training of an RF, the decision trees are provided with a random subset of the training data (i.e. bagging), and are trained independently. Training a single decision tree involves recursively splitting each node, such that the training data in the newly created child node is clustered conforming to class labels. Each tree is grown until a stopping criterion is reached (e.g. the number of samples in a node is less than a threshold or the tree depth reaches a maximum value) and the class probability distributions are estimated in the leaf nodes. After fulfilling one of the stopping criteria, the density model 𝑝(𝑦) in each leaf node is estimated by using all the samples falling into the leaf node, which will be used as a prediction of class probabilities in the inference stage. A simple way to estimate the probability distribution function 𝑝(𝑦) is by averaging all the samples in the leaf node, and there are many variants, such as fitting a Gaussian distribution, kernel density estimation, etc.</p><p>In <ref type="bibr" target="#b8">(9)</ref>, 𝐻(𝑆) is the local score for a set of samples in S (S is either L or R), which is usually calculated by entropy, as shown in Eqn. <ref type="bibr" target="#b9">(10)</ref>, and it can be replaced by variance <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21]</ref> or by the Gini index <ref type="bibr" target="#b13">[14]</ref>.</p><formula xml:id="formula_9">𝐻(𝑆) = -∑ [𝑝(𝑘|𝑆) * log(𝑝(𝑘|𝑆))] 𝐾 𝑘=1 , (<label>10</label></formula><formula xml:id="formula_10">)</formula><p>where 𝐾 is the number of classes, and 𝑝(𝑘|𝑆) is the probability for class 𝑘, which is estimated from the set 𝑆. For the regression problem, the differential entropy is used, and is defined as,</p><formula xml:id="formula_11">𝐻(𝑞) = ∫ 𝑞(𝑦|𝑥) * log (𝑞(𝑦|𝑥) 𝑦 )𝑑 𝑦 ,<label>(11)</label></formula><p>where 𝑞(𝑦|𝑥) denotes the conditional probability of a target variable given an input sample. Assuming that 𝑞(. , . ) is of Gaussian distribution, and has only a set of finite samples, the differential entropy can be written as,</p><formula xml:id="formula_12">𝐻 𝐺𝑎𝑢𝑠𝑠 (𝑆) = 𝐾 2 (1 -log(2π)) + 1 2 log(det(Σ 𝑆 )),<label>(12)</label></formula><p>where det (Σ 𝑆 ) is the determinant of the estimated covariance matrix of the target variables in 𝑆.</p><p>RF-based approaches hold some properties, which make them powerful classifiers as SVM (support vector machine) <ref type="bibr" target="#b9">[10]</ref> and AdaBoost (short for "Adaptive Boosting") <ref type="bibr" target="#b12">[13]</ref>. Both SVM and AdaBoost work as to approximate the Bayes decision rule -known to be the optimal classifiers -via minimizing a margin-based global loss function.</p><p>RF-based image super-resolution (SR), following a recent emerging stream <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b30">31]</ref> on single-image SR, formulates the SR problem as a clustering-regression problem. These emerging approaches attempt to reconstruct an HR image from patches with the aid of an external database. These methods first decompose an image into patches, then classify the patches into different clusters, and later regressors are trained for all the clusters respectively, which generate mappings from an input LR patch's features to its corresponding HR patch. In the inference stage, an LR image follows the same procedures, such that it is divided into patches and features are extracted from each patch. Then, the patches are classified into different clusters using K-NN <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref> or RF <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref>, and their super-resolved HR patches are computed through regression in the leaf nodes (see Fig. <ref type="figure" target="#fig_0">1</ref>). This kind of clustering-regression-based random forest <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref> methods has achieved state-of-the-art performance in SISR, both in terms of accuracy and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">FEATURE-AUGMENTED RANDOM FOREST</head><p>Classification and regression can be regarded as probability problems from the statistical theory.</p><p>Historical frequentist probability is the probability obtained from the relative frequency in a large number of trials. In contrast, the Bayesian probability is an interpretation of the concept of probability, in which probability is interpreted as an expectation taking the knowledge and personal belief into account. From the Bayesian theory, the posterior probability of a random event is a conditional probability, which can be calculated if the relevant evidence or context is considered. Therefore, the posterior probability is the probability 𝑝(𝜃|𝑥) of the parameters 𝜃 given the evidence 𝑥. We denote the probability distribution function of the prior for parameters 𝜃 as 𝑝(𝜃), and the likelihood as 𝑝(𝑥|𝜃), which is the probability of 𝑥 given 𝜃. Then, based on the Bayesian rule, the posterior probability can be defined as follows:</p><formula xml:id="formula_13">𝑝(𝜃|𝑥) = 𝑝(𝑥|𝜃)𝑝(𝜃) 𝑝(𝑥) . (<label>13</label></formula><formula xml:id="formula_14">)</formula><p>The posterior probability can be denoted in a memorable form as: 𝑃𝑜𝑠𝑡𝑒𝑟𝑖𝑜𝑟 𝑝𝑟𝑜𝑏𝑎𝑏𝑖𝑙𝑖𝑡𝑦 ∝ 𝐿𝑖𝑘𝑒𝑙𝑖ℎ𝑜𝑜𝑑 × 𝑃𝑟𝑖𝑜𝑟 𝑝𝑟𝑜𝑏𝑎𝑏𝑖𝑙𝑖𝑡𝑦.</p><p>Based on the Bayesian framework, the likelihood term and the prior term are both required to be determined in order to solve the inverse problems, and the extracted features are normally worked as prior or likelihood, particularly on some image restoration problems. From this point of view, most research works, from classic feature extractors to deep-learning neural networks, are essentially done under the Bayesian inference framework.</p><p>Since SISR is a well-known ill-posed problem, researchers have put their efforts into the priors of the problem with skills from mathematics, computer vision and machine learning. One of the obvious and most studied priors is the edge prior, which can be found in many pioneering works: new edgedirected interpolation (NEDI) <ref type="bibr" target="#b40">[41]</ref>, soft-decision adaptive interpolation (SAI) <ref type="bibr" target="#b41">[42]</ref>, directional filtering and data-fusion (DFDF) <ref type="bibr" target="#b42">[43]</ref>, modified edge-directed interpolation (MEDI) <ref type="bibr" target="#b43">[44]</ref>, and so on. The edge prior is effective on image processing, and the first and second-order gradients are studied and employed by Yang et al. <ref type="bibr" target="#b27">[28]</ref> in a pioneering dictionary-learning-based algorithm. However, the effect of edgebased features has not been investigated in depth. Normally it is unstable to directly use pixel intensities as features, which are susceptible to the environmental lighting variations and camera noise. Instead, the differences between the neighboring pixels' intensity values, which are computationally efficient, and are immune to lighting changes and noise, are examined. This type of features can be implemented efficiently through convolutional filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Augmented Features via Gradient Magnitude Filters</head><p>Typically, the feature filter 𝐹 can be chosen as a high-pass filter, while in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b27">28]</ref>, the first and second-order gradient operators are used to generate an up-sampled version from a low-resolution image, then four patches are extracted from the gradient maps at each location, and finally the patches are concatenated to form feature vectors. The four 1-D filters used to extract the derivatives are described in Eqn. <ref type="bibr" target="#b13">(14)</ref>,</p><formula xml:id="formula_15">𝐹 1 = [-1, 0, 1], 𝐹 2 = 𝐹 1 𝑇 𝐹 3 = [1, 0, -2, 0, 1], 𝐹 4 = 𝐹 3 𝑇 }. (<label>14</label></formula><formula xml:id="formula_16">)</formula><p>These features can work well on dictionary-learning-based methods, because when searching a matched patch in a dictionary, the distance is calculated based on the whole feature vectors with the Euclidean distance. However, when training a split node in a decision tree of an RF, only one or a few of the feature dimensions are chosen as candidate features for comparison. Therefore, more discriminative features are required for RF, when compared with dictionary-learning-based methods. </p><p>where 𝜕𝐼/𝜕𝑥 and 𝜕𝐼/𝜕𝑦 are the gradients in the x-axis and y-axis directions, respectively, at a given pixel. Meanwhile, the gradient magnitude image can provide the edge strength, as described in Eqn. <ref type="bibr" target="#b15">(16)</ref>.</p><p>Fig. <ref type="figure" target="#fig_3">4</ref> shows a toy example of a man-made "circle" image, to demonstrate its discriminative property.</p><formula xml:id="formula_18">‖∇𝐼‖ = √( 𝜕𝐼 𝜕𝑥 ) 2 + ( 𝜕𝐼 𝜕𝑦 ) 2 . (<label>16</label></formula><formula xml:id="formula_19">)</formula><p>With a natural image shown in Fig. <ref type="figure" target="#fig_4">5</ref>, it can be observed that the gradient magnitude image has more detailed textures than the gradient images (𝜕𝐼/𝜕𝑥 and 𝜕𝐼/𝜕𝑦), as well as the sum of the horizontal gradient and vertical gradient image, i.e. 𝜕𝐼/𝜕𝑥 + 𝜕𝐼/𝜕𝑦 , perceptually. An explanation for this phenomenon is that non-linear features are usually more discriminative. Thus, in our work, all the first and second-order gradients, and gradient magnitude are employed, and are concatenated to form more discriminative, augmented features.</p><p>On the other hand, the image orientation (gradient angle) is defined by the following formulation,</p><formula xml:id="formula_20">∠∇𝐼 = arctan (𝜕𝑦/𝜕𝑥),<label>(17)</label></formula><p>where atan( ) is the gradient orientation, with a value between -90 and 90. As shown in Eqn. ( <ref type="formula" target="#formula_20">17</ref>), when the value of 𝜕𝑥 is equal to 0 or close to 0, the value of ∠∇ becomes infinitely large and unstable, i.e., different 𝜕𝑦 will result in approximately the same ∠∇ value. Based on this analysis, we only use the two gradient magnitude filters derived from the four gradient filters <ref type="bibr" target="#b27">[28]</ref> to generate the augmented features. Experiments validate that the use of the augmented features can improve the conventional RF algorithm <ref type="bibr" target="#b7">[8]</ref> to achieve a performance gain of more than 0.1dB, which is a remarkable improvement, with the same setting and parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fine-grained Features for Regression</head><p>The inference stage of the RF-based image super-resolution process is similar to the content-based image retrieval (CBIR) framework, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The general approximated nearest neighbor (ANN)</p><p>search framework <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref> is an efficient strategy for large-scale image retrieval, which mainly consists of 4 parts: (1) extracting compact features (e.g., locality-sensitive Hashing (LSH) <ref type="bibr" target="#b47">[48]</ref> feature) for a query image;</p><p>(2) coarse-level search using Hamming distance to measure the similarity between binary compact Hash features, then narrow the search scope into a smaller candidate group; (3) fine-level search by using Euclidean distance to measure the similarity between their corresponding feature vectors; and</p><p>(4) finding the object in the smaller candidate group that is the nearest one to the query image.</p><p>In the inference stage of conventional RF-based SISR, PCA projection is worked as a Hash-like function to compress the feature dimension for decreasing the search range, which can speed up the searching as the coarse-level search in a CBIR framework, but the impact of using PCA on feature dimensionality reduction has been overlooked in previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b27">28]</ref>. Inspired by the finelevel search using augmented features in CBIR frameworks, the high dimensional features in the leaf nodes in an RF can further improve the prediction accuracy in the regression step, which has not been studied previously. Consequently, we use the original features, rather than PCA or the LSH compressed features, to perform ridge regression in the leaf nodes. Experimental results show that the new RF scheme can greatly improve the quality of super-resolved images, by using this augmented feature. Another explanation for this is that the regression problems can benefit more from higher dimensional features than classification problems.</p><p>Based on the observation that the original edge-like features are used for the final regressors in the leaf  In the new scheme, we unify the research of LSH-based SISR and image retrieval (CBIR) <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref>.</p><p>In brief, the new achievement on unsupervised LSH can be evaluated not only in CBIR systems, but also in the clustering-regression RF-based SISR methods. Moreover, as evidence from <ref type="bibr" target="#b55">[56]</ref>, proper unsupervised LSH models, e.g., iterative quantization (ITQ) <ref type="bibr" target="#b56">[57]</ref> used for feature dimension reduction instead of PCA, can reduce the damage on the image structure. This can further improve the superresolved image quality. Different from <ref type="bibr" target="#b55">[56]</ref> using an ITQ-like algorithm to rotate the original features into a new feature space, with the use of the proposed original-compressed coupled feature sets, any unsupervised LSH generated features can directly be employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generalized Weighted Ridge Regression Model</head><p>In this sub-section, we further analyze the ridge regression employed in the RF leaf nodes. The anchored neighborhood regression (ANR) <ref type="bibr" target="#b1">[2]</ref> model relaxes the 𝑙 1 -norm in Eqn. <ref type="bibr" target="#b5">(6)</ref> to the 𝑙 2 -norm constraint, with least-squares minimization as the following equation,</p><formula xml:id="formula_21">min 𝛼 ‖𝐹D 𝑙 𝛼 -𝐹𝑦‖ 2 2 + 𝜆‖𝛼‖ 2 , (<label>18</label></formula><formula xml:id="formula_22">)</formula><p>Based on the ridge regression <ref type="bibr" target="#b15">[16]</ref> theory, this 𝑙 2 -norm constrained least square regression regularized problem has a closed-form solution, according to the Tikhonov regularization theory, as follows:</p><formula xml:id="formula_23">𝛼 = (𝐷 𝑙 𝑇 𝐷 𝑙 + 𝜆𝐼) -1 𝐷 𝑙 𝑇 𝐹𝑦. (<label>19</label></formula><formula xml:id="formula_24">)</formula><p>With the assumption in <ref type="bibr" target="#b27">[28]</ref>, where HR patches and their counterpart LR patches share the same reconstructed coefficient α, i.e. 𝑥 = D ℎ 𝛼, from Eqn. <ref type="bibr" target="#b18">(19)</ref> we have</p><formula xml:id="formula_25">𝑥 = D ℎ (𝐷 𝑙 𝑇 𝐷 𝑙 + 𝜆𝐼) -1 𝐷 𝑙 𝑇 𝐹𝑦. (<label>20</label></formula><formula xml:id="formula_26">)</formula><p>If we define 𝑃 𝐺 as a pre-calculated projection matrix, as follows,</p><formula xml:id="formula_27">𝑃 𝐺 = D ℎ (𝐷 𝑙 𝑇 𝐷 𝑙 + 𝜆𝐼) -1 𝐷 𝑙 𝑇 , (<label>21</label></formula><formula xml:id="formula_28">)</formula><p>then the HR patches can be reconstructed with 𝑥 = 𝑃 𝐺 𝐹𝑦.</p><p>Having studied the model in Eqn. <ref type="bibr" target="#b17">(18)</ref>, the authors in <ref type="bibr" target="#b0">[1]</ref> argued that different weights should be given to different atoms when reconstructing an HR patch so as to emphasize the similarity to the anchor atom.</p><p>Based on this idea, <ref type="bibr" target="#b0">[1]</ref> proposed a weighted collaborative representation (WCR) model by generalizing the normal collaborative representation (CR) model in the ANR,</p><formula xml:id="formula_29">min 𝛼 ‖𝐹D 𝑙 𝛼 -𝐹𝑦‖ 2 2 + ‖𝜆 𝑊𝐶𝑅 𝛼‖ 2 , (<label>22</label></formula><formula xml:id="formula_30">)</formula><p>where 𝜆 𝑊𝐶𝑅 is a diagonal weight matrix, in which the non-zero entries are proportional to the similarities between the atoms and the anchor atom.</p><p>Same as the ANR model, a new closed-form solution can be computed offline through the following equation,</p><formula xml:id="formula_31">𝛼 * = (𝐷 𝑙 𝑇 𝐷 𝑙 + 𝜆 𝑊𝐶𝑅 ) -1 𝐷 𝑙 𝑇 𝐹𝑦,<label>(23)</label></formula><p>and the new projection matrix can be derived as</p><formula xml:id="formula_32">𝑃 𝐺 * = D ℎ (𝐷 𝑙 𝑇 𝐷 𝑙 + 𝜆 𝑊𝐶𝑅 ) -1 𝐷 𝑙 𝑇 . (<label>24</label></formula><formula xml:id="formula_33">)</formula><p>The WCR model further improves the ANR/A+ model in terms of image quality, while keeping the same level of computation. In <ref type="bibr" target="#b8">[9]</ref>, the local geometry prior of the data sub-space is used. However, all the weighted ridge regression models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9]</ref> are constructed based on an existing dictionary, e.g., Zeyde et al. <ref type="bibr" target="#b3">[4]</ref> used K-SVD to train a sparse-coding-based dictionary with 1024 items. This limits the models to collect samples in a smaller sub-space when constructing linear regressors based on existing anchor points.</p><p>Fig. <ref type="figure">7</ref>: Gaussian mixture model (GMM) is used to generate the weights for weighted ridge regression, and the weight of each entry lies on its belonging cluster's weight and its weight in the belonging cluster.</p><p>When training the regressors in an RF, there is no existing anchor point in the clustered groups of the leaf nodes, similar to the previous models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9]</ref>. A solution to mentioned problem is inspired from the work on image classification using locality-constrained linear coding (LLC) <ref type="bibr" target="#b48">[49]</ref>, where Gaussian mixture model (GMM) is used to describe the locality-constrained affine subspace coding (LASC) <ref type="bibr" target="#b49">[50]</ref>.</p><p>We employ GMM to construct the data distribution in the sub-space for each leaf node, which derives the weights of all the entries in the ridge regression models. Through the derived weights, we can obtain a generalized weighted ridge regression (GWRR) model for ridge regression. The new projection matrix is given as follows:</p><formula xml:id="formula_34">𝑃 𝐺 * = D ℎ (𝐷 𝑙 𝑇 𝐷 𝑙 + 𝜆 𝐺𝑊𝑅𝑅 ) -1 𝐷 𝑙 𝑇 ,<label>(25)</label></formula><p>where 𝜆 𝐺𝑊𝑅𝑅 is a diagonal weight matrix, and the weight of each diagonal entry is related to its belonging cluster's weight and its local weight in its belongingwhi cluster, as illustrated in the right part of Fig. <ref type="figure">7</ref>. Obviously, a query entry falling into a bigger cluster and closer to the center of the belonging cluster achieves a larger weight. In a rough form, the diagonal weight matrix 𝜆 𝐺𝑊𝑅𝑅 is given as follows:</p><formula xml:id="formula_35">𝜆 𝐺𝑊𝑅𝑅 = diag{[𝑤 1 ; 𝑤 2 ; … 𝑤 𝑖 ; … ; 𝑤 𝑚 ]}, 𝑤 𝑖 ∝ 𝐶 𝑖 𝑘 ×𝑑 𝑖 𝑘 , 𝑘 = (1, … , 𝐾) ,<label>(26)</label></formula><p>where 𝑤 𝑖 is the weight of the 𝑖 th entry, 𝑚 is number of samples in the leaf nodes, 𝐶 𝑖 𝑘 is the 𝑘 th cluster's weight for the 𝑖 th entry, 𝑑 𝑖 𝑘 is the 𝑖 th entry's local weight in the 𝑘 th cluster, which is approximated with the inverse value of the distance to the center of the belonging cluster, and 𝐾 is the number of clusters generated by the GMM model for a leaf node.  <ref type="bibr" target="#b0">[1]</ref>, WCR <ref type="bibr" target="#b0">[1]</ref>, and the proposed GWRR, in terms of PSNR (dB) with an upscale factor (×3) on some public standard test images Note that when the number of samples in a leaf node becomes bigger, the performance of the GWRR model will achieve less advantage than the normal regression model, because the higher weights will be averaged by a large number of other samples. Theoretically, the regression of a leaf node can benefit from the GWRR model, particularly when there are a few samples falling into the leaf node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental results in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Initial Estimation with Iterative Back Projection</head><p>Generally speaking, SISR is a low-level computer vision task, which attempts to restore an HR image 𝒳 from a single input LR image 𝒴. A mathematical model for image degradation can be formulated as follows:</p><formula xml:id="formula_36">𝒴 = (𝒳 * ℬ) ↓ 𝑠,<label>(27)</label></formula><p>where ℬ is a low-pass (blur) filter and 𝑠 denotes the down-sampling operator with 𝑠 factor. Based on a given LR image 𝒴, how to achieve an approximated HR image 𝒳 ̂ is a classic inverse problem, which requires priors based on the Bayesian theory.</p><p>Irani and Peleg <ref type="bibr" target="#b53">[54]</ref> firstly proposed an iterative back projection (IBP) method for SR reconstruction, and IBP is the most effective way to obtain an HR image when comparing it with other SR methods. In the IBP method, the reconstruction error of an estimated LR image 𝒴 ̂ is the difference between the input LR 𝒴 and the synthesized image 𝒴 ̂ generated from the estimated HR image 𝒳 ̂ as follows:</p><formula xml:id="formula_37">𝑒(𝒴 ̂) = 𝒴 -𝒴 ̂= 𝒴 -(𝒳 ̂ * ℬ) ↓ 𝑠.<label>(28)</label></formula><p>IBP is an efficient approach to obtain the HR image by minimizing the reconstruction error defined by Eqn. <ref type="bibr" target="#b27">(28)</ref>. For the IBP approach on SISR, the updating procedure can be summarized as the following two steps, performed iteratively:</p><p>• Compute the reconstruction error 𝑒(𝒳 ̂) with the following equation:</p><formula xml:id="formula_38">𝑒(𝒳 ̂) = 𝑒(𝒴 ̂) ↑ s * 𝑝,<label>(29)</label></formula><p>where ↑ is the up-sampling operator and 𝑝 is a constant back-projection kernel to approximate the inverse operation of the low-pass filter ℬ.</p><p>• Update the estimating HR image 𝒳 ̂ by back-projecting errors as follows:</p><formula xml:id="formula_39">𝒳 ̂𝑡+1 = 𝒳 ̂𝑡 + 𝑒(𝒳 ̂𝑡),<label>(30)</label></formula><p>where 𝒳 ̂𝑡 is the estimated HR image at the 𝑡-th iteration.</p><p>Most learning-based algorithms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref> follow the milestone work in <ref type="bibr" target="#b27">[28]</ref>, which uses the coarse estimation firstly obtained via bicubic interpolation. As we know, the classic IBP algorithm is an efficient way to obtain high-quality up-scaled images, but it will inevitably produce artifacts (such as ringing, jaggy effects, and noise) at the output, because the kernel operator 𝑝 in Eqn. ( <ref type="formula" target="#formula_38">29</ref>) is hard to estimate accurately. That is the reason why algorithms with IBP need an additional denoising process <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b57">58]</ref>.</p><p>However, the sparse-constraint-based approach <ref type="bibr" target="#b27">[28]</ref> does not have this denoising capability.</p><p>As the 𝑙 2 -norm constraint-based ridge regression has the denoising effect, due to its averaging-like process, this means that the ridge regression-based RF scheme has the denoise capability intrinsically.</p><p>Based on this observation, we obtain the coarse estimation of an HR image 𝒳 ̂ by applying IBP to the corresponding input LR image 𝒴. Experimental results in Table-2 and Table-3 validate that using IBP, instead of bicubic, to obtain the initial coarse estimation can help the RF-based SR method obtain a remarkable improvement. As the number of trees is an important parameter in RF-based approaches, we plot the performance with respect to the number of trees. As shown in Fig. <ref type="figure" target="#fig_7">8</ref>, the performance of the RF-based image superresolution method increases as expected, but the increment becomes relatively smaller after a certain number of trees are used. The experimental results in Fig. <ref type="figure" target="#fig_7">8</ref> are obtained on the Set14 dataset, and 2 million samples from the dataset are used for all training stages. It shows that using 45 trees is an optimal number, as a trade-off between performance and computational cost. Therefore, we set the number of trees for the proposed FARF method at 45, and our method with this number is denoted as FARF*. The performances of our methods, and other methods, are tabulated in Table <ref type="table" target="#tab_3">-2</ref> and<ref type="table" target="#tab_3">Table-</ref>3. We also compare our methods with a recently proposed deep-learning-based algorithm, SRCNN algorithm <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>, and our methods outperform it in some cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Fine-Tuning with Proper Trees in Random Forest</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Algorithm Workflow</head><p>The training and inference stages of the proposed FARF algorithm are described in Algorithm 1 and Algorithm 2, respectively. To help the readers understand our paper, the source code of our algorithm will be available at: https://github.com/HarleyHK/FARF, for reference. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>In this section, we evaluate our algorithm on standard super-resolution benchmarks Set 5, Set14 and B100 <ref type="bibr" target="#b19">[20]</ref>, and compare it with some state-of-the-art methods. They are bicubic interpolation, adjusted anchored neighborhood regression (A+) <ref type="bibr" target="#b4">[5]</ref>, standard RF <ref type="bibr" target="#b7">[8]</ref>, alternating regression forests (ARF) <ref type="bibr" target="#b7">[8]</ref>,</p><p>and the convolutional neural-network-based image super-resolution (SRCNN) <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>, as listed in Table-  The objective quality metric, PSNR, in Table-2 also shows that the fine-tuned FARF, i.e. FARF*, can further improve the image quality, which is comparable to recently proposed state-of-the-art deeplearning-based algorithms, such as SRCNN <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>Comparing our proposed FARF algorithm to other methods, the improved visual quality of our results is obvious, as shown in Fig. <ref type="figure">9</ref>. This shows that our method can produce more details, particularly on some texture-rich regions.  <ref type="figure">9</ref>: Super-resolution (×3) images from B100, bicubic, A+ (ACCV-2014) <ref type="bibr" target="#b4">[5]</ref>, ARF (CVPR-2015) <ref type="bibr" target="#b7">[8]</ref>, SRCNN (PAMI-2016) <ref type="bibr" target="#b37">[38]</ref>, our proposed algorithm FARF, and ground truth. The results show that our FARF algorithm can produce more details and its performance is comparable to a recent state-of-the-art deep-learning method <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>This paper presents a feature-augmented random forest (FARF) scheme for the single image superresolution (SISR) task by augmenting features and redesigning the inner structure of a random forest </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: An overview of the proposed FARF framework for image super-resolution.</figDesc><graphic coords="2,101.57,124.97,392.10,284.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Random forest for clustering data.</figDesc><graphic coords="5,167.15,339.60,260.77,135.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Features extracted from LR image patches through the first and second-order gradients and gradient magnitude filters, are concatenated to form augmented features with more discriminative</figDesc><graphic coords="7,185.90,589.30,223.05,81.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Visualization of the features from a generated image. Row-1: an original gray image, the orientation (gradient angle) image, and the gradient magnitude image; Row-2: horizontal gradient 𝜕𝐼/𝜕𝑥, vertical gradient 𝜕𝐼/𝜕𝑦, and the sum: (𝜕𝐼/𝜕𝑥 + 𝜕𝐼/𝜕𝑦).</figDesc><graphic coords="8,162.75,470.20,90.10,90.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Visualization of the features from a natural image. Row-1: original color image, image gradient orientation and image gradient magnitude; Row-2: horizontal gradient 𝜕𝐼/𝜕𝑥, vertical gradient 𝜕𝐼/𝜕𝑦 and the sum: (𝜕𝐼/𝜕𝑥 + 𝜕𝐼/𝜕𝑦).</figDesc><graphic coords="9,95.40,375.55,132.88,90.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>scheme, the original-compressed coupled feature sets are worked for different purposes at different stages, i.e., the original edge features are used for regression in the leaf nodes, and the compressed features derived from the LSH-like functions are employed for node splitting (clustering) in the training stage, and node searching in the inference stage in the split nodes.</figDesc><graphic coords="10,113.60,339.60,378.10,120.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Augmented features for regressors and the LSH compressed features for searching in a random forest</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: The image super-resolution quality (i.e., measured by PSNR) with different numbers of trees in a random forest for superresolution (3x) experiments on Set14. The number of trees = 45 gives a better trade-off between efficiency and complexity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Algorithm 1 : 1 : 2 : 3 : 4 : 5 : 6 :Algorithm 2 : 1 :} 2 : 3 : 4 : 5 :</head><label>1123456212345</label><figDesc>Training Stage of FARF based Image Super-Resolution: Input: {𝓎 𝑖 , 𝓍 𝑖 } 𝑖=1 𝑁 : training LR-HR patch pairs; Output: the trained random forest 𝒯 with regressors ℛ = (ℛ 1 , … ), the LSH model: ℳ 𝐿𝑆𝐻 ; Upscale the input LR patch images as initial coarse estimations using IBP; ⇒ {Eqn. (29, 30)} Obtain discriminative features calculated from patches by the first-order, second-order (horizontal and vertical) gradients, and gradient magnitudes for up-scaled coarse versions; ⇒ {Eqn. (15, 16)} Conduct LSH on raw features to obtain compressed features, at the same time obtain the trained LSH projection model ℳ 𝐿𝑆𝐻 ; Train a random forest with the compressed features via the LSH model ℳ 𝐿𝑆𝐻 ; Train the weighted ridge regressors ℛ by the GWRR models in leaf nodes; ⇒ {Eqn. (25)} Save the random forest 𝒯 with ridge regressors ℛ , and the trained LSH model: ℳ 𝐿𝑆𝐻 . Inference Stage of FARF based Image Super-Resolution: Input: testing LR image 𝒴, the trained random forest 𝒯 with ridge regressors ℛ = (ℛ 1 , … ), the trained LSH model: ℳ 𝐿𝑆𝐻 ; Output: super-solved image 𝒳 ̂; Upscale the patches from LR 𝒴 to form an initial coarse estimation by IBP; ⇒ {Eqn. (29, 30)Compute the discriminative features for all the patches; ⇒ {Eqn. (15, 16)} Compute the compressed feature via the LSH model ℳ 𝐿𝑆𝐻 ; For each patch, using the compressed feature to search the leaf nodes to obtain its corresponding regressor from the trained random forest 𝒯; Get the super-resolved image 𝒳 ̂ through all the super-solved patches by weighted ridge regressors ℛ in leaf nodes. ⇒ {Eqn. (22)}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>(</head><label></label><figDesc>RF), with different feature recipes at different stages, where the compressed features are used for clustering in the split nodes and the original features are used for regression in the leaf nodes. The contributions of this paper are threefold: (1) the more discriminative gradient magnitude-based augmented features are proposed for clustering on split nodes and regression on leaf nodes; (2) By extending principal component analysis (PCA) to a generalized unsupervised locality-sensitive hashing (LSH) model for dimensionality reduction, we lay out an original compressed coupled feature set for tackling the clustering-regression tasks, which unify SISR and content-based image retrieval (CBIR) for LSH evaluation; and (3) we have extended WCR model to a generalized GWRR model for ridgeregression. The proposed FAFR scheme can achieve highly competitive quality results, e.g., obtaining about a 0.3dB gain in PSNR, on average, when compared to conventional RF-based super-resolution approaches. Furthermore, a fine-tuned version of our proposed FARF approach is provided, whose performance is comparable to recent state-of-the-art deep-learning-based algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc> show that the proposed GWRR model can achieve the same level of performance as WCR<ref type="bibr" target="#b0">[1]</ref>, and obtain 0.2dB gain more than the ANR<ref type="bibr" target="#b0">[1]</ref> model.</figDesc><table><row><cell>Images</cell><cell>baboon</cell><cell>baby</cell><cell>bird</cell><cell>butterfly</cell><cell>foreman</cell><cell>head</cell><cell>lenna</cell><cell>woman</cell><cell>Average</cell></row><row><cell>ANR</cell><cell>23.52</cell><cell>35.06</cell><cell>34.44</cell><cell>25.74</cell><cell>32.92</cell><cell>33.54</cell><cell>32.92</cell><cell>30.17</cell><cell>31.04</cell></row><row><cell>WCR</cell><cell>23.55</cell><cell>35.09</cell><cell>34.75</cell><cell>26.18</cell><cell>33.51</cell><cell>33.61</cell><cell>33.16</cell><cell>30.42</cell><cell>31.28</cell></row><row><cell>GWRR</cell><cell>23.54</cell><cell>35.09</cell><cell>34.74</cell><cell>26.13</cell><cell>33.46</cell><cell>33.58</cell><cell>33.12</cell><cell>30.38</cell><cell>31.25</cell></row><row><cell cols="3">Table-1: Performances of ANR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>2 and Table-3. We set the same parameters for all the RF-based algorithms, i.e., the number of trees in an RF is 10, and the maximum depth of each tree is 15. We use the same set of training images (91 images) for all the algorithms, as previous works<ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8]</ref> do. RF+ means a normal RF-based algorithm added with the two gradient magnitudes augmented features, and RF# is the normal RF-based algorithm, where the original raw features, instead of using the PCA compressed features, are used to learn the regressors in leaf nodes. FARF denotes our proposed feature-augmented RF scheme, which combines RF+ and RF# by adding the gradient magnitude features and using the original raw features for regression.Table-2: Results of the proposed method compared with state-of-the-art works on 3 datasets in terms of PSNR (dB) using three different magnification factors (#) (×2, ×3, ×4).</figDesc><table><row><cell cols="12">FARF* is a further refined version of FARF, by performing further fine-tuning: (1) using the superior,</cell></row><row><cell cols="12">unsupervised LSH projection, instead of PCA for dimensionality reduction, (2) employing IBP, instead</cell></row><row><cell cols="12">of the traditional bicubic interpolation algorithm, to obtain the initial coarse estimation in the inference</cell></row><row><cell cols="9">stage, (3) setting proper number of trees (e.g., 45) for training an RF.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>dataset</cell><cell>#</cell><cell>bicubic</cell><cell>A+</cell><cell>RF</cell><cell>ARF</cell><cell>RF +</cell><cell>RF #</cell><cell>FARF -</cell><cell>FARF</cell><cell>FARF*</cell><cell>SRCNN</cell></row><row><cell></cell><cell>×2</cell><cell>33.66</cell><cell>36.55</cell><cell>36.52</cell><cell>36.65</cell><cell>36.67</cell><cell>36.63</cell><cell>36.68</cell><cell>36.78</cell><cell>36.81</cell><cell>36.66</cell></row><row><cell>Set5</cell><cell>×3</cell><cell>30.39</cell><cell>32.59</cell><cell>32.44</cell><cell>32.53</cell><cell>32.56</cell><cell>32.53</cell><cell>32.62</cell><cell>32.73</cell><cell>32.78</cell><cell>32.75</cell></row><row><cell></cell><cell>×4</cell><cell>28.42</cell><cell>30.29</cell><cell>30.10</cell><cell>30.17</cell><cell>30.18</cell><cell>30.22</cell><cell>30.27</cell><cell>30.39</cell><cell>30.45</cell><cell>30.48</cell></row><row><cell></cell><cell>×2</cell><cell>30.23</cell><cell>32.28</cell><cell>32.26</cell><cell>32.33</cell><cell>32.37</cell><cell>32.32</cell><cell>32.37</cell><cell>32.41</cell><cell>32.45</cell><cell>32.42</cell></row><row><cell>Set14</cell><cell>×3</cell><cell>27.54</cell><cell>29.13</cell><cell>29.04</cell><cell>29.10</cell><cell>29.17</cell><cell>29.11</cell><cell>29.17</cell><cell>29.23</cell><cell>29.29</cell><cell>29.28</cell></row><row><cell></cell><cell>×4</cell><cell>26.00</cell><cell>27.33</cell><cell>27.22</cell><cell>27.28</cell><cell>27.31</cell><cell>27.29</cell><cell>27.36</cell><cell>27.45</cell><cell>27.48</cell><cell>27.49</cell></row><row><cell></cell><cell>×2</cell><cell>29.32</cell><cell>30.78</cell><cell>31.13</cell><cell>31.21</cell><cell>31.22</cell><cell>31.23</cell><cell>31.34</cell><cell>31.35</cell><cell>31.38</cell><cell>31.36</cell></row><row><cell>B100</cell><cell>×3</cell><cell>27.15</cell><cell>28.18</cell><cell>28.21</cell><cell>28.26</cell><cell>28.27</cell><cell>28.26</cell><cell>28.30</cell><cell>28.35</cell><cell>28.38</cell><cell>28.41</cell></row><row><cell></cell><cell>×4</cell><cell>25.92</cell><cell>26.77</cell><cell>26.74</cell><cell>26.77</cell><cell>26.78</cell><cell>26.79</cell><cell>26.83</cell><cell>26.88</cell><cell>26.91</cell><cell>26.90</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table - 2</head><label>-</label><figDesc>summarizes the performances of our proposed algorithm on the 3 datasets, in terms of the average peak signal to noise ratio (PSNR) scores, with different magnification factors (×2, ×3, ×4).Table-3 gives more details of the results on some images from the Set5 dataset, with magnification factor ×3. As the results have shown based on the 3 datasets, our proposed algorithm FARF outperforms A+ and ARF for all the magnification factors. Table-3: Results of the proposed method compared with state-of-the-arts methods on 3 datasets in terms of PSNR (dB) with magnification factors (×3) on dataset Set5.</figDesc><table><row><cell cols="2">Set5(×3) bicubic</cell><cell>Zeyde</cell><cell>A+</cell><cell>RF</cell><cell>ARF</cell><cell>FARF -</cell><cell>FARF</cell><cell>FARF*</cell><cell>SRCNN</cell></row><row><cell>baby</cell><cell>33.91</cell><cell>35.13</cell><cell>35.23</cell><cell>35.25</cell><cell>35.15</cell><cell>35.20</cell><cell>35.34</cell><cell>35.37</cell><cell>35.25</cell></row><row><cell>bird</cell><cell>32.58</cell><cell>34.62</cell><cell>35.53</cell><cell>35.23</cell><cell>35.31</cell><cell>35.39</cell><cell>35.53</cell><cell>35.54</cell><cell>35.47</cell></row><row><cell>butterfly</cell><cell>24.04</cell><cell>25.93</cell><cell>27.13</cell><cell>27.00</cell><cell>27.39</cell><cell>27.65</cell><cell>27.68</cell><cell>27.82</cell><cell>27.95</cell></row><row><cell>head</cell><cell>32.88</cell><cell>33.61</cell><cell>33.82</cell><cell>33.73</cell><cell>33.73</cell><cell>33.75</cell><cell>33.84</cell><cell>33.85</cell><cell>33.71</cell></row><row><cell>woman</cell><cell>28.56</cell><cell>30.32</cell><cell>31.24</cell><cell>30.98</cell><cell>31.08</cell><cell>31.11</cell><cell>31.27</cell><cell>31.34</cell><cell>31.37</cell></row><row><cell>average</cell><cell>30.39</cell><cell>31.92</cell><cell>32.59</cell><cell>32.44</cell><cell>32.53</cell><cell>32.62</cell><cell>32.73</cell><cell>32.78</cell><cell>32.75</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast super-resolution based on weighted collaborative representation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-M</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014 19. 2014</date>
			<biblScope unit="page" from="914" to="918" />
		</imprint>
	</monogr>
	<note>Digital Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Anchored neighborhood regression for fast example-based super-resolution</title>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1920" to="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shape quantization and recognition with randomized trees</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1545" to="1588" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on curves and surfaces</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A+: Adjusted anchored neighborhood regression for fast super-resolution</title>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">De</forename><surname>Smet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="111" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A generalized solution of the orthogonal Procrustes problem</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Schönemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jé Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3304" to="3311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast and accurate image upscaling with super-resolution forests</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3791" to="3799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Single Image Super-Resolution via Locally Regularized Anchored Neighborhood Regression and Nonlocal Means</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="26" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A tutorial on support vector machines for pattern recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data mining and knowledge discovery</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="167" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An introduction to support vector machines</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge University Press Cambridge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Experiments with a new boosting algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">icml</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1867" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Solutions of ill-posed problems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Tikhonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I A K</forename><surname>Arsenin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>John</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977">1977</date>
			<pubPlace>Winston Washington, DC</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast discriminative visual codebooks using randomized clustering forests</title>
		<author>
			<persName><forename type="first">F</forename><surname>Moosmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cascaded face alignment via intimacy definition feature</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-M</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">53024</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Naive bayes super-resolution forest</title>
		<author>
			<persName><forename type="first">J</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pé Rez-Pellitero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="325" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
	<note>in Computer Vision, 2001. ICCV 2001. Proceedings</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1685" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optimized cartesian k-means</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="180" to="192" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Alternating decision forests</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="508" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Alternating regression forests for object detection and pose estimation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Experiments with a new boosting algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">icml</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Greedy function approximation: a gradient boosting machine</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of statistics</title>
		<imprint>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Class-specific hough forests for object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Decision forests for computer vision and medical image analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="143" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Super-resolution through neighbor embedding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2004. Proceedings of the 2004 IEEE Computer Society Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Alberi-Morel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast direct super-resolution by simple functions</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="561" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning multiple linear mappings for efficient single image superresolution</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="846" to="861" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adaptive local nonparametric regression for fast single image super-resolution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Communications and Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015, 2015</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Single Image Super-Resolution via Locally Regularized Anchored Neighborhood Regression and Nonlocal Means</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="26" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image super-resolution based on dictionary learning and anchored neighborhood regression with mutual incoherence</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="591" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">CCR: Clustering and Collaborative Representation for Fast Single Image Super-Resolution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="405" to="417" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04802</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">New edge-directed interpolation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Orchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1521" to="1527" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Image interpolation by adaptive 2-D autoregressive modeling and soft-decision estimation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="887" to="896" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An edge-guided image interpolation algorithm via directional filtering and data fusion</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2226" to="2238" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Modified edge-directed interpolation for images</title>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Siu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic imaging</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13011" to="013011" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Example-based super-resolution</title>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pasztor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep learning of binary hash codes for fast image retrieval</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep supervised hashing for fast image retrieval</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2064" to="2072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Similarity search in high dimensions via hashing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="518" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Locality-constrained linear coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">From dictionary of visual words to subspaces: locality-constrained affine subspace coding</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2348" to="2357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Guided iterative back-projection scheme for single-image super-resolution</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-M</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Global High Tech Congress on Electronics (GHTCE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="175" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Image processing using smooth ordering of its patches</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2764" to="2774" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">BM3D frames and variational image deblurring</title>
		<author>
			<persName><forename type="first">A</forename><surname>Danielyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1715" to="1728" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Improving resolution by image registration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVGIP: Graphical Models and Image Processing</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Antipodally Invariant Metrics For Fast Regression-Based Super-Resolution</title>
		<author>
			<persName><forename type="first">E</forename><surname>Perez-Pellitero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ruiz-Hidalgo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">2468</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Joint Maximum Purity Forest with Application to Image Super-Resolution</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-M</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.09200</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2916" to="2929" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Nonlocal back projection for adaptive image enlargement</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2009-11">November 2009</date>
			<biblScope unit="page" from="349" to="352" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
