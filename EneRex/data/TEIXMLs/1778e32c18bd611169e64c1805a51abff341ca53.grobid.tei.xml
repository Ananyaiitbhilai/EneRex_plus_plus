<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NATURAL LANGUAGE INFERENCE OVER INTERACTION SPACE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-09-13">13 Sep 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yichen</forename><surname>Gong</surname></persName>
							<email>yichen.gong@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
								<address>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Horizon Robotics, Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Heng</forename><surname>Luo</surname></persName>
							<email>heng.luo@hobot.cc</email>
							<affiliation key="aff1">
								<orgName type="institution">Horizon Robotics, Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
							<email>jian.zhang@hobot.cc</email>
							<affiliation key="aff1">
								<orgName type="institution">Horizon Robotics, Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NATURAL LANGUAGE INFERENCE OVER INTERACTION SPACE</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-09-13">13 Sep 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">66EA275FE3D99C2D6FD0A24982E6C008</idno>
					<idno type="arXiv">arXiv:1709.04348v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-11-21T16:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Natural Language Inference (NLI) task requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis. We introduce Interactive Inference Network (IIN), a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve natural language inference, and a denser interaction tensor contains richer semantic information. One instance of such architecture, Densely Interactive Inference Network (DIIN), demonstrates the state-of-the-art performance on large scale NLI copora and large-scale NLI alike corpus. It's noteworthy that DIIN achieve a greater than 20% error reduction on the challenging Multi-Genre NLI (MultiNLI; Williams et al. 2017) dataset with respect to the strongest published system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Natural Language Inference (NLI also known as recognizing textual entiailment, or RTE) task requires one to determine whether the logical relationship between two sentences is among entailment (if the premise is true, then the hypothesis must be true), contradiction (if the premise is true, then the hypothesis must be false) and neutral (neither entailment nor contradiction). NLI is known as a fundamental and yet challenging task for natural language understanding <ref type="bibr" target="#b65">(Williams et al., 2017)</ref>, not only because it requires one to identify the language pattern, but also to understand certain common sense knowledge. In Table <ref type="table">1</ref>, three samples from MultiNLI corpus show solving the task requires one to handle the full complexity of lexical and compositional semantics. Entailment is related to a broad range of tasks: in abstractive summarization, the generated summarization text should be entailed by the text; in paraphrase identification, the paraphrased sentences entail each other; in information retrieval, the retrieved text is entailed by the source context <ref type="bibr">(Bos &amp; Markert, 2005)</ref>. The previous work on NLI (or RTE) has extensively researched on conventional approaches <ref type="bibr" target="#b25">(Fyodorov et al., 2000;</ref><ref type="bibr">Bos &amp; Markert, 2005;</ref><ref type="bibr" target="#b41">MacCartney &amp; Manning, 2009)</ref>. Recent progress on NLI is enabled by the availability of 570k human annotated dataset <ref type="bibr">(Bowman et al., 2015)</ref> and the advancement of representation learning technique.</p><p>Among the core representation learning techniques, attention mechanism is broadly applied in many NLU tasks since its introduction: machine translation <ref type="bibr" target="#b15">(Bahdanau et al., 2014)</ref>, abstractive summarization <ref type="bibr" target="#b52">(Rush et al., 2015)</ref>, Reading <ref type="bibr">Comprehension(Hermann et al., 2015)</ref>, dialog system <ref type="bibr" target="#b43">(Mei et al., 2016)</ref>, etc. As described by <ref type="bibr" target="#b61">Vaswani et al. (2017)</ref>, "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key". Attention mechanism is known for its alignment between representations, focusing one part of representation over another, and modeling the dependency regardless of sequence length. The attention weight follows softmax layer is the essential component of attention <ref type="bibr" target="#b15">(Bahdanau et al., 2014)</ref>. The singlechannel attention weight can be viewed as a single-channel interaction tensor. An single-channel interaction tensor represents the word-by-word interaction between two sentences in one dimension. On the other hand, A multi-channel attention weight, applied on multi-head attention <ref type="bibr">(Vaswani et al.,</ref> Table <ref type="table">1</ref>: Samples from MultiNLI datasets. 2017) to align sentences in different representation subspace, can be viewed as a multi-channel interaction tensor. Observing attention's powerful capability, We hypothesize that interaction tensor contains the necessary semantic information required for understanding the text. In this work, we demonstrate the feasibility that natural language inference task can be tackled directly through extracting semantic feature in interaction space. By incorporating the powerful feature extractor such as deep 2-D convolutional neural network architectures, we can extract n-gram pair semantic interaction features from interaction tensor. Our Interactive Inference Network (IIN) architecture is fully compatible with convolutional feature extractor that works well on CIFAR100 <ref type="bibr" target="#b37">(Krizhevsky, 2009)</ref> or ImageNET <ref type="bibr" target="#b53">(Russakovsky et al., 2015)</ref> with minor adaptation. It builds up a bridge between NLU and computer vision. By hierarchically stacking the feature extractor, the model can understand the text from word level, phrase level to sentence level.</p><p>The goal of reducing sequential computation lays the foundation of several recent work such as Extended Neural GPU <ref type="bibr">(Kaiser &amp; Bengio, 2016)</ref>, ByteNet <ref type="bibr" target="#b34">(Kalchbrenner et al., 2016)</ref>, ConvS2S <ref type="bibr" target="#b26">(Gehring et al., 2017)</ref>, Transformer <ref type="bibr" target="#b61">(Vaswani et al., 2017)</ref>. Recurrent structure generate a sequence of hidden state h t , as the function of previous hidden state h t-1 and the input for position t. The hard constraint precludes the parallelization within training, results in higher computational time complexity thus delays training <ref type="bibr" target="#b61">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b26">Gehring et al., 2017)</ref>. To tackle the problem, we propose a simple encoder without using any recurrent or recursive structure in this paper.</p><p>Our experiments show that one instance of Interactive Inference Network, Densely Interactive Inference Network, achieves new state-of-the-art performance on both SNLI and MultiNLI copora. To test the generality of our architecture, we interpret the paraphrase identification task as natural language inference task where matching as entailment, not matching as neutral. We test the model on Quora Question Pair dataset, which contains over 400k real world question pair, and achieves new state-of-the-art performance.</p><p>We introduce the related work in Section 2, and discuss the general framework of IIN along with a specific instance that enjoys state-of-the-art performance on multiple datasets in Section 3. We describe experiments and analysis in Section 4. Finally, we conclude and discuss future work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The early exploration on NLI mainly rely on conventional methods and small scale datasets <ref type="bibr" target="#b42">(Marelli et al., 2014)</ref>. The availability of SNLI dataset with 570k human annotated sentence pairs has enabled a good deal of progress on natural language understanding. The essential representation learning techniques for NLU such as attention <ref type="bibr" target="#b6">(Wang &amp; Jiang, 2015)</ref>, memory <ref type="bibr">(Munkhdalai &amp; Yu, 2016)</ref> and the use of parse structure <ref type="bibr">(Bowman et al., 2016;</ref><ref type="bibr" target="#b45">Mou et al., 2015)</ref> are studied on the SNLI which serves as an important benchmark for sentence understanding. The models trained on NLI task can be divided into two categories: (i) sentence encoding-based model which aims to find vector representation for each sentence and classifies the relation by using the concatenation of two vector representation along with their absolute element-wise difference and element-wise product <ref type="bibr">(Bowman et al., 2016;</ref><ref type="bibr" target="#b62">Vendrov et al., 2015;</ref><ref type="bibr" target="#b45">Mou et al., 2015;</ref><ref type="bibr" target="#b40">Liu et al., 2016;</ref><ref type="bibr">Munkhdalai &amp; Yu, 2016)</ref>. (ii) Joint feature models which use the cross sentence feature or attention from one sentence to another <ref type="bibr" target="#b51">(Rocktäschel et al., 2015;</ref><ref type="bibr" target="#b6">Wang &amp; Jiang, 2015;</ref><ref type="bibr" target="#b24">Cheng et al., 2016;</ref><ref type="bibr" target="#b8">Parikh et al., 2016;</ref><ref type="bibr" target="#b64">Wang et al., 2017;</ref><ref type="bibr" target="#b10">Yu &amp; Munkhdalai, 2017;</ref><ref type="bibr" target="#b55">Sha et al., 2016)</ref>.</p><p>After neural attention mechanism is successfully applied on the machine translation task, such technique has became widely used in both natural language process and computer vision domains. Many variants of attention technique such as hard-attention <ref type="bibr" target="#b66">(Xu et al., 2015)</ref>, self-attention <ref type="bibr" target="#b8">(Parikh et al., 2016)</ref>, multi-hop attention <ref type="bibr" target="#b27">(Gong &amp; Bowman, 2017)</ref>, bidirectional attention <ref type="bibr" target="#b54">(Seo et al., 2016)</ref> and multi-head attention <ref type="bibr" target="#b61">(Vaswani et al., 2017)</ref> are also introduced to tackle more complicated tasks. Before this work, neural attention mechanism is mainly used to make alignment, focusing on specific part of the representation. In this work, we want to show that attention weight contains rich semantic information required for understanding the logical relationship between sentence pair.</p><p>Though RNN or LSTM are very good for variable length sequence modeling, using Convolutional neural network in NLU tasks is very desirable because of its parallelism in computation. Convolutional structure has been successfully applied in various domain such as machine translation <ref type="bibr" target="#b26">(Gehring et al., 2017)</ref>, sentence classification <ref type="bibr" target="#b35">(Kim, 2014</ref>), text matching <ref type="bibr" target="#b30">(Hu et al., 2014)</ref> and sentiment analysis <ref type="bibr">(Kalchbrenner et al., 2014)</ref>, etc. The convolution structure is also applied on different level of granularity such as byte <ref type="bibr" target="#b68">(Zhang &amp; LeCun, 2017)</ref>, character <ref type="bibr" target="#b69">(Zhang et al., 2015)</ref>, word <ref type="bibr" target="#b26">(Gehring et al., 2017)</ref> and sentences <ref type="bibr" target="#b45">(Mou et al., 2015)</ref> levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">INTERACTIVE INFERENCE NETWORK</head><p>The Interactive Inference Network (IIN) is a hierarchical multi-stage process and consists of five components. Each of the components is compatible with different type of implementations. Potentially all exiting approaches in machine learning, such as decision tree, support vector machine and neural network approach, can be transfer to replace certain component in this architecture. We focus on neural network approaches below. Figure <ref type="figure" target="#fig_0">1</ref> provides a visual illustration of Interactive Inference Network.</p><p>1. Embedding Layer converts each word or phrase to a vector representation and construct the representation matrix for sentences. In embedding layer, a model can map tokens to vectors with the pre-trained word representation such as GloVe <ref type="bibr" target="#b50">(Pennington et al., 2014)</ref>, word2Vec <ref type="bibr" target="#b44">(Mikolov et al., 2013)</ref> and fasttext <ref type="bibr" target="#b32">(Joulin et al., 2016)</ref>. It can also utilize the preprocessing tool, e.g. named entity recognizer, part-of-speech recognizer, lexical parser and coreference identifier etc., to incorporate more lexical and syntactical information into the feature vector. 2. Encoding Layer encodes the representations by incorporating the context information or enriching the representation with desirable features for future use. For instance, a model can adopt bidirectional recurrent neural network to model the temporal interaction on both direction, recursive neural network <ref type="bibr">(Socher et al., 2011b)</ref> (also known as TreeRNN) to model the compositionality and the recursive structure of language, or self-attention to model the long-term dependency on sentence. Different components of encoder can be combined to obtain a better sentence matrix representation. 3. Interaction Layer creates an word-by-word interaction tensor by both premise representation and hypothesis representation matrix. In a TreeRNN setting, the interaction layer models the interaction between each node pair <ref type="bibr">(Socher et al., 2011a)</ref>. The interaction can be modeled in different ways. A common approach is to compute the cosine similarity or dot product between each pair of feature vector. On the other hand, a denser interaction tensor can be obtained by using linear layers to scale down element-wise product between each pair of feature vector. In the interaction tensor, one channel of the tensor represents the words interact in one dimension(perspective) and therefore having d channels shows that the model understands the sentences with a implicit world representation of d dimensions(perspectives). 4. Feature Extraction Layer adopts feature extractor to extract the semantic feature from interaction tensor. The convolutional feature extractors, such as AlexNet(Krizhevsky et al., 2012), VGG <ref type="bibr" target="#b56">(Simonyan &amp; Zisserman, 2014)</ref>, Inception <ref type="bibr" target="#b59">(Szegedy et al., 2014)</ref>, ResNet <ref type="bibr" target="#b28">(He et al., 2016)</ref> and DenseNet <ref type="bibr" target="#b31">(Huang et al., 2016)</ref>, proven work well on image recognition are completely compatible under such architecture. The advance in computer vision is now transferable to natural language understanding. Unlike the work <ref type="bibr" target="#b35">(Kim, 2014;</ref><ref type="bibr" target="#b69">Zhang et al., 2015)</ref> who employs 1-D sliding window, our CNN architecture allows 2-D kernel to extract semantic interaction feature from the word-by-word interaction between n-gram pair. Sequential or tree-like feature extractors are also applicable in the feature extraction layer.</p><p>5. Output Layer decodes the acquired feature to give prediction. Under the setting of NLI, it generates the confidence on each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DENSELY INTERACTIVE INFERENCE NETWORK</head><p>One example of IIN is Densely Interactive Inference Network (DIIN), a relatively simple structure but produces state-of-the-art performance on multiple datasets.</p><p>Embedding Layer: For DIIN, we use the concatenation of word embedding, character feature and syntactical features. The word embedding is obtained by mapping token to high dimensional vector space by pre-trained word vector (840B GloVe). Word embedding is updated during training.</p><p>As in <ref type="bibr" target="#b36">(Kim et al., 2016;</ref><ref type="bibr" target="#b39">Lee et al., 2016)</ref>, The character feature is obtained by using a convolutional neural network followed by a max pooling over the learned character vectors. Syntactical features include one-hot part-of-speech tagging feature and binary exact match feature. The exact match value is activated if there are tokens with same stem or lemma in the other sentence as the corresponding token. The exact match feature is simple while found extremely useful in reading comprehension task <ref type="bibr">(Chen et al., 2017a)</ref>. It helps to speed up the convergence speed in NLI task. Now we have premise representation P ∈ R p×d and hypothesis representation H ∈ R h×d , where p refers to the sequence length of premise, h refers to the sequence length of hypothesis and d means the dimension of both representation. The 1-D convolutional neural network and character features weights share the same set of parameters between premise and hypothesis.</p><p>Encoding Layer: In the encoding layer, premise representation P and hypothesis representation H are passed through a two-layer highway network, thus having P hw ∈ R p×d and H hw ∈ R h×d for new premise representation and new hypothesis representation. These new representation are then passed to intra-attention (self-attention) layer to take into account the word order and context information. Take premise as example, we model intra-attention by</p><formula xml:id="formula_0">A ij = α(P hw i , P hw j , w itrAtt ) ∈ R<label>(1)</label></formula><formula xml:id="formula_1">P itrAtt i = p j=1 exp(A ij ) p k=1 exp(A kj ) P hw j , ∀i, j ∈ [1, ..., p]<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">P itrAtt i is a weighted summation of P hw . We choose α(a, b, , w itrAtt ) = w itrAtt [a; b; a • b],</formula><p>where w itrAtt ∈ R 3d is a trainable weight, • is element-wise multiplication, [;] is vector concatenation across row, and the implicit multiplication is matrix multiplication. Then both P hw and P itrAtt are fed into a semantic composite fuse gate (fuse gate in short), which acts as a skip connection. The fuse gate is implemented as</p><formula xml:id="formula_3">z i = tanh(W 1 [P hw i ; P itrAtt i ] + b 1 )<label>(3)</label></formula><formula xml:id="formula_4">r i = σ(W 2 [P hw i ; P itrAtt i ] + b 2 ) (4) f i = σ(W 3 [P hw i ; P itrAtt i ] + b 3 )<label>(5)</label></formula><formula xml:id="formula_5">P enc i = r i • P hw i + f i • z i<label>(6)</label></formula><p>where</p><formula xml:id="formula_6">W 1 , W 2 , W 3 ∈ R 2d×d and b 1 b 2 , b 3 ∈ R d are trainable weights, σ is sigmoid nonlinear operation.</formula><p>We do the same operation on hypothesis representation, thus having H enc . The weights of intraattention and fuse gate for premise and hypothesis are not shared, but the difference between the weights of are penalized. The penalization aims to ensure the parallel structure learns the similar functionality but is aware of the subtle semantic difference between premise and hypothesis.</p><p>Interaction Layer: The interaction layer models the interaction between premise encoded representation P enc and hypothesis encoded representation H enc as follows:</p><formula xml:id="formula_7">I ij = β(P enc i , H enc j ) ∈ R d , ∀i ∈ [1, ..., p], ∀j ∈ [1, ..., h]<label>(7)</label></formula><p>where P enc i is the i-th row vector of P enc , and H enc j is the j-th row vector of H enc . Though there are many implementations of interaction, we find β(a, b) = a • b very useful.</p><p>Feature Extraction Layer: We adopt DenseNet <ref type="bibr" target="#b31">(Huang et al., 2016)</ref> as convolutional feature extractor in DIIN. Though our experiments show ResNet <ref type="bibr" target="#b28">(He et al., 2016)</ref> works well in the architecture, we choose DenseNet because it is effective in saving parameters. One interesting observation with ResNet is that if we remove the skip connection in residual structure, the model does not converge at all. We found batch normalization delays convergence without contributing to accuracy, therefore we does not use it in our case. A ReLU activation function is applied after all convolution unless otherwise noted. Once we have the interaction tensor I, we use a convolution with 1 × 1 kernel to scale down the tensor in a ratio, FSDR, without following ReLU. If the input channel is k then the output channel is f loor(k × FSDR). Then the generated feature map is feed into three sets of Dense block <ref type="bibr" target="#b31">(Huang et al., 2016)</ref> and transition block pair. The DenseNet block contains n layers of 3 × 3 convolution layer with growth rate of GR. The transition layer has a convolution layer with 1 × 1 kernel for scaling down purpose, followed by a max pooling layer with stride 2. The transition scale down ratio in transition layer is TSDR.</p><p>Output Layer: DIIN uses a linear layer to classify final flattened feature representation to three classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we present the evaluation of our model. We first perform quantitative evaluation, comparing our model with other competitive models. We then conduct some qualitative analyses to understand how DIIN achieve the high level understanding through interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DATA</head><p>Here we introduce three datasets we evaluate our model on. The evaluation metric for all dataset is accuracy.</p><p>SNLI Stanford Natural Language Inference (SNLI; Bowman et al. 2015) has 570k human annotated sentence pairs. The premise data is draw from the captions of the Flickr30k corpus, and the hypothesis data is manually composed. The labels provided in are "entailment", "neutral', "contradiction" and "-". "-" shows that annotators cannot reach consensus with each other, thus removed during training and testing as in other works. We use the same data split as in <ref type="bibr">Bowman et al. (2015)</ref>.</p><p>MultiNLI Multi-Genre NLI Corpus (MultiNLI; <ref type="bibr" target="#b65">Williams et al. 2017</ref>) has 433k sentence pairs, whose collection process and task detail are modeled closely to SNLI. The premise data is collected from maximally broad range of genre of American English such as written non-fiction genres (SLATE, OUP, GOVERNMENT, VERBATIM, TRAVEL), spoken genres (TELEPHONE, FACE-TO-FACE), less formal written genres (FICTION, LETTERS) and a specialized one for 9/11. Half of these selected genres appear in training set while the rest are not, creating in-domain (matched) and cross-domain (mismatched) development/test sets. We use the same data split as provided by <ref type="bibr" target="#b65">Williams et al. (2017)</ref>. Since test set labels are not provided, the test performance is obtained through submission on Kaggle.com <ref type="foot" target="#foot_0">1</ref> . Each team is limited to two submissions per day.</p><p>Quora question pair Quora question pair dataset contains over 400k real world question pair selected from Quora.com. A binary annotation which stands for match (duplicate) or not match (not duplicate) is provided for each question pair. In our case, duplicate question pair can be interpreted as entailment relation and not duplicate as neutral. We use the same split ratio as mentioned in <ref type="bibr" target="#b64">(Wang et al., 2017)</ref>.</p><p>We also study the human performance for both SNLI and MultiNLI. In the dev&amp;test set on SNLI and matched&amp;mismatched development set on MultiNLI, each sentences pair is provided with a set of "annotator labels". The "annotator labels" contains a list of five labels annotated by five different annotators. The final "gold label" is set to certain label if it has equal to or greater than three votes. Otherwise, the label is set to "-", since there is no agreement. If we look from another perspective, the ratio that "annotator label" fails to match "gold label" is the human performance.</p><p>For example, if three annotators vote A and the other two annotators vote B, then following the crowd-sourcing guideline, the "gold label" would be A; in this case, the human performance is 60% for this particular sample. In light of this guideline, we calculate the human performance for SNLI dev&amp;test set and MultiNLI development sets. We don't take into account samples that labeled with "-", since we discard them during model testing. The human performance score is 88.1% for SNLI development set, 87.7% for SNLI test set, 88.5% for MultiNLI matched development set and 89.2% for MultiNLI mismatched development set. Since we don't have access to MultiNLI test set labels, we don't provide the human performance for MultiNLI test set here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EXPERIMENTS SETTING</head><p>We implement our algorithm with Tensorflow <ref type="bibr" target="#b14">(Abadi et al., 2016)</ref> framework. An Adadelta optimizer(Zeiler, 2012) with ρ as 0.95 and as 1e-8 is used to optimize all the trainable weights. The initial learning rate is set to 0.5 and batch size to 70. When the model does not improve best indomain performance for 30,000 steps, an SGD optimizer with learning rate of 3e-4 is used to help model to find a better local optimum. Dropout layers are applied before all linear layers and after word-embedding layer. We use an exponential decayed keep rate during training, where the initial keep rate is 1.0 and the decay rate is 0.977 for every 10,000 step. We initialize our word embeddings with pre-trained 300D GloVe 840B vectors <ref type="bibr" target="#b50">Pennington et al. (2014)</ref> while the out-of-vocabulary word are randomly initialized with uniform distribution. The character embeddings are randomly initialized. All weights are constraint by L2 regularization, and the L2 regularization at step t is calculated as follows:</p><formula xml:id="formula_8">L2Ratio t = σ( (t -L2F ullStep/2) × 8 L2F ullStep/2 ) × L2F ullRatio<label>(8)</label></formula><p>where L2F ullRatio determines the maximum L2 regularization ratio, and L2F ullStep determines at which step the maximum L2 regularization ratio would be applied on the L2 regularization. We choose L2F ullRatio as 0.9e -5 and L2F ullStep as 100,000. The ratio of L2 penalty between the difference of two encoder weights is set to 1e -3. For a dense block in feature extraction layer, the number of layer n is set to 8 and growth rate is set to 20. The first scale down ratio F SDR in feature extraction layer is set to 0.3 and transitional scale down ratio T SDR is set to 0.5. The sequence length is set as a hard cutoff on all experiments: 48 for MultiNLI, 32 for SNLI and 24 for Quora Question Pair Dataset. During the experiments on MultiNLI, we use 15% of data from SNLI as in <ref type="bibr" target="#b65">Williams et al. (2017)</ref>. We select the parameter by the best run of development accuracy. Our ensembling approach considers the majority vote of the predictions given by multiple runs of the same model under different random parameter initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">EXPERIMENT ON MULTINLI</head><p>We compare our result with all other published systems in Table <ref type="table" target="#tab_1">2</ref>. Besides ESIM, the state-of-theart model on SNLI, all other models appear at RepEval 2017 workshop. RepEval 2017 workshop requires all submitted model to be sentence encoding-based model therefore alignment between sentences and memory module are not eligible for competition. All models except ours share one common feature that they use LSTM as a essential building block as encoder. Our approach, without using any recurrent structure, achieves the new state-of-the-art performance of 80.0%, exceeding current state-of-the-art performance by more than 5%. Unlike the observation from <ref type="bibr" target="#b47">Nangia et al. (2017)</ref>, we find the out-of-domain test performance is consistently lower than in-domain test performance. Selecting parameters from the best in-domain development accuracy partially contributes to this result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">EXPERIMENT ON SNLI</head><p>In  <ref type="bibr" target="#b65">(Williams et al., 2017)</ref> 67.0 67.6 2. InnerAtt <ref type="bibr" target="#b16">(Balazs et al., 2017)</ref> 72.1 72.1 3. ESIM <ref type="bibr" target="#b65">(Williams et al., 2017)</ref> 72.3 72.1 4. Gated-Att BiLSTM <ref type="bibr">(Chen et al., 2017b)</ref> 73.2 73.6 5. Shorcut-Stacked encoder <ref type="bibr" target="#b48">(Nie &amp; Bansal, 2017)</ref> 74   <ref type="bibr" target="#b51">Rocktäschel et al. (2015)</ref> aligns each sentence word-by-word with attention on top of LSTMs. <ref type="bibr" target="#b6">Wang &amp; Jiang (2015)</ref> enforces cross sentence attention word-by-word matching with the proprosed mL-STM model. <ref type="bibr" target="#b24">Cheng et al. (2016)</ref> proposes long short-term memory-network(LSTMN) with deep attention fusion that links the current word to previous word stored in memory. <ref type="bibr" target="#b8">Parikh et al. (2016)</ref> decomposes the task into sub-problems and conquer them respectively. <ref type="bibr" target="#b10">Yu &amp; Munkhdalai (2017)</ref> proposes neural tree indexer, a full n-ary tree whose subtrees can be overlapped. Re-read LSTM proposed by <ref type="bibr" target="#b55">Sha et al. (2016)</ref> considers the attention vector of one sentence as the inner-state of LSTM for another sentence. <ref type="bibr" target="#b22">Chen et al. (2016)</ref> propose a sequential model that infers locally, and a ensemble with tree-like inference module that further improves performance. We show our model, DIIN, achieves state-of-the-art performance on the competitive leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">EXPERIMENT ON QUORA QUESTION PAIR DATASET</head><p>In this subsection, we evaluate the effectiveness of our model for paraphrase identification as natural language inference task. Other than our baselines, we compare with <ref type="bibr" target="#b64">Wang et al. (2017)</ref> and  <ref type="bibr">(2017)</ref>. BIMPM models different perspective of matching between sentence pair on both direction, then aggregates matching vector with LSTM. DECATT word and DECATT char uses automatically collected in-domain paraphrase data to noisy pretrain n-gram word embedding and ngram subword embedding correspondingly on decomposable attention model proposed by <ref type="bibr" target="#b8">(Parikh et al., 2016)</ref>. In Table <ref type="table" target="#tab_3">4</ref>, our experiment shows DIIN has better performance than all other models and an ensemble score is higher than the former best result for more than 1 percent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>We conduct a ablation study on our base model to examine the effectiveness of each component. We study our model on MultiNLI dataset and we use Matched validation score as the standard for model selection. The result is shown in Table <ref type="table" target="#tab_4">5</ref>. In the experiment 2, we remove the convolutional feature extractor and then model is structured as a sentence-encoding based model. The sentence representation matrix is max-pooled over time to obtain a feature vector. Once we have the feature vector p for premise and h for hypothesis, we use [p; h; |p-h|; p•h] as final feature vector to classify the relationship. We obtain 73.2 for matched score and 73.6 on mismatched data. The result is competitive among other sentence-encoding based model. We further study how encoding layer contribute in enriching the feature space in interaction tensor. If we remove encoding layer completely, then we'll obtain a 73.5 for matched score and 73.2 for mismatched score. The result demonstrate the feature extraction layer have powerful capability to capture the semantic feature.</p><p>In experiment 4, we remove both self-attention and fuse gate, thus retaining only highway network. The result improves to 77.7 and 77.3 respectively on matched and mismatched development set. However, in experiment 5, when we only remove fuse gate, to our surprise, the performance degrade to 73.5 for matched score and 73.8 for mismatched. On the other hand, if we use the addition of the representation after highway network and the representation after self-attention as skip connection as in experiment 9, the performance increase to 77.3 and 76.3. The comparison indicates self-attention layer makes the training harder to converge while a skip connection could ease the gradient flow for both highway layer and self-attention layer. By comparing the base model and the model the in experiment 6, we show that the fuse gate not only well serves as a skip connection, but also makes good decision upon which information the fuse for both representation. To show that dense interaction tensor contains more semantic information, we replace the dense interaction tensor with dot product similarity matrix between the encoded representation of premise and hypothesis. The result shows that the dot product similarity matrix has an inferior capacity of semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dimensionality and Parameter number study</head><p>To study the influence of the model dimension d which is also the channel number of interaction tensor, we design experiments to find out whether dimension has influence on performance. We also present the parameter count of these models.</p><p>The dimensionality is 448 where 300 comes from word embedding, 100 comes from char feature, 47 comes from Part of speech tagging and 1 comes from the binary exact match feature. Since Highway network sets the output dimensionality default as that in input, we design a variant to highway network so that different output size could be obtained. The variant of highway layer is </p><formula xml:id="formula_9">t i = tanh(W t x i + b t )<label>(9)</label></formula><formula xml:id="formula_10">g i = σ(W g x i + b g )<label>(10)</label></formula><formula xml:id="formula_11">x i = x i d in = d out W x x i + b x d in = d out<label>(11)</label></formula><formula xml:id="formula_12">o i = g i • t i + (1 -g i ) • x i<label>(12)</label></formula><p>where x i is the i-th vector of input matrix x, o i is the i-th vector of output matrix o, W t , W g , W x ∈ R din×dout and b t , b g , b x ∈ R dout are trainable weights.</p><p>The result shows that higher dimension number have better performance when the dimension number is lower certain threshold, however, when the number of dimensionality is greater than the threshold, larger number of parameter and higher dimensionality doesn't contribute to performance.</p><p>In the case of SNLI, due to its simplicity in language pattern, 250D would be suffice to obtain a good performance. On the other hand, it requires 350D to achieve a competitive performance on MultiNLI. We fail to reproduce our best performance with the new structure on MultiNLI. It shows that the additional layer on highway network doesn't helps convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Error analysis</head><p>To analyze the model prediction, we use annotated subset of development set provided by <ref type="bibr" target="#b65">Williams et al. (2017)</ref> that consists of 1,000 examples each tagged with zero or more following tags:</p><p>• CONDITIONAL: whether the sentence contains a conditional.</p><p>• WORD OVERLAP: whether both sentences share more than 70% of their tokens.</p><p>• NEGATION: whether a negation shows up in either sentence.</p><p>• ANTO: whether two sentences contain antonym pair.  • LONG SENTENCE: whether premise or hypothesis is longer than 30 or 16 tokens respectively. • TENSE DIFFERENCE: whether any verb in two sentences uses different tense.</p><p>• ACTIVE/PASSIVE: whether there is an active-to-passive (or vice versa) transformation from the premise to the hypothesis. • PARAPHRASE: whether the two sentences are close paraphrases • QUANTITY/TIME REASONING: whether understanding the pair requires quantity or time reasoning. • COREF: Whether the hypothesis contains a pronoun or referring expression that needs to be resolved using the premise. • QUANTIFIER: Whether either sentence contains one of the following quantifier: much, enough, more, most, less, least, no, none, some, any, many, few, several, almost, nearly. • MODAL: Whether one of the following modal verbs appears in either sentence: can, could, may, might, must, will, would, should.</p><p>• BELIEF: Whether one of the following belief verbs appear in either sentence: know, believe, understand, doubt, think, suppose, recognize, forget, remember, imagine, mean, agree, disagree, deny, promise.</p><p>For more detailed descriptions, please resort to <ref type="bibr" target="#b65">Williams et al. (2017)</ref>. The result is shown in Table <ref type="table" target="#tab_7">7</ref>. We find DIIN is consistently better on sentence pair with WORD OVERLAP, ANTO, LONG SENTENCE, PARAPHRASE and BELIEF tags by a large margin. During investigation, we hypothesize exact match feature helps the model to better understand paraphrase, therefore we study the result from second ablation ablation study where exact match feature is not used. Surprisingly, the model without exact model feature does not work worse on PARAPHRASE, instead, the accuracy on ANTO drops about 10%. DIIN is also work well on LONG SENTENCE, partially because the receptive field is large enough to cover all tokens.</p><p>Visualization We also visualize the hidden representation from interaction tensor I and the feature map from first dense block in Figure <ref type="figure" target="#fig_2">2</ref>. We pick a sentence pair whose premise is "South Carolina has no referendum right, so the Supreme Court canceled the vote and upheld the ban." and hypothesis is "South Carolina has a referendum right, so the Supreme Court was powerless over the state.". The upper row of figures are sampled from hidden representation of interaction tensor I. We observe the values of neurons are highly correlated row-wise and column-wise in the interaction tensor I and different channel of hidden representation shows different aspect of interaction. Though in certain channel same words, "referendum", or phrases, "supreme court", cause activation, different word or phrase pair, such as "ban" and "powerless over", also cause activation in other activation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORK</head><p>We show the interaction tensor (or attention weight) contains semantic information to understand the natural language. We introduce Interactive Inference Network, a novel class of architecture that allows the model to solve NLI or NLI alike tasks via extracting semantic feature from interaction tensor end-to-end. One instance of such architecture, Densely Interactive Inference Network (DIIN), achieves state-of-the-art performance on multiple datasets. By ablating each component in DIIN and changing the dimensionality, we show the effectiveness of each component in DIIN.</p><p>Though we have the initial exploration of natural language inference in interaction space, the full potential is not yet clear. We will keep exploring the potential of interaction space. Incorporating common-sense knowledge from external resources such as knowledge base to leverage the capacity of the mode is another research goal of ours.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A visual illustration of Interactive Inference Network (IIN).</figDesc><graphic coords="4,108.00,81.86,396.00,397.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>It shows the model's strong capacity of understanding text in different perspective. The lower row of Figure 2 shows the feature map from first dense block. After being convolved from the interaction tensor and previous feature map, new feature maps shows activation in different position, demonstrating different semantic features are found. The first figure in the lower row has similar pattern as normal attention weight whereas others has no obvious pattern. Different channels of feature maps indicate different kinds of semantic feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: A visualization of hidden representation. The premise is "South Carolina has no referendum right, so the Supreme Court canceled the vote and upheld the ban." and the hypothesis is "South Carolina has a referendum right, so the Supreme Court was powerless over the state.". The upper row are sampled from interaction tensor I and the lower row are sample from the feature map of first dense block. We use viridis colormap, where yellow represents activation and purple shows the neuron is not active.</figDesc><graphic coords="12,108.00,207.66,396.00,218.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>Model</cell><cell>Test Accuracy</cell></row><row><cell></cell><cell>Matched Mismatched</cell></row><row><cell>1. BiLSTM</cell><cell></cell></row></table><note><p><p><p><p><p><p><p><p><p><p><p><p><p>, we compare our model to other model performance on SNLI. Experiments (2-7) are sentence encoding based model.</p>Bowman et al. (2016)  </p>provides a BiLSTM baseline.</p><ref type="bibr" target="#b62">Vendrov et al. (2015)</ref> </p>adopts two layer GRU encoder with pre-trained "skip-thoughts" vectors. To capture sentence-level semantics,</p><ref type="bibr" target="#b45">Mou et al. (2015)</ref> </p>use tree-based</p>CNN and Bowman et al. (2016)  </p>propose a stack-augmented parser-interpreter neural network (SPINN) which incorporates parsing information in a sequential manner.</p><ref type="bibr" target="#b40">Liu et al. (2016)</ref> </p>uses intra-attention on top of BiLSTM to generate sentence representation, and</p>Munkhdalai &amp; Yu (2016)  </p>proposes an memory augmented neural network</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>MultiNLI result.</figDesc><table><row><cell>.6</cell><cell>73.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>SNLI result.    to encode the sentence. The next group of model, experiments (8-18), uses cross sentence feature.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Quora question dataset result. First six rows are copied from<ref type="bibr" target="#b64">Wang et al. (2017)</ref> and next two rows from<ref type="bibr" target="#b60">(Tomar et al., 2017)</ref>.</figDesc><table><row><cell>Model</cell><cell cols="2">Accuracy</cell></row><row><cell></cell><cell cols="2">Dev Acc Test Acc</cell></row><row><cell>1. Siamese-CNN</cell><cell>-</cell><cell>79.60</cell></row><row><cell>2. Multi-Perspective CNN</cell><cell>-</cell><cell>81.38</cell></row><row><cell>3. Siamese-LSTM</cell><cell>-</cell><cell>82.58</cell></row><row><cell>4. Multi-Perspective-LSTM</cell><cell>-</cell><cell>83.21</cell></row><row><cell>5. L.D.C</cell><cell>-</cell><cell>85.55</cell></row><row><cell>6. BiMPM(Wang et al., 2017)</cell><cell>88.69</cell><cell>88.17</cell></row><row><cell>7. pt-DECATT word (Tomar et al., 2017)</cell><cell>88.44</cell><cell>87.54</cell></row><row><cell>8. pt-DECATT char (Tomar et al., 2017)</cell><cell>88.89</cell><cell>88.40</cell></row><row><cell>9. DIIN</cell><cell>89.44</cell><cell>89.06</cell></row><row><cell>10. DIIN (ensemble)</cell><cell>90.48</cell><cell>89.84</cell></row><row><cell>Tomar et al.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Ablation study result.</figDesc><table><row><cell cols="2">Ablation Experiments</cell><cell></cell><cell cols="2">Dev Accuracy</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Matched Mismatched</cell></row><row><cell>1. DIIN</cell><cell></cell><cell></cell><cell>79.2</cell><cell>79.1</cell></row><row><cell cols="2">2. DIIN -conv structure</cell><cell></cell><cell>73.2</cell><cell>73.6</cell></row><row><cell cols="2">3. DIIN -encoding layer</cell><cell></cell><cell>73.5</cell><cell>73.2</cell></row><row><cell cols="2">4. DIIN -self-att and fuse gate</cell><cell></cell><cell>77.7</cell><cell>77.3</cell></row><row><cell>5. DIIN -fuse gate</cell><cell></cell><cell></cell><cell>73.5</cell><cell>73.8</cell></row><row><cell cols="3">6. DIIN -fuse gate + addition as skip connection</cell><cell>77.3</cell><cell>76.3</cell></row><row><cell cols="3">7. DIIN -dense interaction tensor + similarity matrix</cell><cell>75.2</cell><cell>75.5</cell></row><row><cell></cell><cell></cell><cell cols="2">Dev Accuracy</cell><cell></cell></row><row><cell>Dimension</cell><cell cols="4">Param Count SNLI Matched Mismatched</cell></row><row><cell>1. DIIN(448)</cell><cell>4.36 M</cell><cell>88.4</cell><cell>79.2</cell><cell>79.1</cell></row><row><cell>2. 10</cell><cell>708 K</cell><cell>81.6</cell><cell>71.7</cell><cell>71.9</cell></row><row><cell>3. 30</cell><cell>765 K</cell><cell>85.2</cell><cell>75.0</cell><cell>74.9</cell></row><row><cell>4. 50</cell><cell>832 K</cell><cell>86.0</cell><cell>76.1</cell><cell>76.7</cell></row><row><cell>5. 100</cell><cell>1.05 M</cell><cell>86.9</cell><cell>76.9</cell><cell>77.1</cell></row><row><cell>6. 150</cell><cell>1.34 M</cell><cell>87.6</cell><cell>77.6</cell><cell>77.4</cell></row><row><cell>7. 250</cell><cell>2.14 M</cell><cell>88.1</cell><cell>78.1</cell><cell>77.7</cell></row><row><cell>8. 350</cell><cell>3.23 M</cell><cell>88.0</cell><cell>78.7</cell><cell>78.2</cell></row><row><cell>9. 447</cell><cell>4.55 M</cell><cell>88.1</cell><cell>78.4</cell><cell>78.0</cell></row><row><cell>10. 540</cell><cell>6.08 M</cell><cell>88.1</cell><cell>78.8</cell><cell>78.7</cell></row><row><cell>11. 600</cell><cell>7.20 M</cell><cell>88.4</cell><cell>78.7</cell><cell>78.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Dimensionality and parameter number study result.</figDesc><table><row><cell>designed as follows:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>MultiNLI result.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In-domain (matched) leaderboard: https://inclass.kaggle.com/c/ multinli-matched-open-evaluation/leaderboard; cross-domain(mismatched) leaderboard: https://inclass.kaggle.com/c/multinli-mismatched-open-evaluation/ leaderboard</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Yuchen Lu, Chang Huang and Kai Yu for their sincere and insightful advice.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">(</forename><surname>Handcrafted</surname></persName>
		</author>
		<author>
			<persName><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Lstm Encoder(bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2016. 2015. 2015</date>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">80</biblScope>
		</imprint>
	</monogr>
	<note>6 3. pretrained GRU encoders. 4 4. tree-based CNN encoders(Mou et al.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Spinn-Pi Encoders(bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">83</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">BiLSTM intra-attention encoders</title>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">84</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">(</forename><surname>Nse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">&amp;</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><surname>Rocktäschel</surname></persName>
		</author>
		<title level="m">LSTM with attention</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">83</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Wang</forename><forename type="middle">&amp;</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">86</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><surname>Cheng</surname></persName>
		</author>
		<idno>86.3</idno>
		<title level="m">LSTMN with deep attention fusion</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="volume">86</biblScope>
		</imprint>
	</monogr>
	<note>3 11. decomposable attention model</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intra-sentence attention +</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="86" to="88" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">(</forename><surname>Bimpm</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Nti-Slstm-Lstm(</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">&amp;</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Sha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2017. 2016</date>
			<biblScope unit="volume">87</biblScope>
		</imprint>
	</monogr>
	<note>3 15. re-read LSTM</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><surname>Esim(chen</surname></persName>
		</author>
		<idno>88.0</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
		<title level="m">ESIM ensemble with syntactic tree-LSTM</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="88" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Wang</forename><surname>Bimpm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernanda</forename><surname>Viegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-03">March 2016</date>
		</imprint>
	</monogr>
	<note>TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems. arXiv.org</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>arXiv.org</idno>
		<imprint>
			<date type="published" when="2014-09">September 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Refining Raw Sentence Representations for Textual Entailment Recognition via Attention</title>
		<author>
			<persName><forename type="first">Jorge</forename><forename type="middle">A</forename><surname>Balazs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edison</forename><surname>Marrese-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Loyola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
		<idno>arXiv.org</idno>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Recognising Textual Entailment with Logical Inference</title>
		<author>
			<persName><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katja</forename><surname>Markert</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><surname>Hlt/Emnlp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.05326</idno>
		<imprint>
			<date type="published" when="2015-08">August 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Jon</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><surname>Potts</surname></persName>
		</author>
		<title level="m">A Fast Unified Model for Parsing and Sentence Understanding. arXiv.org</title>
		<imprint>
			<date type="published" when="2016-03">March 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00051</idno>
		<title level="m">Reading Wikipedia to Answer Open-Domain Questions. arXiv.org</title>
		<imprint>
			<date type="published" when="2017-03">March 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
		<title level="m">Enhanced LSTM for Natural Language Inference. arXiv.org</title>
		<imprint>
			<date type="published" when="2016-09">September 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Recurrent Neural Network-Based Sentence Encoder with Gated Attention for Natural Language Inference</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.01353</idno>
		<imprint>
			<date type="published" when="2017-08">August 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Long Short-Term Memory-Networks for Machine Reading</title>
		<author>
			<persName><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno>arXiv.org</idno>
		<imprint>
			<date type="published" when="2016-01">January 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">a natural language inference system</title>
		<author>
			<persName><forename type="first">Yaroslav</forename><surname>Fyodorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoad</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nissim</forename><surname>Francez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Inference in Computational Semantics</title>
		<meeting>the 2nd Workshop on Inference in Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2000-11">November 2000</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Convolutional Sequence to Sequence Learning. arXiv.org</title>
		<imprint>
			<date type="published" when="2017-05">May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<title level="m">Ruminating Reader: Reasoning with Gated Multi-Hop Attention. arXiv.org</title>
		<imprint>
			<date type="published" when="2017-04">April 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Teaching Machines to Read and Comprehend. arXiv.org</title>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Convolutional Neural Network Architectures for Matching Natural Language Sentences. NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Densely Connected Convolutional Networks. arXiv.org</title>
		<imprint>
			<date type="published" when="2016-08">August 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bag of Tricks for Efficient Text Classification. arXiv.org</title>
		<imprint>
			<date type="published" when="2016-07">July 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno>arXiv.org</idno>
		<title level="m">Can Active Memory Replace Attention? NIPS, 2016. Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A Convolutional Neural Network for Modelling Sentences</title>
		<imprint>
			<date type="published" when="2014-04">April 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu</title>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Machine Translation in Linear Time. CoRR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<title level="m">Convolutional Neural Networks for Sentence Classification. arXiv.org</title>
		<imprint>
			<date type="published" when="2014-08">August 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<title level="m">Character-Aware Neural Language Models. AAAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName><forename type="first">Al</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Fully Character-Level Neural Machine Translation without Explicit Segmentation</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<idno>arXiv.org</idno>
		<imprint>
			<date type="published" when="2016-10">October 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention. arXiv.org</title>
		<imprint>
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">An extended model of natural logic</title>
		<author>
			<persName><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A SICK cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>LREC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<title level="m">Coherent Dialogue with Attention-based Language Models. arXiv.org</title>
		<imprint>
			<date type="published" when="2016-11">November 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Inference by Tree-Based Convolution and Heuristic Matching. arXiv.org</title>
		<imprint>
			<date type="published" when="2015-12">December 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Semantic Encoders. arXiv.org</title>
		<imprint>
			<date type="published" when="2016-07">July 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">The RepEval 2017 Shared Task: Multi-Genre Natural Language Inference with Sentence Representations</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno>arXiv.org</idno>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02312</idno>
		<title level="m">Shortcut-Stacked Sentence Encoders for Multi-Domain Inference. arXiv.org</title>
		<imprint>
			<date type="published" when="2017-08">August 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><surname>Täckström</surname></persName>
		</author>
		<title level="m">Dipanjan Das, and Jakob Uszkoreit. A Decomposable Attention Model for Natural Language Inference. arXiv.org</title>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<title level="m">Reasoning about Entailment with Neural Attention. arXiv.org</title>
		<imprint>
			<date type="published" when="2015-09">September 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00685</idno>
		<title level="m">A Neural Attention Model for Abstractive Sentence Summarization. arXiv.org</title>
		<imprint>
			<date type="published" when="2015-09">September 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Michael S Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei-Fei</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bidirectional Attention Flow for Machine Comprehension. arXiv.org</title>
		<imprint>
			<date type="published" when="2016-11">November 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Reading and Thinking -Re-read LSTM Unit for Textual Entailment Recognition</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COLING</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>arXiv.org</idno>
		<imprint>
			<date type="published" when="2014-09">September 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Parsing Natural Scenes and Natural Language with Recursive Neural Networks</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Chiung-Yu Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Going Deeper with Convolutions. arXiv.org</title>
		<imprint>
			<date type="published" when="2014-09">September 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Neural Paraphrase Identification of Questions with Noisy Pretraining</title>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Singh Tomar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thyago</forename><surname>Duque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<idno>arXiv.org</idno>
		<imprint>
			<date type="published" when="2017-04">April 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Attention Is All You Need. arXiv.org</title>
		<imprint>
			<date type="published" when="2017-06">June 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<title level="m">Sanja Fidler, and Raquel Urtasun. Order-Embeddings of Images and Language. arXiv.org</title>
		<imprint>
			<date type="published" when="2015-11">November 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<title level="m">Learning Natural Language Inference with LSTM. arXiv.org</title>
		<imprint>
			<date type="published" when="2015-12">December 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Bilateral Multi-Perspective Matching for Natural Language Sentences. cs</title>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<title level="m">A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference. arXiv.org</title>
		<imprint>
			<date type="published" when="2017-04">April 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>arXiv.org</idno>
		<title level="m">Richard Zemel, and Yoshua Bengio. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<imprint>
			<date type="published" when="2015-02">February 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Neural Tree Indexers for Text Understanding</title>
		<author>
			<persName><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
	</analytic>
	<monogr>
		<title level="m">EACL, 2017. Matthew D Zeiler. ADADELTA: An Adaptive Learning Rate Method. arXiv.org</title>
		<imprint>
			<date type="published" when="2012-12">December 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Which Encoding is the Best for Text Classification in Chinese, English, Japanese and Korean? arXiv.org</title>
		<imprint>
			<date type="published" when="2017-08">August 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Character-level Convolutional Networks for Text Classification</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.01626</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="1509">1509. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
